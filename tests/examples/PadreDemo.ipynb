{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "from pypadre.app import p_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticates given user with password and sets token in the Config file\n",
    "# Requirement: First create your user through interface\n",
    "pypadre.authenticate(\"cfellicious\", \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets in local before deleting\n",
      "[]\n",
      "Datasets in local\n",
      "[]\n",
      "Datasets on server\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Delete all local datasets. Then show all available datasets on local and server\n",
    "list_of_datasets = pypadre.local_backend.datasets.list()\n",
    "print(\"Datasets in local before deleting\")\n",
    "print(list_of_datasets)\n",
    "for ds in list_of_datasets:\n",
    "    pypadre.datasets.delete(ds)\n",
    "\n",
    "print(\"Datasets in local\")\n",
    "print(pypadre.local_backend.datasets.list())\n",
    "\n",
    "print(\"Datasets on server\")\n",
    "print(pypadre.remote_backend.datasets.list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets available on openMl for diabetes\n",
      "[{'did': 37, 'name': 'diabetes', 'format': 'ARFF', 'status': 'active', 'MajorityClassSize': 500, 'MaxNominalAttDistinctValues': 2, 'MinorityClassSize': 268, 'NumberOfClasses': 2, 'NumberOfFeatures': 9, 'NumberOfInstances': 768, 'NumberOfInstancesWithMissingValues': 0, 'NumberOfMissingValues': 0, 'NumberOfNumericFeatures': 8, 'NumberOfSymbolicFeatures': 1}]\n",
      "Saving OpenMl Datasets... \n",
      "Uploading dataset diabetes, (768, 9), Multivariat\n"
     ]
    }
   ],
   "source": [
    "# Download a dataset from OpenML and upload it to the server\n",
    "# For the following example we are downloading diabetes dataset from openMl and uploading it to the Server\n",
    "# Some datasets in OpenML do not work and raise exceptions\n",
    "name = \"diabetes\"\n",
    "downloads = pypadre.datasets.search_downloads(name)\n",
    "print(\"Datasets available on openMl for \" + name)\n",
    "print(downloads)\n",
    "print(\"Saving OpenMl Datasets... \")\n",
    "for externel_dataset in pypadre.datasets.download(downloads):\n",
    "    pypadre.datasets.put(externel_dataset, upload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata for dataset 1\n",
      "\tid=9f9b1f0e-378f-486e-9b5a-96fd0a9bac7a\n",
      "\tname=diabetes\n",
      "\tversion=1\n",
      "\tdescription=**Author**: [Vincent Sigillito](vgs@aplcen.apl.jhu.edu)  \n",
      "\n",
      "**Source**: [Obtained from UCI](https://archive.ics.uci.edu/ml/datasets/pima+indians+diabetes) \n",
      "\n",
      "**Please cite**: [UCI citation policy](https://archive.ics.uci.edu/ml/citation_policy.html)  \n",
      "\n",
      "1. Title: Pima Indians Diabetes Database\n",
      " \n",
      " 2. Sources:\n",
      "    (a) Original owners: National Institute of Diabetes and Digestive and\n",
      "                         Kidney Diseases\n",
      "    (b) Donor of database: Vincent Sigillito (vgs@aplcen.apl.jhu.edu)\n",
      "                           Research Center, RMI Group Leader\n",
      "                           Applied Physics Laboratory\n",
      "                           The Johns Hopkins University\n",
      "                           Johns Hopkins Road\n",
      "                           Laurel, MD 20707\n",
      "                           (301) 953-6231\n",
      "    (c) Date received: 9 May 1990\n",
      " \n",
      " 3. Past Usage:\n",
      "     1. Smith,~J.~W., Everhart,~J.~E., Dickson,~W.~C., Knowler,~W.~C., &\n",
      "        Johannes,~R.~S. (1988). Using the ADAP learning algorithm to forecast\n",
      "        the onset of diabetes mellitus.  In {it Proceedings of the Symposium\n",
      "        on Computer Applications and Medical Care} (pp. 261--265).  IEEE\n",
      "        Computer Society Press.\n",
      " \n",
      "        The diagnostic, binary-valued variable investigated is whether the\n",
      "        patient shows signs of diabetes according to World Health Organization\n",
      "        criteria (i.e., if the 2 hour post-load plasma glucose was at least \n",
      "        200 mg/dl at any survey  examination or if found during routine medical\n",
      "        care).   The population lives near Phoenix, Arizona, USA.\n",
      " \n",
      "        Results: Their ADAP algorithm makes a real-valued prediction between\n",
      "        0 and 1.  This was transformed into a binary decision using a cutoff of \n",
      "        0.448.  Using 576 training instances, the sensitivity and specificity\n",
      "        of their algorithm was 76% on the remaining 192 instances.\n",
      " \n",
      " 4. Relevant Information:\n",
      "       Several constraints were placed on the selection of these instances from\n",
      "       a larger database.  In particular, all patients here are females at\n",
      "       least 21 years old of Pima Indian heritage.  ADAP is an adaptive learning\n",
      "       routine that generates and executes digital analogs of perceptron-like\n",
      "       devices.  It is a unique algorithm; see the paper for details.\n",
      " \n",
      " 5. Number of Instances: 768\n",
      " \n",
      " 6. Number of Attributes: 8 plus class \n",
      " \n",
      " 7. For Each Attribute: (all numeric-valued)\n",
      "    1. Number of times pregnant\n",
      "    2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
      "    3. Diastolic blood pressure (mm Hg)\n",
      "    4. Triceps skin fold thickness (mm)\n",
      "    5. 2-Hour serum insulin (mu U/ml)\n",
      "    6. Body mass index (weight in kg/(height in m)^2)\n",
      "    7. Diabetes pedigree function\n",
      "    8. Age (years)\n",
      "    9. Class variable (0 or 1)\n",
      " \n",
      " 8. Missing Attribute Values: None\n",
      " \n",
      " 9. Class Distribution: (class value 1 is interpreted as \"tested positive for\n",
      "    diabetes\")\n",
      " \n",
      "    Class Value  Number of instances\n",
      "    0            500\n",
      "    1            268\n",
      " \n",
      " 10. Brief statistical analysis:\n",
      " \n",
      "     Attribute number:    Mean:   Standard Deviation:\n",
      "     1.                     3.8     3.4\n",
      "     2.                   120.9    32.0\n",
      "     3.                    69.1    19.4\n",
      "     4.                    20.5    16.0\n",
      "     5.                    79.8   115.2\n",
      "     6.                    32.0     7.9\n",
      "     7.                     0.5     0.3\n",
      "     8.                    33.2    11.8\n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Relabeled values in attribute 'class'\n",
      "    From: 0                       To: tested_negative     \n",
      "    From: 1                       To: tested_positive\n",
      "\toriginalSource=https://www.openml.org/data/v1/download/37/diabetes.arff\n",
      "\ttype=Multivariat\n",
      "\tpublished=True\n",
      "Binary description:\n",
      "\tn_att=9\n",
      "\tn_target=1\n",
      "\tstatus=DescribeResult(nobs=768, minmax=(array([ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.078, 21.   ,\n",
      "        0.   ]), array([ 17.  , 199.  , 122.  ,  99.  , 846.  ,  67.1 ,   2.42,  81.  ,\n",
      "         1.  ])), mean=array([  3.84505208, 120.89453125,  69.10546875,  20.53645833,\n",
      "        79.79947917,  31.99257812,   0.4718763 ,  33.24088542,\n",
      "         0.34895833]), variance=array([1.13540563e+01, 1.02224831e+03, 3.74647271e+02, 2.54473245e+02,\n",
      "       1.32811801e+04, 6.21599840e+01, 1.09778638e-01, 1.38303046e+02,\n",
      "       2.27482616e-01]), skewness=array([ 0.89991194,  0.17341396, -1.84000523,  0.10915876,  2.26781046,\n",
      "       -0.42814328,  1.9161592 ,  1.12738926,  0.6337757 ]), kurtosis=array([ 0.15038274,  0.62881333,  5.13869066, -0.52449449,  7.15957492,\n",
      "        3.26125742,  5.55079205,  0.63117694, -1.59832836]))\n",
      "Metadata for dataset 1\n",
      "\towner=cfellicious\n",
      "\tpublished=True\n",
      "\tuid=1\n",
      "\tname=diabetes\n",
      "\tversion=1\n",
      "\tdescription=**Author**: [Vincent Sigillito](vgs@aplcen.apl.jhu.edu)  \n",
      "\n",
      "**Source**: [Obtained from UCI](https://archive.ics.uci.edu/ml/datasets/pima+indians+diabetes) \n",
      "\n",
      "**Please cite**: [UCI citation policy](https://archive.ics.uci.edu/ml/citation_policy.html)  \n",
      "\n",
      "1. Title: Pima Indians Diabetes Database\n",
      " \n",
      " 2. Sources:\n",
      "    (a) Original owners: National Institute of Diabetes and Digestive and\n",
      "                         Kidney Diseases\n",
      "    (b) Donor of database: Vincent Sigillito (vgs@aplcen.apl.jhu.edu)\n",
      "                           Research Center, RMI Group Leader\n",
      "                           Applied Physics Laboratory\n",
      "                           The Johns Hopkins University\n",
      "                           Johns Hopkins Road\n",
      "                           Laurel, MD 20707\n",
      "                           (301) 953-6231\n",
      "    (c) Date received: 9 May 1990\n",
      " \n",
      " 3. Past Usage:\n",
      "     1. Smith,~J.~W., Everhart,~J.~E., Dickson,~W.~C., Knowler,~W.~C., &\n",
      "        Johannes,~R.~S. (1988). Using the ADAP learning algorithm to forecast\n",
      "        the onset of diabetes mellitus.  In {it Proceedings of the Symposium\n",
      "        on Computer Applications and Medical Care} (pp. 261--265).  IEEE\n",
      "        Computer Society Press.\n",
      " \n",
      "        The diagnostic, binary-valued variable investigated is whether the\n",
      "        patient shows signs of diabetes according to World Health Organization\n",
      "        criteria (i.e., if the 2 hour post-load plasma glucose was at least \n",
      "        200 mg/dl at any survey  examination or if found during routine medical\n",
      "        care).   The population lives near Phoenix, Arizona, USA.\n",
      " \n",
      "        Results: Their ADAP algorithm makes a real-valued prediction between\n",
      "        0 and 1.  This was transformed into a binary decision using a cutoff of \n",
      "        0.448.  Using 576 training instances, the sensitivity and specificity\n",
      "        of their algorithm was 76% on the remaining 192 instances.\n",
      " \n",
      " 4. Relevant Information:\n",
      "       Several constraints were placed on the selection of these instances from\n",
      "       a larger database.  In particular, all patients here are females at\n",
      "       least 21 years old of Pima Indian heritage.  ADAP is an adaptive learning\n",
      "       routine that generates and executes digital analogs of perceptron-like\n",
      "       devices.  It is a unique algorithm; see the paper for details.\n",
      " \n",
      " 5. Number of Instances: 768\n",
      " \n",
      " 6. Number of Attributes: 8 plus class \n",
      " \n",
      " 7. For Each Attribute: (all numeric-valued)\n",
      "    1. Number of times pregnant\n",
      "    2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
      "    3. Diastolic blood pressure (mm Hg)\n",
      "    4. Triceps skin fold thickness (mm)\n",
      "    5. 2-Hour serum insulin (mu U/ml)\n",
      "    6. Body mass index (weight in kg/(height in m)^2)\n",
      "    7. Diabetes pedigree function\n",
      "    8. Age (years)\n",
      "    9. Class variable (0 or 1)\n",
      " \n",
      " 8. Missing Attribute Values: None\n",
      " \n",
      " 9. Class Distribution: (class value 1 is interpreted as \"tested positive for\n",
      "    diabetes\")\n",
      " \n",
      "    Class Value  Number of instances\n",
      "    0            500\n",
      "    1            268\n",
      " \n",
      " 10. Brief statistical analysis:\n",
      " \n",
      "     Attribute number:    Mean:   Standard Deviation:\n",
      "     1.                     3.8     3.4\n",
      "     2.                   120.9    32.0\n",
      "     3.                    69.1    19.4\n",
      "     4.                    20.5    16.0\n",
      "     5.                    79.8   115.2\n",
      "     6.                    32.0     7.9\n",
      "     7.                     0.5     0.3\n",
      "     8.                    33.2    11.8\n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Relabeled values in attribute 'class'\n",
      "    From: 0                       To: tested_negative     \n",
      "    From: 1                       To: tested_positive\n",
      "\tprofile=None\n",
      "\ttype=http://www.padre-lab.eu/onto/Multivariat\n",
      "\tstandardSplitting=None\n",
      "\toriginalSource=https://www.openml.org/data/v1/download/37/diabetes.arff\n",
      "\t_links={'self': {'href': 'http://www.staging.padre-lab.eu/api/datasets/1'}, 'datasets': {'href': 'http://www.staging.padre-lab.eu/api/datasets{?page,size,sort}', 'templated': True}, 'relatedExperiment': {'href': 'https://www.staging.padre-lab.eu/api/experiments/search?search=dataset.uid:1'}, 'relatedRuns': {'href': 'https://www.staging.padre-lab.eu/api/runs/search?search=experiment.dataset.uid:1'}, 'relatedRunSplits': {'href': 'https://www.staging.padre-lab.eu/api/splits/search?search=run.experiment.dataset.uid:1'}}\n",
      "Binary description:\n",
      "\tn_att=9\n",
      "\tn_target=1\n",
      "\tstatus=DescribeResult(nobs=768, minmax=(array([ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.078, 21.   ,\n",
      "        0.   ]), array([ 17.  , 199.  , 122.  ,  99.  , 846.  ,  67.1 ,   2.42,  81.  ,\n",
      "         1.  ])), mean=array([  3.84505208, 120.89453125,  69.10546875,  20.53645833,\n",
      "        79.79947917,  31.99257812,   0.4718763 ,  33.24088542,\n",
      "         0.34895833]), variance=array([1.13540563e+01, 1.02224831e+03, 3.74647271e+02, 2.54473245e+02,\n",
      "       1.32811801e+04, 6.21599840e+01, 1.09778638e-01, 1.38303046e+02,\n",
      "       2.27482616e-01]), skewness=array([ 0.89991194,  0.17341396, -1.84000523,  0.10915876,  2.26781046,\n",
      "       -0.42814328,  1.9161592 ,  1.12738926,  0.6337757 ]), kurtosis=array([ 0.15038274,  0.62881333,  5.13869066, -0.52449449,  7.15957492,\n",
      "        3.26125742,  5.55079205,  0.63117694, -1.59832836]))\n"
     ]
    }
   ],
   "source": [
    "# Total available datasets in Padre and print their details\n",
    "for ds in pypadre.datasets.list():\n",
    "    pypadre.datasets.print_dataset_details(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets 1 on server\n",
      "1_diabetes: http://www.padre-lab.eu/onto/Multivariat, (768, 9), pandas\n"
     ]
    }
   ],
   "source": [
    "# Get dataset from server(Which is download from openMl in the Previous step) and use it later in the experiment \n",
    "did = \"1\"\n",
    "print(\"Datasets %s on server\" % did)\n",
    "ds = pypadre.remote_backend.datasets.get(did)\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Step_0': {'algorithm': {'attributes': {'path': '.steps[0][1]'},\n",
      "                          'value': 'c-oja pas vector classification'},\n",
      "            'doc': {'attributes': {'path': '.steps[0][1].__doc__'},\n",
      "                    'value': 'C-Support Vector Classification.\\n'\n",
      "                             '\\n'\n",
      "                             '    The implementation is based on libsvm. The '\n",
      "                             'fit time complexity\\n'\n",
      "                             '    is more than quadratic with the number of '\n",
      "                             'samples which makes it hard\\n'\n",
      "                             '    to scale to dataset with more than a couple '\n",
      "                             'of 10000 samples.\\n'\n",
      "                             '\\n'\n",
      "                             '    The multiclass support is handled according '\n",
      "                             'to a one-vs-one scheme.\\n'\n",
      "                             '\\n'\n",
      "                             '    For details on the precise mathematical '\n",
      "                             'formulation of the provided\\n'\n",
      "                             '    kernel functions and how `gamma`, `coef0` '\n",
      "                             'and `degree` affect each\\n'\n",
      "                             '    other, see the corresponding section in the '\n",
      "                             'narrative documentation:\\n'\n",
      "                             '    :ref:`svm_kernels`.\\n'\n",
      "                             '\\n'\n",
      "                             '    Read more in the :ref:`User Guide '\n",
      "                             '<svm_classification>`.\\n'\n",
      "                             '\\n'\n",
      "                             '    Parameters\\n'\n",
      "                             '    ----------\\n'\n",
      "                             '    C : float, optional (default=1.0)\\n'\n",
      "                             '        Penalty parameter C of the error term.\\n'\n",
      "                             '\\n'\n",
      "                             \"    kernel : string, optional (default='rbf')\\n\"\n",
      "                             '         Specifies the kernel type to be used in '\n",
      "                             'the algorithm.\\n'\n",
      "                             \"         It must be one of 'linear', 'poly', \"\n",
      "                             \"'rbf', 'sigmoid', 'precomputed' or\\n\"\n",
      "                             '         a callable.\\n'\n",
      "                             \"         If none is given, 'rbf' will be used. \"\n",
      "                             'If a callable is given it is\\n'\n",
      "                             '         used to pre-compute the kernel matrix '\n",
      "                             'from data matrices; that matrix\\n'\n",
      "                             '         should be an array of shape '\n",
      "                             '``(n_samples, n_samples)``.\\n'\n",
      "                             '\\n'\n",
      "                             '    degree : int, optional (default=3)\\n'\n",
      "                             '        Degree of the polynomial kernel function '\n",
      "                             \"('poly').\\n\"\n",
      "                             '        Ignored by all other kernels.\\n'\n",
      "                             '\\n'\n",
      "                             \"    gamma : float, optional (default='auto')\\n\"\n",
      "                             \"        Kernel coefficient for 'rbf', 'poly' and \"\n",
      "                             \"'sigmoid'.\\n\"\n",
      "                             \"        If gamma is 'auto' then 1/n_features \"\n",
      "                             'will be used instead.\\n'\n",
      "                             '\\n'\n",
      "                             '    coef0 : float, optional (default=0.0)\\n'\n",
      "                             '        Independent term in kernel function.\\n'\n",
      "                             \"        It is only significant in 'poly' and \"\n",
      "                             \"'sigmoid'.\\n\"\n",
      "                             '\\n'\n",
      "                             '    probability : boolean, optional '\n",
      "                             '(default=False)\\n'\n",
      "                             '        Whether to enable probability estimates. '\n",
      "                             'This must be enabled prior\\n'\n",
      "                             '        to calling `fit`, and will slow down '\n",
      "                             'that method.\\n'\n",
      "                             '\\n'\n",
      "                             '    shrinking : boolean, optional '\n",
      "                             '(default=True)\\n'\n",
      "                             '        Whether to use the shrinking heuristic.\\n'\n",
      "                             '\\n'\n",
      "                             '    tol : float, optional (default=1e-3)\\n'\n",
      "                             '        Tolerance for stopping criterion.\\n'\n",
      "                             '\\n'\n",
      "                             '    cache_size : float, optional\\n'\n",
      "                             '        Specify the size of the kernel cache (in '\n",
      "                             'MB).\\n'\n",
      "                             '\\n'\n",
      "                             \"    class_weight : {dict, 'balanced'}, optional\\n\"\n",
      "                             '        Set the parameter C of class i to '\n",
      "                             'class_weight[i]*C for\\n'\n",
      "                             '        SVC. If not given, all classes are '\n",
      "                             'supposed to have\\n'\n",
      "                             '        weight one.\\n'\n",
      "                             '        The \"balanced\" mode uses the values of y '\n",
      "                             'to automatically adjust\\n'\n",
      "                             '        weights inversely proportional to class '\n",
      "                             'frequencies in the input data\\n'\n",
      "                             '        as ``n_samples / (n_classes * '\n",
      "                             'np.bincount(y))``\\n'\n",
      "                             '\\n'\n",
      "                             '    verbose : bool, default: False\\n'\n",
      "                             '        Enable verbose output. Note that this '\n",
      "                             'setting takes advantage of a\\n'\n",
      "                             '        per-process runtime setting in libsvm '\n",
      "                             'that, if enabled, may not work\\n'\n",
      "                             '        properly in a multithreaded context.\\n'\n",
      "                             '\\n'\n",
      "                             '    max_iter : int, optional (default=-1)\\n'\n",
      "                             '        Hard limit on iterations within solver, '\n",
      "                             'or -1 for no limit.\\n'\n",
      "                             '\\n'\n",
      "                             \"    decision_function_shape : 'ovo', 'ovr', \"\n",
      "                             \"default='ovr'\\n\"\n",
      "                             \"        Whether to return a one-vs-rest ('ovr') \"\n",
      "                             'decision function of shape\\n'\n",
      "                             '        (n_samples, n_classes) as all other '\n",
      "                             'classifiers, or the original\\n'\n",
      "                             \"        one-vs-one ('ovo') decision function of \"\n",
      "                             'libsvm which has shape\\n'\n",
      "                             '        (n_samples, n_classes * (n_classes - 1) '\n",
      "                             '/ 2).\\n'\n",
      "                             '\\n'\n",
      "                             '        .. versionchanged:: 0.19\\n'\n",
      "                             \"            decision_function_shape is 'ovr' by \"\n",
      "                             'default.\\n'\n",
      "                             '\\n'\n",
      "                             '        .. versionadded:: 0.17\\n'\n",
      "                             \"           *decision_function_shape='ovr'* is \"\n",
      "                             'recommended.\\n'\n",
      "                             '\\n'\n",
      "                             '        .. versionchanged:: 0.17\\n'\n",
      "                             '           Deprecated '\n",
      "                             \"*decision_function_shape='ovo' and None*.\\n\"\n",
      "                             '\\n'\n",
      "                             '    random_state : int, RandomState instance or '\n",
      "                             'None, optional (default=None)\\n'\n",
      "                             '        The seed of the pseudo random number '\n",
      "                             'generator to use when shuffling\\n'\n",
      "                             '        the data.  If int, random_state is the '\n",
      "                             'seed used by the random number\\n'\n",
      "                             '        generator; If RandomState instance, '\n",
      "                             'random_state is the random number\\n'\n",
      "                             '        generator; If None, the random number '\n",
      "                             'generator is the RandomState\\n'\n",
      "                             '        instance used by `np.random`.\\n'\n",
      "                             '\\n'\n",
      "                             '    Attributes\\n'\n",
      "                             '    ----------\\n'\n",
      "                             '    support_ : array-like, shape = [n_SV]\\n'\n",
      "                             '        Indices of support vectors.\\n'\n",
      "                             '\\n'\n",
      "                             '    support_vectors_ : array-like, shape = '\n",
      "                             '[n_SV, n_features]\\n'\n",
      "                             '        Support vectors.\\n'\n",
      "                             '\\n'\n",
      "                             '    n_support_ : array-like, dtype=int32, shape '\n",
      "                             '= [n_class]\\n'\n",
      "                             '        Number of support vectors for each '\n",
      "                             'class.\\n'\n",
      "                             '\\n'\n",
      "                             '    dual_coef_ : array, shape = [n_class-1, '\n",
      "                             'n_SV]\\n'\n",
      "                             '        Coefficients of the support vector in '\n",
      "                             'the decision function.\\n'\n",
      "                             '        For multiclass, coefficient for all '\n",
      "                             '1-vs-1 classifiers.\\n'\n",
      "                             '        The layout of the coefficients in the '\n",
      "                             'multiclass case is somewhat\\n'\n",
      "                             '        non-trivial. See the section about '\n",
      "                             'multi-class classification in the\\n'\n",
      "                             '        SVM section of the User Guide for '\n",
      "                             'details.\\n'\n",
      "                             '\\n'\n",
      "                             '    coef_ : array, shape = [n_class-1, '\n",
      "                             'n_features]\\n'\n",
      "                             '        Weights assigned to the features '\n",
      "                             '(coefficients in the primal\\n'\n",
      "                             '        problem). This is only available in the '\n",
      "                             'case of a linear kernel.\\n'\n",
      "                             '\\n'\n",
      "                             '        `coef_` is a readonly property derived '\n",
      "                             'from `dual_coef_` and\\n'\n",
      "                             '        `support_vectors_`.\\n'\n",
      "                             '\\n'\n",
      "                             '    intercept_ : array, shape = [n_class * '\n",
      "                             '(n_class-1) / 2]\\n'\n",
      "                             '        Constants in decision function.\\n'\n",
      "                             '\\n'\n",
      "                             '    Examples\\n'\n",
      "                             '    --------\\n'\n",
      "                             '    >>> import numpy as np\\n'\n",
      "                             '    >>> X = np.array([[-1, -1], [-2, -1], [1, '\n",
      "                             '1], [2, 1]])\\n'\n",
      "                             '    >>> y = np.array([1, 1, 2, 2])\\n'\n",
      "                             '    >>> from sklearn.svm import SVC\\n'\n",
      "                             '    >>> clf = SVC()\\n'\n",
      "                             '    >>> clf.fit(X, y) #doctest: '\n",
      "                             '+NORMALIZE_WHITESPACE\\n'\n",
      "                             '    SVC(C=1.0, cache_size=200, '\n",
      "                             'class_weight=None, coef0=0.0,\\n'\n",
      "                             \"        decision_function_shape='ovr', degree=3, \"\n",
      "                             \"gamma='auto', kernel='rbf',\\n\"\n",
      "                             '        max_iter=-1, probability=False, '\n",
      "                             'random_state=None, shrinking=True,\\n'\n",
      "                             '        tol=0.001, verbose=False)\\n'\n",
      "                             '    >>> print(clf.predict([[-0.8, -1]]))\\n'\n",
      "                             '    [1]\\n'\n",
      "                             '\\n'\n",
      "                             '    See also\\n'\n",
      "                             '    --------\\n'\n",
      "                             '    SVR\\n'\n",
      "                             '        Support Vector Machine for Regression '\n",
      "                             'implemented using libsvm.\\n'\n",
      "                             '\\n'\n",
      "                             '    LinearSVC\\n'\n",
      "                             '        Scalable Linear Support Vector Machine '\n",
      "                             'for classification\\n'\n",
      "                             '        implemented using liblinear. Check the '\n",
      "                             'See also section of\\n'\n",
      "                             '        LinearSVC for more comparison element.\\n'\n",
      "                             '\\n'\n",
      "                             '    '},\n",
      "            'hyper_parameters': {'execution_parameters': {},\n",
      "                                 'model_parameters': {'class_weight': {'attributes': {'path': '.steps[0][1].class_weight'},\n",
      "                                                                       'value': None},\n",
      "                                                      'decision_function_shape': {'attributes': {'path': '.steps[0][1].decision_function_shape'},\n",
      "                                                                                  'value': 'ovr'},\n",
      "                                                      'error_penalty': {'attributes': {'path': '.steps[0][1].C'},\n",
      "                                                                        'value': 1.0},\n",
      "                                                      'independent_kernel_term': {'attributes': {'path': '.steps[0][1].coef0'},\n",
      "                                                                                  'value': 0.0},\n",
      "                                                      'kernel': {'attributes': {'path': '.steps[0][1].kernel'},\n",
      "                                                                 'value': 'rbf'},\n",
      "                                                      'kernel_coefficient': {'attributes': {'path': '.steps[0][1].gamma'},\n",
      "                                                                             'value': 'auto'},\n",
      "                                                      'poly_degree': {'attributes': {'path': '.steps[0][1].degree'},\n",
      "                                                                      'value': 3},\n",
      "                                                      'probability': {'attributes': {'path': '.steps[0][1].probability'},\n",
      "                                                                      'value': True},\n",
      "                                                      'random_state': {'attributes': {'path': '.steps[0][1].random_state'},\n",
      "                                                                       'value': None},\n",
      "                                                      'shrinking': {'attributes': {'path': '.steps[0][1].shrinking'},\n",
      "                                                                    'value': True},\n",
      "                                                      'tolerance': {'attributes': {'path': '.steps[0][1].tol'},\n",
      "                                                                    'value': 0.001},\n",
      "                                                      'verbose': {'attributes': {'path': '.steps[0][1].verbose'},\n",
      "                                                                  'value': False}},\n",
      "                                 'optimisation_parameters': {'cache_size': {'attributes': {'path': '.steps[0][1].cache_size'},\n",
      "                                                                            'value': 200},\n",
      "                                                             'max_iterations': {'attributes': {'path': '.steps[0][1].max_iter'},\n",
      "                                                                                'value': -1}}}}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: 2019-03-14 16:24:07.820 Experiment<id:Test Experiment SVM 7b43774e->:\tstart: phase=experiment\n",
      "INFO: 2019-03-14 16:24:09.682 \tRun<id:1cb3a134-cb41-4d79-837e-b4b04b52edcd;name:Test Experiment SVM 7b43774e->:\tstart: phase=run\n",
      "INFO: 2019-03-14 16:24:11.508 \t\tSplit<id:3f24e9c8-917e-470b-8061-3f4fa41db752;name:Test Experiment SVM 7b43774e->:\tstart: phase=split\n",
      "INFO: 2019-03-14 16:24:12.799 \t\tSplit<id:3f24e9c8-917e-470b-8061-3f4fa41db752;name:Test Experiment SVM 7b43774e->:\tstop: phase=split\n",
      "INFO: 2019-03-14 16:24:12.802 \tRun<id:1cb3a134-cb41-4d79-837e-b4b04b52edcd;name:Test Experiment SVM 7b43774e->:\tstop: phase=run\n",
      "INFO: 2019-03-14 16:24:12.803 Experiment<id:Test Experiment SVM 7b43774e->:\tstop: phase=experiment\n"
     ]
    }
   ],
   "source": [
    "# Run experiment with dataset downloaded in the Previous steps from openMl\n",
    "import uuid\n",
    "import pprint\n",
    "from padre.core import Experiment\n",
    "\n",
    "def create_test_pipeline():\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.decomposition import PCA\n",
    "    # estimators = [('reduce_dim', PCA()), ('clf', SVC())]\n",
    "    estimators = [('SVC', SVC(probability=True))]\n",
    "    return Pipeline(estimators)\n",
    "\n",
    "test_experiment_name = \"Test Experiment SVM \" + str(uuid.uuid4())[0:9] # Unique name for experiment\n",
    "ex = Experiment(name=test_experiment_name,\n",
    "                description=\"Testing Support Vector Machines via SKLearn Pipeline\",\n",
    "                dataset=ds,\n",
    "                workflow=create_test_pipeline(), \n",
    "                keep_splits=True, strategy=\"random\")\n",
    "\n",
    "conf = ex.configuration()  # configuration, which has been automatically extracted from the pipeline\n",
    "pprint.pprint(ex.hyperparameters())  # get and print hyperparameters\n",
    "ex.execute()  # run the experiment and report\n",
    "\n",
    "pypadre.metrics_evaluator.add_experiments([ex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: 2019-03-14 16:24:13.877 <padre.experimentcreator.ExperimentCreator object at 0x7ff7d5ec2b70>:\tTest Experiment SVM 7b43774e- created successfully!\n",
      "{'description': 'Testing Support Vector Machines via SKLearn Pipeline', 'dataset': ['diabetes'], 'workflow': ['SVC'], 'strategy': 'random', 'params': {'c-oja pas vector classification': {'C': [1.0], 'tol': [0.001], 'coef0': [0.0], 'gamma': ['auto'], 'degree': [3], 'kernel': ['rbf'], 'verbose': [False], 'shrinking': [True], 'probability': [True], 'class_weight': [None], 'random_state': [None], 'decision_function_shape': ['ovr']}}}\n"
     ]
    }
   ],
   "source": [
    "# Get experiment from server\n",
    "experiments = pypadre.remote_backend.experiments.list_experiments(test_experiment_name)\n",
    "experiment_instance = pypadre.remote_backend.experiments.get_experiment(\"1\")\n",
    "print(experiment_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "var spec = {\"config\": {\"view\": {\"width\": 400, \"height\": 300}}, \"data\": {\"name\": \"data-080bdbcc17a0907eded341ee04dbfc80\"}, \"mark\": \"point\", \"encoding\": {\"color\": {\"type\": \"quantitative\", \"field\": \"class\"}, \"x\": {\"type\": \"quantitative\", \"field\": \"mass\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"age\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v2.6.0.json\", \"datasets\": {\"data-080bdbcc17a0907eded341ee04dbfc80\": [{\"preg\": 6.0, \"plas\": 148.0, \"pres\": 72.0, \"skin\": 35.0, \"insu\": 0.0, \"mass\": 33.6, \"pedi\": 0.627, \"age\": 50.0, \"class\": 1.0}, {\"preg\": 1.0, \"plas\": 85.0, \"pres\": 66.0, \"skin\": 29.0, \"insu\": 0.0, \"mass\": 26.6, \"pedi\": 0.351, \"age\": 31.0, \"class\": 0.0}, {\"preg\": 8.0, \"plas\": 183.0, \"pres\": 64.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 23.3, \"pedi\": 0.672, \"age\": 32.0, \"class\": 1.0}, {\"preg\": 1.0, \"plas\": 89.0, \"pres\": 66.0, \"skin\": 23.0, \"insu\": 94.0, \"mass\": 28.1, \"pedi\": 0.167, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 137.0, \"pres\": 40.0, \"skin\": 35.0, \"insu\": 168.0, \"mass\": 43.1, \"pedi\": 2.288, \"age\": 33.0, \"class\": 1.0}, {\"preg\": 5.0, \"plas\": 116.0, \"pres\": 74.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 25.6, \"pedi\": 0.201, \"age\": 30.0, \"class\": 0.0}, {\"preg\": 3.0, \"plas\": 78.0, \"pres\": 50.0, \"skin\": 32.0, \"insu\": 88.0, \"mass\": 31.0, \"pedi\": 0.248, \"age\": 26.0, \"class\": 1.0}, {\"preg\": 10.0, \"plas\": 115.0, \"pres\": 0.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 35.3, \"pedi\": 0.134, \"age\": 29.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 197.0, \"pres\": 70.0, \"skin\": 45.0, \"insu\": 543.0, \"mass\": 30.5, \"pedi\": 0.158, \"age\": 53.0, \"class\": 1.0}, {\"preg\": 8.0, \"plas\": 125.0, \"pres\": 96.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 0.0, \"pedi\": 0.232, \"age\": 54.0, \"class\": 1.0}, {\"preg\": 4.0, \"plas\": 110.0, \"pres\": 92.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 37.6, \"pedi\": 0.191, \"age\": 30.0, \"class\": 0.0}, {\"preg\": 10.0, \"plas\": 168.0, \"pres\": 74.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 38.0, \"pedi\": 0.537, \"age\": 34.0, \"class\": 1.0}, {\"preg\": 10.0, \"plas\": 139.0, \"pres\": 80.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 27.1, \"pedi\": 1.441, \"age\": 57.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 189.0, \"pres\": 60.0, \"skin\": 23.0, \"insu\": 846.0, \"mass\": 30.1, \"pedi\": 0.398, \"age\": 59.0, \"class\": 1.0}, {\"preg\": 5.0, \"plas\": 166.0, \"pres\": 72.0, \"skin\": 19.0, \"insu\": 175.0, \"mass\": 25.8, \"pedi\": 0.587, \"age\": 51.0, \"class\": 1.0}, {\"preg\": 7.0, \"plas\": 100.0, \"pres\": 0.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 30.0, \"pedi\": 0.484, \"age\": 32.0, \"class\": 1.0}, {\"preg\": 0.0, \"plas\": 118.0, \"pres\": 84.0, \"skin\": 47.0, \"insu\": 230.0, \"mass\": 45.8, \"pedi\": 0.551, \"age\": 31.0, \"class\": 1.0}, {\"preg\": 7.0, \"plas\": 107.0, \"pres\": 74.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 29.6, \"pedi\": 0.254, \"age\": 31.0, \"class\": 1.0}, {\"preg\": 1.0, \"plas\": 103.0, \"pres\": 30.0, \"skin\": 38.0, \"insu\": 83.0, \"mass\": 43.3, \"pedi\": 0.183, \"age\": 33.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 115.0, \"pres\": 70.0, \"skin\": 30.0, \"insu\": 96.0, \"mass\": 34.6, \"pedi\": 0.529, \"age\": 32.0, \"class\": 1.0}, {\"preg\": 3.0, \"plas\": 126.0, \"pres\": 88.0, \"skin\": 41.0, \"insu\": 235.0, \"mass\": 39.3, \"pedi\": 0.704, \"age\": 27.0, \"class\": 0.0}, {\"preg\": 8.0, \"plas\": 99.0, \"pres\": 84.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 35.4, \"pedi\": 0.388, \"age\": 50.0, \"class\": 0.0}, {\"preg\": 7.0, \"plas\": 196.0, \"pres\": 90.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 39.8, \"pedi\": 0.451, \"age\": 41.0, \"class\": 1.0}, {\"preg\": 9.0, \"plas\": 119.0, \"pres\": 80.0, \"skin\": 35.0, \"insu\": 0.0, \"mass\": 29.0, \"pedi\": 0.263, \"age\": 29.0, \"class\": 1.0}, {\"preg\": 11.0, \"plas\": 143.0, \"pres\": 94.0, \"skin\": 33.0, \"insu\": 146.0, \"mass\": 36.6, \"pedi\": 0.254, \"age\": 51.0, \"class\": 1.0}, {\"preg\": 10.0, \"plas\": 125.0, \"pres\": 70.0, \"skin\": 26.0, \"insu\": 115.0, \"mass\": 31.1, \"pedi\": 0.205, \"age\": 41.0, \"class\": 1.0}, {\"preg\": 7.0, \"plas\": 147.0, \"pres\": 76.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 39.4, \"pedi\": 0.257, \"age\": 43.0, \"class\": 1.0}, {\"preg\": 1.0, \"plas\": 97.0, \"pres\": 66.0, \"skin\": 15.0, \"insu\": 140.0, \"mass\": 23.2, \"pedi\": 0.487, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 13.0, \"plas\": 145.0, \"pres\": 82.0, \"skin\": 19.0, \"insu\": 110.0, \"mass\": 22.2, \"pedi\": 0.245, \"age\": 57.0, \"class\": 0.0}, {\"preg\": 5.0, \"plas\": 117.0, \"pres\": 92.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 34.1, \"pedi\": 0.337, \"age\": 38.0, \"class\": 0.0}, {\"preg\": 5.0, \"plas\": 109.0, \"pres\": 75.0, \"skin\": 26.0, \"insu\": 0.0, \"mass\": 36.0, \"pedi\": 0.546, \"age\": 60.0, \"class\": 0.0}, {\"preg\": 3.0, \"plas\": 158.0, \"pres\": 76.0, \"skin\": 36.0, \"insu\": 245.0, \"mass\": 31.6, \"pedi\": 0.851, \"age\": 28.0, \"class\": 1.0}, {\"preg\": 3.0, \"plas\": 88.0, \"pres\": 58.0, \"skin\": 11.0, \"insu\": 54.0, \"mass\": 24.8, \"pedi\": 0.267, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 6.0, \"plas\": 92.0, \"pres\": 92.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 19.9, \"pedi\": 0.188, \"age\": 28.0, \"class\": 0.0}, {\"preg\": 10.0, \"plas\": 122.0, \"pres\": 78.0, \"skin\": 31.0, \"insu\": 0.0, \"mass\": 27.6, \"pedi\": 0.512, \"age\": 45.0, \"class\": 0.0}, {\"preg\": 4.0, \"plas\": 103.0, \"pres\": 60.0, \"skin\": 33.0, \"insu\": 192.0, \"mass\": 24.0, \"pedi\": 0.966, \"age\": 33.0, \"class\": 0.0}, {\"preg\": 11.0, \"plas\": 138.0, \"pres\": 76.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 33.2, \"pedi\": 0.42, \"age\": 35.0, \"class\": 0.0}, {\"preg\": 9.0, \"plas\": 102.0, \"pres\": 76.0, \"skin\": 37.0, \"insu\": 0.0, \"mass\": 32.9, \"pedi\": 0.665, \"age\": 46.0, \"class\": 1.0}, {\"preg\": 2.0, \"plas\": 90.0, \"pres\": 68.0, \"skin\": 42.0, \"insu\": 0.0, \"mass\": 38.2, \"pedi\": 0.503, \"age\": 27.0, \"class\": 1.0}, {\"preg\": 4.0, \"plas\": 111.0, \"pres\": 72.0, \"skin\": 47.0, \"insu\": 207.0, \"mass\": 37.1, \"pedi\": 1.39, \"age\": 56.0, \"class\": 1.0}, {\"preg\": 3.0, \"plas\": 180.0, \"pres\": 64.0, \"skin\": 25.0, \"insu\": 70.0, \"mass\": 34.0, \"pedi\": 0.271, \"age\": 26.0, \"class\": 0.0}, {\"preg\": 7.0, \"plas\": 133.0, \"pres\": 84.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 40.2, \"pedi\": 0.696, \"age\": 37.0, \"class\": 0.0}, {\"preg\": 7.0, \"plas\": 106.0, \"pres\": 92.0, \"skin\": 18.0, \"insu\": 0.0, \"mass\": 22.7, \"pedi\": 0.235, \"age\": 48.0, \"class\": 0.0}, {\"preg\": 9.0, \"plas\": 171.0, \"pres\": 110.0, \"skin\": 24.0, \"insu\": 240.0, \"mass\": 45.4, \"pedi\": 0.721, \"age\": 54.0, \"class\": 1.0}, {\"preg\": 7.0, \"plas\": 159.0, \"pres\": 64.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 27.4, \"pedi\": 0.294, \"age\": 40.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 180.0, \"pres\": 66.0, \"skin\": 39.0, \"insu\": 0.0, \"mass\": 42.0, \"pedi\": 1.893, \"age\": 25.0, \"class\": 1.0}, {\"preg\": 1.0, \"plas\": 146.0, \"pres\": 56.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 29.7, \"pedi\": 0.564, \"age\": 29.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 71.0, \"pres\": 70.0, \"skin\": 27.0, \"insu\": 0.0, \"mass\": 28.0, \"pedi\": 0.586, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 7.0, \"plas\": 103.0, \"pres\": 66.0, \"skin\": 32.0, \"insu\": 0.0, \"mass\": 39.1, \"pedi\": 0.344, \"age\": 31.0, \"class\": 1.0}, {\"preg\": 7.0, \"plas\": 105.0, \"pres\": 0.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 0.0, \"pedi\": 0.305, \"age\": 24.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 103.0, \"pres\": 80.0, \"skin\": 11.0, \"insu\": 82.0, \"mass\": 19.4, \"pedi\": 0.491, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 101.0, \"pres\": 50.0, \"skin\": 15.0, \"insu\": 36.0, \"mass\": 24.2, \"pedi\": 0.526, \"age\": 26.0, \"class\": 0.0}, {\"preg\": 5.0, \"plas\": 88.0, \"pres\": 66.0, \"skin\": 21.0, \"insu\": 23.0, \"mass\": 24.4, \"pedi\": 0.342, \"age\": 30.0, \"class\": 0.0}, {\"preg\": 8.0, \"plas\": 176.0, \"pres\": 90.0, \"skin\": 34.0, \"insu\": 300.0, \"mass\": 33.7, \"pedi\": 0.467, \"age\": 58.0, \"class\": 1.0}, {\"preg\": 7.0, \"plas\": 150.0, \"pres\": 66.0, \"skin\": 42.0, \"insu\": 342.0, \"mass\": 34.7, \"pedi\": 0.718, \"age\": 42.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 73.0, \"pres\": 50.0, \"skin\": 10.0, \"insu\": 0.0, \"mass\": 23.0, \"pedi\": 0.248, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 7.0, \"plas\": 187.0, \"pres\": 68.0, \"skin\": 39.0, \"insu\": 304.0, \"mass\": 37.7, \"pedi\": 0.254, \"age\": 41.0, \"class\": 1.0}, {\"preg\": 0.0, \"plas\": 100.0, \"pres\": 88.0, \"skin\": 60.0, \"insu\": 110.0, \"mass\": 46.8, \"pedi\": 0.962, \"age\": 31.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 146.0, \"pres\": 82.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 40.5, \"pedi\": 1.781, \"age\": 44.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 105.0, \"pres\": 64.0, \"skin\": 41.0, \"insu\": 142.0, \"mass\": 41.5, \"pedi\": 0.173, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 84.0, \"pres\": 0.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 0.0, \"pedi\": 0.304, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 8.0, \"plas\": 133.0, \"pres\": 72.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 32.9, \"pedi\": 0.27, \"age\": 39.0, \"class\": 1.0}, {\"preg\": 5.0, \"plas\": 44.0, \"pres\": 62.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 25.0, \"pedi\": 0.587, \"age\": 36.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 141.0, \"pres\": 58.0, \"skin\": 34.0, \"insu\": 128.0, \"mass\": 25.4, \"pedi\": 0.699, \"age\": 24.0, \"class\": 0.0}, {\"preg\": 7.0, \"plas\": 114.0, \"pres\": 66.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 32.8, \"pedi\": 0.258, \"age\": 42.0, \"class\": 1.0}, {\"preg\": 5.0, \"plas\": 99.0, \"pres\": 74.0, \"skin\": 27.0, \"insu\": 0.0, \"mass\": 29.0, \"pedi\": 0.203, \"age\": 32.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 109.0, \"pres\": 88.0, \"skin\": 30.0, \"insu\": 0.0, \"mass\": 32.5, \"pedi\": 0.855, \"age\": 38.0, \"class\": 1.0}, {\"preg\": 2.0, \"plas\": 109.0, \"pres\": 92.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 42.7, \"pedi\": 0.845, \"age\": 54.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 95.0, \"pres\": 66.0, \"skin\": 13.0, \"insu\": 38.0, \"mass\": 19.6, \"pedi\": 0.334, \"age\": 25.0, \"class\": 0.0}, {\"preg\": 4.0, \"plas\": 146.0, \"pres\": 85.0, \"skin\": 27.0, \"insu\": 100.0, \"mass\": 28.9, \"pedi\": 0.189, \"age\": 27.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 100.0, \"pres\": 66.0, \"skin\": 20.0, \"insu\": 90.0, \"mass\": 32.9, \"pedi\": 0.867, \"age\": 28.0, \"class\": 1.0}, {\"preg\": 5.0, \"plas\": 139.0, \"pres\": 64.0, \"skin\": 35.0, \"insu\": 140.0, \"mass\": 28.6, \"pedi\": 0.411, \"age\": 26.0, \"class\": 0.0}, {\"preg\": 13.0, \"plas\": 126.0, \"pres\": 90.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 43.4, \"pedi\": 0.583, \"age\": 42.0, \"class\": 1.0}, {\"preg\": 4.0, \"plas\": 129.0, \"pres\": 86.0, \"skin\": 20.0, \"insu\": 270.0, \"mass\": 35.1, \"pedi\": 0.231, \"age\": 23.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 79.0, \"pres\": 75.0, \"skin\": 30.0, \"insu\": 0.0, \"mass\": 32.0, \"pedi\": 0.396, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 0.0, \"pres\": 48.0, \"skin\": 20.0, \"insu\": 0.0, \"mass\": 24.7, \"pedi\": 0.14, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 7.0, \"plas\": 62.0, \"pres\": 78.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 32.6, \"pedi\": 0.391, \"age\": 41.0, \"class\": 0.0}, {\"preg\": 5.0, \"plas\": 95.0, \"pres\": 72.0, \"skin\": 33.0, \"insu\": 0.0, \"mass\": 37.7, \"pedi\": 0.37, \"age\": 27.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 131.0, \"pres\": 0.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 43.2, \"pedi\": 0.27, \"age\": 26.0, \"class\": 1.0}, {\"preg\": 2.0, \"plas\": 112.0, \"pres\": 66.0, \"skin\": 22.0, \"insu\": 0.0, \"mass\": 25.0, \"pedi\": 0.307, \"age\": 24.0, \"class\": 0.0}, {\"preg\": 3.0, \"plas\": 113.0, \"pres\": 44.0, \"skin\": 13.0, \"insu\": 0.0, \"mass\": 22.4, \"pedi\": 0.14, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 74.0, \"pres\": 0.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 0.0, \"pedi\": 0.102, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 7.0, \"plas\": 83.0, \"pres\": 78.0, \"skin\": 26.0, \"insu\": 71.0, \"mass\": 29.3, \"pedi\": 0.767, \"age\": 36.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 101.0, \"pres\": 65.0, \"skin\": 28.0, \"insu\": 0.0, \"mass\": 24.6, \"pedi\": 0.237, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 5.0, \"plas\": 137.0, \"pres\": 108.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 48.8, \"pedi\": 0.227, \"age\": 37.0, \"class\": 1.0}, {\"preg\": 2.0, \"plas\": 110.0, \"pres\": 74.0, \"skin\": 29.0, \"insu\": 125.0, \"mass\": 32.4, \"pedi\": 0.698, \"age\": 27.0, \"class\": 0.0}, {\"preg\": 13.0, \"plas\": 106.0, \"pres\": 72.0, \"skin\": 54.0, \"insu\": 0.0, \"mass\": 36.6, \"pedi\": 0.178, \"age\": 45.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 100.0, \"pres\": 68.0, \"skin\": 25.0, \"insu\": 71.0, \"mass\": 38.5, \"pedi\": 0.324, \"age\": 26.0, \"class\": 0.0}, {\"preg\": 15.0, \"plas\": 136.0, \"pres\": 70.0, \"skin\": 32.0, \"insu\": 110.0, \"mass\": 37.1, \"pedi\": 0.153, \"age\": 43.0, \"class\": 1.0}, {\"preg\": 1.0, \"plas\": 107.0, \"pres\": 68.0, \"skin\": 19.0, \"insu\": 0.0, \"mass\": 26.5, \"pedi\": 0.165, \"age\": 24.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 80.0, \"pres\": 55.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 19.1, \"pedi\": 0.258, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 4.0, \"plas\": 123.0, \"pres\": 80.0, \"skin\": 15.0, \"insu\": 176.0, \"mass\": 32.0, \"pedi\": 0.443, \"age\": 34.0, \"class\": 0.0}, {\"preg\": 7.0, \"plas\": 81.0, \"pres\": 78.0, \"skin\": 40.0, \"insu\": 48.0, \"mass\": 46.7, \"pedi\": 0.261, \"age\": 42.0, \"class\": 0.0}, {\"preg\": 4.0, \"plas\": 134.0, \"pres\": 72.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 23.8, \"pedi\": 0.277, \"age\": 60.0, \"class\": 1.0}, {\"preg\": 2.0, \"plas\": 142.0, \"pres\": 82.0, \"skin\": 18.0, \"insu\": 64.0, \"mass\": 24.7, \"pedi\": 0.761, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 6.0, \"plas\": 144.0, \"pres\": 72.0, \"skin\": 27.0, \"insu\": 228.0, \"mass\": 33.9, \"pedi\": 0.255, \"age\": 40.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 92.0, \"pres\": 62.0, \"skin\": 28.0, \"insu\": 0.0, \"mass\": 31.6, \"pedi\": 0.13, \"age\": 24.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 71.0, \"pres\": 48.0, \"skin\": 18.0, \"insu\": 76.0, \"mass\": 20.4, \"pedi\": 0.323, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 6.0, \"plas\": 93.0, \"pres\": 50.0, \"skin\": 30.0, \"insu\": 64.0, \"mass\": 28.7, \"pedi\": 0.356, \"age\": 23.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 122.0, \"pres\": 90.0, \"skin\": 51.0, \"insu\": 220.0, \"mass\": 49.7, \"pedi\": 0.325, \"age\": 31.0, \"class\": 1.0}, {\"preg\": 1.0, \"plas\": 163.0, \"pres\": 72.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 39.0, \"pedi\": 1.222, \"age\": 33.0, \"class\": 1.0}, {\"preg\": 1.0, \"plas\": 151.0, \"pres\": 60.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 26.1, \"pedi\": 0.179, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 125.0, \"pres\": 96.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 22.5, \"pedi\": 0.262, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 81.0, \"pres\": 72.0, \"skin\": 18.0, \"insu\": 40.0, \"mass\": 26.6, \"pedi\": 0.283, \"age\": 24.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 85.0, \"pres\": 65.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 39.6, \"pedi\": 0.93, \"age\": 27.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 126.0, \"pres\": 56.0, \"skin\": 29.0, \"insu\": 152.0, \"mass\": 28.7, \"pedi\": 0.801, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 96.0, \"pres\": 122.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 22.4, \"pedi\": 0.207, \"age\": 27.0, \"class\": 0.0}, {\"preg\": 4.0, \"plas\": 144.0, \"pres\": 58.0, \"skin\": 28.0, \"insu\": 140.0, \"mass\": 29.5, \"pedi\": 0.287, \"age\": 37.0, \"class\": 0.0}, {\"preg\": 3.0, \"plas\": 83.0, \"pres\": 58.0, \"skin\": 31.0, \"insu\": 18.0, \"mass\": 34.3, \"pedi\": 0.336, \"age\": 25.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 95.0, \"pres\": 85.0, \"skin\": 25.0, \"insu\": 36.0, \"mass\": 37.4, \"pedi\": 0.247, \"age\": 24.0, \"class\": 1.0}, {\"preg\": 3.0, \"plas\": 171.0, \"pres\": 72.0, \"skin\": 33.0, \"insu\": 135.0, \"mass\": 33.3, \"pedi\": 0.199, \"age\": 24.0, \"class\": 1.0}, {\"preg\": 8.0, \"plas\": 155.0, \"pres\": 62.0, \"skin\": 26.0, \"insu\": 495.0, \"mass\": 34.0, \"pedi\": 0.543, \"age\": 46.0, \"class\": 1.0}, {\"preg\": 1.0, \"plas\": 89.0, \"pres\": 76.0, \"skin\": 34.0, \"insu\": 37.0, \"mass\": 31.2, \"pedi\": 0.192, \"age\": 23.0, \"class\": 0.0}, {\"preg\": 4.0, \"plas\": 76.0, \"pres\": 62.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 34.0, \"pedi\": 0.391, \"age\": 25.0, \"class\": 0.0}, {\"preg\": 7.0, \"plas\": 160.0, \"pres\": 54.0, \"skin\": 32.0, \"insu\": 175.0, \"mass\": 30.5, \"pedi\": 0.588, \"age\": 39.0, \"class\": 1.0}, {\"preg\": 4.0, \"plas\": 146.0, \"pres\": 92.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 31.2, \"pedi\": 0.539, \"age\": 61.0, \"class\": 1.0}, {\"preg\": 5.0, \"plas\": 124.0, \"pres\": 74.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 34.0, \"pedi\": 0.22, \"age\": 38.0, \"class\": 1.0}, {\"preg\": 5.0, \"plas\": 78.0, \"pres\": 48.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 33.7, \"pedi\": 0.654, \"age\": 25.0, \"class\": 0.0}, {\"preg\": 4.0, \"plas\": 97.0, \"pres\": 60.0, \"skin\": 23.0, \"insu\": 0.0, \"mass\": 28.2, \"pedi\": 0.443, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 4.0, \"plas\": 99.0, \"pres\": 76.0, \"skin\": 15.0, \"insu\": 51.0, \"mass\": 23.2, \"pedi\": 0.223, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 162.0, \"pres\": 76.0, \"skin\": 56.0, \"insu\": 100.0, \"mass\": 53.2, \"pedi\": 0.759, \"age\": 25.0, \"class\": 1.0}, {\"preg\": 6.0, \"plas\": 111.0, \"pres\": 64.0, \"skin\": 39.0, \"insu\": 0.0, \"mass\": 34.2, \"pedi\": 0.26, \"age\": 24.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 107.0, \"pres\": 74.0, \"skin\": 30.0, \"insu\": 100.0, \"mass\": 33.6, \"pedi\": 0.404, \"age\": 23.0, \"class\": 0.0}, {\"preg\": 5.0, \"plas\": 132.0, \"pres\": 80.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 26.8, \"pedi\": 0.186, \"age\": 69.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 113.0, \"pres\": 76.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 33.3, \"pedi\": 0.278, \"age\": 23.0, \"class\": 1.0}, {\"preg\": 1.0, \"plas\": 88.0, \"pres\": 30.0, \"skin\": 42.0, \"insu\": 99.0, \"mass\": 55.0, \"pedi\": 0.496, \"age\": 26.0, \"class\": 1.0}, {\"preg\": 3.0, \"plas\": 120.0, \"pres\": 70.0, \"skin\": 30.0, \"insu\": 135.0, \"mass\": 42.9, \"pedi\": 0.452, \"age\": 30.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 118.0, \"pres\": 58.0, \"skin\": 36.0, \"insu\": 94.0, \"mass\": 33.3, \"pedi\": 0.261, \"age\": 23.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 117.0, \"pres\": 88.0, \"skin\": 24.0, \"insu\": 145.0, \"mass\": 34.5, \"pedi\": 0.403, \"age\": 40.0, \"class\": 1.0}, {\"preg\": 0.0, \"plas\": 105.0, \"pres\": 84.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 27.9, \"pedi\": 0.741, \"age\": 62.0, \"class\": 1.0}, {\"preg\": 4.0, \"plas\": 173.0, \"pres\": 70.0, \"skin\": 14.0, \"insu\": 168.0, \"mass\": 29.7, \"pedi\": 0.361, \"age\": 33.0, \"class\": 1.0}, {\"preg\": 9.0, \"plas\": 122.0, \"pres\": 56.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 33.3, \"pedi\": 1.114, \"age\": 33.0, \"class\": 1.0}, {\"preg\": 3.0, \"plas\": 170.0, \"pres\": 64.0, \"skin\": 37.0, \"insu\": 225.0, \"mass\": 34.5, \"pedi\": 0.356, \"age\": 30.0, \"class\": 1.0}, {\"preg\": 8.0, \"plas\": 84.0, \"pres\": 74.0, \"skin\": 31.0, \"insu\": 0.0, \"mass\": 38.3, \"pedi\": 0.457, \"age\": 39.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 96.0, \"pres\": 68.0, \"skin\": 13.0, \"insu\": 49.0, \"mass\": 21.1, \"pedi\": 0.647, \"age\": 26.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 125.0, \"pres\": 60.0, \"skin\": 20.0, \"insu\": 140.0, \"mass\": 33.8, \"pedi\": 0.088, \"age\": 31.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 100.0, \"pres\": 70.0, \"skin\": 26.0, \"insu\": 50.0, \"mass\": 30.8, \"pedi\": 0.597, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 93.0, \"pres\": 60.0, \"skin\": 25.0, \"insu\": 92.0, \"mass\": 28.7, \"pedi\": 0.532, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 129.0, \"pres\": 80.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 31.2, \"pedi\": 0.703, \"age\": 29.0, \"class\": 0.0}, {\"preg\": 5.0, \"plas\": 105.0, \"pres\": 72.0, \"skin\": 29.0, \"insu\": 325.0, \"mass\": 36.9, \"pedi\": 0.159, \"age\": 28.0, \"class\": 0.0}, {\"preg\": 3.0, \"plas\": 128.0, \"pres\": 78.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 21.1, \"pedi\": 0.268, \"age\": 55.0, \"class\": 0.0}, {\"preg\": 5.0, \"plas\": 106.0, \"pres\": 82.0, \"skin\": 30.0, \"insu\": 0.0, \"mass\": 39.5, \"pedi\": 0.286, \"age\": 38.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 108.0, \"pres\": 52.0, \"skin\": 26.0, \"insu\": 63.0, \"mass\": 32.5, \"pedi\": 0.318, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 10.0, \"plas\": 108.0, \"pres\": 66.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 32.4, \"pedi\": 0.272, \"age\": 42.0, \"class\": 1.0}, {\"preg\": 4.0, \"plas\": 154.0, \"pres\": 62.0, \"skin\": 31.0, \"insu\": 284.0, \"mass\": 32.8, \"pedi\": 0.237, \"age\": 23.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 102.0, \"pres\": 75.0, \"skin\": 23.0, \"insu\": 0.0, \"mass\": 0.0, \"pedi\": 0.572, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 9.0, \"plas\": 57.0, \"pres\": 80.0, \"skin\": 37.0, \"insu\": 0.0, \"mass\": 32.8, \"pedi\": 0.096, \"age\": 41.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 106.0, \"pres\": 64.0, \"skin\": 35.0, \"insu\": 119.0, \"mass\": 30.5, \"pedi\": 1.4, \"age\": 34.0, \"class\": 0.0}, {\"preg\": 5.0, \"plas\": 147.0, \"pres\": 78.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 33.7, \"pedi\": 0.218, \"age\": 65.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 90.0, \"pres\": 70.0, \"skin\": 17.0, \"insu\": 0.0, \"mass\": 27.3, \"pedi\": 0.085, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 136.0, \"pres\": 74.0, \"skin\": 50.0, \"insu\": 204.0, \"mass\": 37.4, \"pedi\": 0.399, \"age\": 24.0, \"class\": 0.0}, {\"preg\": 4.0, \"plas\": 114.0, \"pres\": 65.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 21.9, \"pedi\": 0.432, \"age\": 37.0, \"class\": 0.0}, {\"preg\": 9.0, \"plas\": 156.0, \"pres\": 86.0, \"skin\": 28.0, \"insu\": 155.0, \"mass\": 34.3, \"pedi\": 1.189, \"age\": 42.0, \"class\": 1.0}, {\"preg\": 1.0, \"plas\": 153.0, \"pres\": 82.0, \"skin\": 42.0, \"insu\": 485.0, \"mass\": 40.6, \"pedi\": 0.687, \"age\": 23.0, \"class\": 0.0}, {\"preg\": 8.0, \"plas\": 188.0, \"pres\": 78.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 47.9, \"pedi\": 0.137, \"age\": 43.0, \"class\": 1.0}, {\"preg\": 7.0, \"plas\": 152.0, \"pres\": 88.0, \"skin\": 44.0, \"insu\": 0.0, \"mass\": 50.0, \"pedi\": 0.337, \"age\": 36.0, \"class\": 1.0}, {\"preg\": 2.0, \"plas\": 99.0, \"pres\": 52.0, \"skin\": 15.0, \"insu\": 94.0, \"mass\": 24.6, \"pedi\": 0.637, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 109.0, \"pres\": 56.0, \"skin\": 21.0, \"insu\": 135.0, \"mass\": 25.2, \"pedi\": 0.833, \"age\": 23.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 88.0, \"pres\": 74.0, \"skin\": 19.0, \"insu\": 53.0, \"mass\": 29.0, \"pedi\": 0.229, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 17.0, \"plas\": 163.0, \"pres\": 72.0, \"skin\": 41.0, \"insu\": 114.0, \"mass\": 40.9, \"pedi\": 0.817, \"age\": 47.0, \"class\": 1.0}, {\"preg\": 4.0, \"plas\": 151.0, \"pres\": 90.0, \"skin\": 38.0, \"insu\": 0.0, \"mass\": 29.7, \"pedi\": 0.294, \"age\": 36.0, \"class\": 0.0}, {\"preg\": 7.0, \"plas\": 102.0, \"pres\": 74.0, \"skin\": 40.0, \"insu\": 105.0, \"mass\": 37.2, \"pedi\": 0.204, \"age\": 45.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 114.0, \"pres\": 80.0, \"skin\": 34.0, \"insu\": 285.0, \"mass\": 44.2, \"pedi\": 0.167, \"age\": 27.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 100.0, \"pres\": 64.0, \"skin\": 23.0, \"insu\": 0.0, \"mass\": 29.7, \"pedi\": 0.368, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 131.0, \"pres\": 88.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 31.6, \"pedi\": 0.743, \"age\": 32.0, \"class\": 1.0}, {\"preg\": 6.0, \"plas\": 104.0, \"pres\": 74.0, \"skin\": 18.0, \"insu\": 156.0, \"mass\": 29.9, \"pedi\": 0.722, \"age\": 41.0, \"class\": 1.0}, {\"preg\": 3.0, \"plas\": 148.0, \"pres\": 66.0, \"skin\": 25.0, \"insu\": 0.0, \"mass\": 32.5, \"pedi\": 0.256, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 4.0, \"plas\": 120.0, \"pres\": 68.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 29.6, \"pedi\": 0.709, \"age\": 34.0, \"class\": 0.0}, {\"preg\": 4.0, \"plas\": 110.0, \"pres\": 66.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 31.9, \"pedi\": 0.471, \"age\": 29.0, \"class\": 0.0}, {\"preg\": 3.0, \"plas\": 111.0, \"pres\": 90.0, \"skin\": 12.0, \"insu\": 78.0, \"mass\": 28.4, \"pedi\": 0.495, \"age\": 29.0, \"class\": 0.0}, {\"preg\": 6.0, \"plas\": 102.0, \"pres\": 82.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 30.8, \"pedi\": 0.18, \"age\": 36.0, \"class\": 1.0}, {\"preg\": 6.0, \"plas\": 134.0, \"pres\": 70.0, \"skin\": 23.0, \"insu\": 130.0, \"mass\": 35.4, \"pedi\": 0.542, \"age\": 29.0, \"class\": 1.0}, {\"preg\": 2.0, \"plas\": 87.0, \"pres\": 0.0, \"skin\": 23.0, \"insu\": 0.0, \"mass\": 28.9, \"pedi\": 0.773, \"age\": 25.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 79.0, \"pres\": 60.0, \"skin\": 42.0, \"insu\": 48.0, \"mass\": 43.5, \"pedi\": 0.678, \"age\": 23.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 75.0, \"pres\": 64.0, \"skin\": 24.0, \"insu\": 55.0, \"mass\": 29.7, \"pedi\": 0.37, \"age\": 33.0, \"class\": 0.0}, {\"preg\": 8.0, \"plas\": 179.0, \"pres\": 72.0, \"skin\": 42.0, \"insu\": 130.0, \"mass\": 32.7, \"pedi\": 0.719, \"age\": 36.0, \"class\": 1.0}, {\"preg\": 6.0, \"plas\": 85.0, \"pres\": 78.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 31.2, \"pedi\": 0.382, \"age\": 42.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 129.0, \"pres\": 110.0, \"skin\": 46.0, \"insu\": 130.0, \"mass\": 67.1, \"pedi\": 0.319, \"age\": 26.0, \"class\": 1.0}, {\"preg\": 5.0, \"plas\": 143.0, \"pres\": 78.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 45.0, \"pedi\": 0.19, \"age\": 47.0, \"class\": 0.0}, {\"preg\": 5.0, \"plas\": 130.0, \"pres\": 82.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 39.1, \"pedi\": 0.956, \"age\": 37.0, \"class\": 1.0}, {\"preg\": 6.0, \"plas\": 87.0, \"pres\": 80.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 23.2, \"pedi\": 0.084, \"age\": 32.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 119.0, \"pres\": 64.0, \"skin\": 18.0, \"insu\": 92.0, \"mass\": 34.9, \"pedi\": 0.725, \"age\": 23.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 0.0, \"pres\": 74.0, \"skin\": 20.0, \"insu\": 23.0, \"mass\": 27.7, \"pedi\": 0.299, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 5.0, \"plas\": 73.0, \"pres\": 60.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 26.8, \"pedi\": 0.268, \"age\": 27.0, \"class\": 0.0}, {\"preg\": 4.0, \"plas\": 141.0, \"pres\": 74.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 27.6, \"pedi\": 0.244, \"age\": 40.0, \"class\": 0.0}, {\"preg\": 7.0, \"plas\": 194.0, \"pres\": 68.0, \"skin\": 28.0, \"insu\": 0.0, \"mass\": 35.9, \"pedi\": 0.745, \"age\": 41.0, \"class\": 1.0}, {\"preg\": 8.0, \"plas\": 181.0, \"pres\": 68.0, \"skin\": 36.0, \"insu\": 495.0, \"mass\": 30.1, \"pedi\": 0.615, \"age\": 60.0, \"class\": 1.0}, {\"preg\": 1.0, \"plas\": 128.0, \"pres\": 98.0, \"skin\": 41.0, \"insu\": 58.0, \"mass\": 32.0, \"pedi\": 1.321, \"age\": 33.0, \"class\": 1.0}, {\"preg\": 8.0, \"plas\": 109.0, \"pres\": 76.0, \"skin\": 39.0, \"insu\": 114.0, \"mass\": 27.9, \"pedi\": 0.64, \"age\": 31.0, \"class\": 1.0}, {\"preg\": 5.0, \"plas\": 139.0, \"pres\": 80.0, \"skin\": 35.0, \"insu\": 160.0, \"mass\": 31.6, \"pedi\": 0.361, \"age\": 25.0, \"class\": 1.0}, {\"preg\": 3.0, \"plas\": 111.0, \"pres\": 62.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 22.6, \"pedi\": 0.142, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 9.0, \"plas\": 123.0, \"pres\": 70.0, \"skin\": 44.0, \"insu\": 94.0, \"mass\": 33.1, \"pedi\": 0.374, \"age\": 40.0, \"class\": 0.0}, {\"preg\": 7.0, \"plas\": 159.0, \"pres\": 66.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 30.4, \"pedi\": 0.383, \"age\": 36.0, \"class\": 1.0}, {\"preg\": 11.0, \"plas\": 135.0, \"pres\": 0.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 52.3, \"pedi\": 0.578, \"age\": 40.0, \"class\": 1.0}, {\"preg\": 8.0, \"plas\": 85.0, \"pres\": 55.0, \"skin\": 20.0, \"insu\": 0.0, \"mass\": 24.4, \"pedi\": 0.136, \"age\": 42.0, \"class\": 0.0}, {\"preg\": 5.0, \"plas\": 158.0, \"pres\": 84.0, \"skin\": 41.0, \"insu\": 210.0, \"mass\": 39.4, \"pedi\": 0.395, \"age\": 29.0, \"class\": 1.0}, {\"preg\": 1.0, \"plas\": 105.0, \"pres\": 58.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 24.3, \"pedi\": 0.187, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 3.0, \"plas\": 107.0, \"pres\": 62.0, \"skin\": 13.0, \"insu\": 48.0, \"mass\": 22.9, \"pedi\": 0.678, \"age\": 23.0, \"class\": 1.0}, {\"preg\": 4.0, \"plas\": 109.0, \"pres\": 64.0, \"skin\": 44.0, \"insu\": 99.0, \"mass\": 34.8, \"pedi\": 0.905, \"age\": 26.0, \"class\": 1.0}, {\"preg\": 4.0, \"plas\": 148.0, \"pres\": 60.0, \"skin\": 27.0, \"insu\": 318.0, \"mass\": 30.9, \"pedi\": 0.15, \"age\": 29.0, \"class\": 1.0}, {\"preg\": 0.0, \"plas\": 113.0, \"pres\": 80.0, \"skin\": 16.0, \"insu\": 0.0, \"mass\": 31.0, \"pedi\": 0.874, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 138.0, \"pres\": 82.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 40.1, \"pedi\": 0.236, \"age\": 28.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 108.0, \"pres\": 68.0, \"skin\": 20.0, \"insu\": 0.0, \"mass\": 27.3, \"pedi\": 0.787, \"age\": 32.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 99.0, \"pres\": 70.0, \"skin\": 16.0, \"insu\": 44.0, \"mass\": 20.4, \"pedi\": 0.235, \"age\": 27.0, \"class\": 0.0}, {\"preg\": 6.0, \"plas\": 103.0, \"pres\": 72.0, \"skin\": 32.0, \"insu\": 190.0, \"mass\": 37.7, \"pedi\": 0.324, \"age\": 55.0, \"class\": 0.0}, {\"preg\": 5.0, \"plas\": 111.0, \"pres\": 72.0, \"skin\": 28.0, \"insu\": 0.0, \"mass\": 23.9, \"pedi\": 0.407, \"age\": 27.0, \"class\": 0.0}, {\"preg\": 8.0, \"plas\": 196.0, \"pres\": 76.0, \"skin\": 29.0, \"insu\": 280.0, \"mass\": 37.5, \"pedi\": 0.605, \"age\": 57.0, \"class\": 1.0}, {\"preg\": 5.0, \"plas\": 162.0, \"pres\": 104.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 37.7, \"pedi\": 0.151, \"age\": 52.0, \"class\": 1.0}, {\"preg\": 1.0, \"plas\": 96.0, \"pres\": 64.0, \"skin\": 27.0, \"insu\": 87.0, \"mass\": 33.2, \"pedi\": 0.289, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 7.0, \"plas\": 184.0, \"pres\": 84.0, \"skin\": 33.0, \"insu\": 0.0, \"mass\": 35.5, \"pedi\": 0.355, \"age\": 41.0, \"class\": 1.0}, {\"preg\": 2.0, \"plas\": 81.0, \"pres\": 60.0, \"skin\": 22.0, \"insu\": 0.0, \"mass\": 27.7, \"pedi\": 0.29, \"age\": 25.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 147.0, \"pres\": 85.0, \"skin\": 54.0, \"insu\": 0.0, \"mass\": 42.8, \"pedi\": 0.375, \"age\": 24.0, \"class\": 0.0}, {\"preg\": 7.0, \"plas\": 179.0, \"pres\": 95.0, \"skin\": 31.0, \"insu\": 0.0, \"mass\": 34.2, \"pedi\": 0.164, \"age\": 60.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 140.0, \"pres\": 65.0, \"skin\": 26.0, \"insu\": 130.0, \"mass\": 42.6, \"pedi\": 0.431, \"age\": 24.0, \"class\": 1.0}, {\"preg\": 9.0, \"plas\": 112.0, \"pres\": 82.0, \"skin\": 32.0, \"insu\": 175.0, \"mass\": 34.2, \"pedi\": 0.26, \"age\": 36.0, \"class\": 1.0}, {\"preg\": 12.0, \"plas\": 151.0, \"pres\": 70.0, \"skin\": 40.0, \"insu\": 271.0, \"mass\": 41.8, \"pedi\": 0.742, \"age\": 38.0, \"class\": 1.0}, {\"preg\": 5.0, \"plas\": 109.0, \"pres\": 62.0, \"skin\": 41.0, \"insu\": 129.0, \"mass\": 35.8, \"pedi\": 0.514, \"age\": 25.0, \"class\": 1.0}, {\"preg\": 6.0, \"plas\": 125.0, \"pres\": 68.0, \"skin\": 30.0, \"insu\": 120.0, \"mass\": 30.0, \"pedi\": 0.464, \"age\": 32.0, \"class\": 0.0}, {\"preg\": 5.0, \"plas\": 85.0, \"pres\": 74.0, \"skin\": 22.0, \"insu\": 0.0, \"mass\": 29.0, \"pedi\": 1.224, \"age\": 32.0, \"class\": 1.0}, {\"preg\": 5.0, \"plas\": 112.0, \"pres\": 66.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 37.8, \"pedi\": 0.261, \"age\": 41.0, \"class\": 1.0}, {\"preg\": 0.0, \"plas\": 177.0, \"pres\": 60.0, \"skin\": 29.0, \"insu\": 478.0, \"mass\": 34.6, \"pedi\": 1.072, \"age\": 21.0, \"class\": 1.0}, {\"preg\": 2.0, \"plas\": 158.0, \"pres\": 90.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 31.6, \"pedi\": 0.805, \"age\": 66.0, \"class\": 1.0}, {\"preg\": 7.0, \"plas\": 119.0, \"pres\": 0.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 25.2, \"pedi\": 0.209, \"age\": 37.0, \"class\": 0.0}, {\"preg\": 7.0, \"plas\": 142.0, \"pres\": 60.0, \"skin\": 33.0, \"insu\": 190.0, \"mass\": 28.8, \"pedi\": 0.687, \"age\": 61.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 100.0, \"pres\": 66.0, \"skin\": 15.0, \"insu\": 56.0, \"mass\": 23.6, \"pedi\": 0.666, \"age\": 26.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 87.0, \"pres\": 78.0, \"skin\": 27.0, \"insu\": 32.0, \"mass\": 34.6, \"pedi\": 0.101, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 101.0, \"pres\": 76.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 35.7, \"pedi\": 0.198, \"age\": 26.0, \"class\": 0.0}, {\"preg\": 3.0, \"plas\": 162.0, \"pres\": 52.0, \"skin\": 38.0, \"insu\": 0.0, \"mass\": 37.2, \"pedi\": 0.652, \"age\": 24.0, \"class\": 1.0}, {\"preg\": 4.0, \"plas\": 197.0, \"pres\": 70.0, \"skin\": 39.0, \"insu\": 744.0, \"mass\": 36.7, \"pedi\": 2.329, \"age\": 31.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 117.0, \"pres\": 80.0, \"skin\": 31.0, \"insu\": 53.0, \"mass\": 45.2, \"pedi\": 0.089, \"age\": 24.0, \"class\": 0.0}, {\"preg\": 4.0, \"plas\": 142.0, \"pres\": 86.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 44.0, \"pedi\": 0.645, \"age\": 22.0, \"class\": 1.0}, {\"preg\": 6.0, \"plas\": 134.0, \"pres\": 80.0, \"skin\": 37.0, \"insu\": 370.0, \"mass\": 46.2, \"pedi\": 0.238, \"age\": 46.0, \"class\": 1.0}, {\"preg\": 1.0, \"plas\": 79.0, \"pres\": 80.0, \"skin\": 25.0, \"insu\": 37.0, \"mass\": 25.4, \"pedi\": 0.583, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 4.0, \"plas\": 122.0, \"pres\": 68.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 35.0, \"pedi\": 0.394, \"age\": 29.0, \"class\": 0.0}, {\"preg\": 3.0, \"plas\": 74.0, \"pres\": 68.0, \"skin\": 28.0, \"insu\": 45.0, \"mass\": 29.7, \"pedi\": 0.293, \"age\": 23.0, \"class\": 0.0}, {\"preg\": 4.0, \"plas\": 171.0, \"pres\": 72.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 43.6, \"pedi\": 0.479, \"age\": 26.0, \"class\": 1.0}, {\"preg\": 7.0, \"plas\": 181.0, \"pres\": 84.0, \"skin\": 21.0, \"insu\": 192.0, \"mass\": 35.9, \"pedi\": 0.586, \"age\": 51.0, \"class\": 1.0}, {\"preg\": 0.0, \"plas\": 179.0, \"pres\": 90.0, \"skin\": 27.0, \"insu\": 0.0, \"mass\": 44.1, \"pedi\": 0.686, \"age\": 23.0, \"class\": 1.0}, {\"preg\": 9.0, \"plas\": 164.0, \"pres\": 84.0, \"skin\": 21.0, \"insu\": 0.0, \"mass\": 30.8, \"pedi\": 0.831, \"age\": 32.0, \"class\": 1.0}, {\"preg\": 0.0, \"plas\": 104.0, \"pres\": 76.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 18.4, \"pedi\": 0.582, \"age\": 27.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 91.0, \"pres\": 64.0, \"skin\": 24.0, \"insu\": 0.0, \"mass\": 29.2, \"pedi\": 0.192, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 4.0, \"plas\": 91.0, \"pres\": 70.0, \"skin\": 32.0, \"insu\": 88.0, \"mass\": 33.1, \"pedi\": 0.446, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 3.0, \"plas\": 139.0, \"pres\": 54.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 25.6, \"pedi\": 0.402, \"age\": 22.0, \"class\": 1.0}, {\"preg\": 6.0, \"plas\": 119.0, \"pres\": 50.0, \"skin\": 22.0, \"insu\": 176.0, \"mass\": 27.1, \"pedi\": 1.318, \"age\": 33.0, \"class\": 1.0}, {\"preg\": 2.0, \"plas\": 146.0, \"pres\": 76.0, \"skin\": 35.0, \"insu\": 194.0, \"mass\": 38.2, \"pedi\": 0.329, \"age\": 29.0, \"class\": 0.0}, {\"preg\": 9.0, \"plas\": 184.0, \"pres\": 85.0, \"skin\": 15.0, \"insu\": 0.0, \"mass\": 30.0, \"pedi\": 1.213, \"age\": 49.0, \"class\": 1.0}, {\"preg\": 10.0, \"plas\": 122.0, \"pres\": 68.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 31.2, \"pedi\": 0.258, \"age\": 41.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 165.0, \"pres\": 90.0, \"skin\": 33.0, \"insu\": 680.0, \"mass\": 52.3, \"pedi\": 0.427, \"age\": 23.0, \"class\": 0.0}, {\"preg\": 9.0, \"plas\": 124.0, \"pres\": 70.0, \"skin\": 33.0, \"insu\": 402.0, \"mass\": 35.4, \"pedi\": 0.282, \"age\": 34.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 111.0, \"pres\": 86.0, \"skin\": 19.0, \"insu\": 0.0, \"mass\": 30.1, \"pedi\": 0.143, \"age\": 23.0, \"class\": 0.0}, {\"preg\": 9.0, \"plas\": 106.0, \"pres\": 52.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 31.2, \"pedi\": 0.38, \"age\": 42.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 129.0, \"pres\": 84.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 28.0, \"pedi\": 0.284, \"age\": 27.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 90.0, \"pres\": 80.0, \"skin\": 14.0, \"insu\": 55.0, \"mass\": 24.4, \"pedi\": 0.249, \"age\": 24.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 86.0, \"pres\": 68.0, \"skin\": 32.0, \"insu\": 0.0, \"mass\": 35.8, \"pedi\": 0.238, \"age\": 25.0, \"class\": 0.0}, {\"preg\": 12.0, \"plas\": 92.0, \"pres\": 62.0, \"skin\": 7.0, \"insu\": 258.0, \"mass\": 27.6, \"pedi\": 0.926, \"age\": 44.0, \"class\": 1.0}, {\"preg\": 1.0, \"plas\": 113.0, \"pres\": 64.0, \"skin\": 35.0, \"insu\": 0.0, \"mass\": 33.6, \"pedi\": 0.543, \"age\": 21.0, \"class\": 1.0}, {\"preg\": 3.0, \"plas\": 111.0, \"pres\": 56.0, \"skin\": 39.0, \"insu\": 0.0, \"mass\": 30.1, \"pedi\": 0.557, \"age\": 30.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 114.0, \"pres\": 68.0, \"skin\": 22.0, \"insu\": 0.0, \"mass\": 28.7, \"pedi\": 0.092, \"age\": 25.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 193.0, \"pres\": 50.0, \"skin\": 16.0, \"insu\": 375.0, \"mass\": 25.9, \"pedi\": 0.655, \"age\": 24.0, \"class\": 0.0}, {\"preg\": 11.0, \"plas\": 155.0, \"pres\": 76.0, \"skin\": 28.0, \"insu\": 150.0, \"mass\": 33.3, \"pedi\": 1.353, \"age\": 51.0, \"class\": 1.0}, {\"preg\": 3.0, \"plas\": 191.0, \"pres\": 68.0, \"skin\": 15.0, \"insu\": 130.0, \"mass\": 30.9, \"pedi\": 0.299, \"age\": 34.0, \"class\": 0.0}, {\"preg\": 3.0, \"plas\": 141.0, \"pres\": 0.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 30.0, \"pedi\": 0.761, \"age\": 27.0, \"class\": 1.0}, {\"preg\": 4.0, \"plas\": 95.0, \"pres\": 70.0, \"skin\": 32.0, \"insu\": 0.0, \"mass\": 32.1, \"pedi\": 0.612, \"age\": 24.0, \"class\": 0.0}, {\"preg\": 3.0, \"plas\": 142.0, \"pres\": 80.0, \"skin\": 15.0, \"insu\": 0.0, \"mass\": 32.4, \"pedi\": 0.2, \"age\": 63.0, \"class\": 0.0}, {\"preg\": 4.0, \"plas\": 123.0, \"pres\": 62.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 32.0, \"pedi\": 0.226, \"age\": 35.0, \"class\": 1.0}, {\"preg\": 5.0, \"plas\": 96.0, \"pres\": 74.0, \"skin\": 18.0, \"insu\": 67.0, \"mass\": 33.6, \"pedi\": 0.997, \"age\": 43.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 138.0, \"pres\": 0.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 36.3, \"pedi\": 0.933, \"age\": 25.0, \"class\": 1.0}, {\"preg\": 2.0, \"plas\": 128.0, \"pres\": 64.0, \"skin\": 42.0, \"insu\": 0.0, \"mass\": 40.0, \"pedi\": 1.101, \"age\": 24.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 102.0, \"pres\": 52.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 25.1, \"pedi\": 0.078, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 146.0, \"pres\": 0.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 27.5, \"pedi\": 0.24, \"age\": 28.0, \"class\": 1.0}, {\"preg\": 10.0, \"plas\": 101.0, \"pres\": 86.0, \"skin\": 37.0, \"insu\": 0.0, \"mass\": 45.6, \"pedi\": 1.136, \"age\": 38.0, \"class\": 1.0}, {\"preg\": 2.0, \"plas\": 108.0, \"pres\": 62.0, \"skin\": 32.0, \"insu\": 56.0, \"mass\": 25.2, \"pedi\": 0.128, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 3.0, \"plas\": 122.0, \"pres\": 78.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 23.0, \"pedi\": 0.254, \"age\": 40.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 71.0, \"pres\": 78.0, \"skin\": 50.0, \"insu\": 45.0, \"mass\": 33.2, \"pedi\": 0.422, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 13.0, \"plas\": 106.0, \"pres\": 70.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 34.2, \"pedi\": 0.251, \"age\": 52.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 100.0, \"pres\": 70.0, \"skin\": 52.0, \"insu\": 57.0, \"mass\": 40.5, \"pedi\": 0.677, \"age\": 25.0, \"class\": 0.0}, {\"preg\": 7.0, \"plas\": 106.0, \"pres\": 60.0, \"skin\": 24.0, \"insu\": 0.0, \"mass\": 26.5, \"pedi\": 0.296, \"age\": 29.0, \"class\": 1.0}, {\"preg\": 0.0, \"plas\": 104.0, \"pres\": 64.0, \"skin\": 23.0, \"insu\": 116.0, \"mass\": 27.8, \"pedi\": 0.454, \"age\": 23.0, \"class\": 0.0}, {\"preg\": 5.0, \"plas\": 114.0, \"pres\": 74.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 24.9, \"pedi\": 0.744, \"age\": 57.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 108.0, \"pres\": 62.0, \"skin\": 10.0, \"insu\": 278.0, \"mass\": 25.3, \"pedi\": 0.881, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 146.0, \"pres\": 70.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 37.9, \"pedi\": 0.334, \"age\": 28.0, \"class\": 1.0}, {\"preg\": 10.0, \"plas\": 129.0, \"pres\": 76.0, \"skin\": 28.0, \"insu\": 122.0, \"mass\": 35.9, \"pedi\": 0.28, \"age\": 39.0, \"class\": 0.0}, {\"preg\": 7.0, \"plas\": 133.0, \"pres\": 88.0, \"skin\": 15.0, \"insu\": 155.0, \"mass\": 32.4, \"pedi\": 0.262, \"age\": 37.0, \"class\": 0.0}, {\"preg\": 7.0, \"plas\": 161.0, \"pres\": 86.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 30.4, \"pedi\": 0.165, \"age\": 47.0, \"class\": 1.0}, {\"preg\": 2.0, \"plas\": 108.0, \"pres\": 80.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 27.0, \"pedi\": 0.259, \"age\": 52.0, \"class\": 1.0}, {\"preg\": 7.0, \"plas\": 136.0, \"pres\": 74.0, \"skin\": 26.0, \"insu\": 135.0, \"mass\": 26.0, \"pedi\": 0.647, \"age\": 51.0, \"class\": 0.0}, {\"preg\": 5.0, \"plas\": 155.0, \"pres\": 84.0, \"skin\": 44.0, \"insu\": 545.0, \"mass\": 38.7, \"pedi\": 0.619, \"age\": 34.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 119.0, \"pres\": 86.0, \"skin\": 39.0, \"insu\": 220.0, \"mass\": 45.6, \"pedi\": 0.808, \"age\": 29.0, \"class\": 1.0}, {\"preg\": 4.0, \"plas\": 96.0, \"pres\": 56.0, \"skin\": 17.0, \"insu\": 49.0, \"mass\": 20.8, \"pedi\": 0.34, \"age\": 26.0, \"class\": 0.0}, {\"preg\": 5.0, \"plas\": 108.0, \"pres\": 72.0, \"skin\": 43.0, \"insu\": 75.0, \"mass\": 36.1, \"pedi\": 0.263, \"age\": 33.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 78.0, \"pres\": 88.0, \"skin\": 29.0, \"insu\": 40.0, \"mass\": 36.9, \"pedi\": 0.434, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 107.0, \"pres\": 62.0, \"skin\": 30.0, \"insu\": 74.0, \"mass\": 36.6, \"pedi\": 0.757, \"age\": 25.0, \"class\": 1.0}, {\"preg\": 2.0, \"plas\": 128.0, \"pres\": 78.0, \"skin\": 37.0, \"insu\": 182.0, \"mass\": 43.3, \"pedi\": 1.224, \"age\": 31.0, \"class\": 1.0}, {\"preg\": 1.0, \"plas\": 128.0, \"pres\": 48.0, \"skin\": 45.0, \"insu\": 194.0, \"mass\": 40.5, \"pedi\": 0.613, \"age\": 24.0, \"class\": 1.0}, {\"preg\": 0.0, \"plas\": 161.0, \"pres\": 50.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 21.9, \"pedi\": 0.254, \"age\": 65.0, \"class\": 0.0}, {\"preg\": 6.0, \"plas\": 151.0, \"pres\": 62.0, \"skin\": 31.0, \"insu\": 120.0, \"mass\": 35.5, \"pedi\": 0.692, \"age\": 28.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 146.0, \"pres\": 70.0, \"skin\": 38.0, \"insu\": 360.0, \"mass\": 28.0, \"pedi\": 0.337, \"age\": 29.0, \"class\": 1.0}, {\"preg\": 0.0, \"plas\": 126.0, \"pres\": 84.0, \"skin\": 29.0, \"insu\": 215.0, \"mass\": 30.7, \"pedi\": 0.52, \"age\": 24.0, \"class\": 0.0}, {\"preg\": 14.0, \"plas\": 100.0, \"pres\": 78.0, \"skin\": 25.0, \"insu\": 184.0, \"mass\": 36.6, \"pedi\": 0.412, \"age\": 46.0, \"class\": 1.0}, {\"preg\": 8.0, \"plas\": 112.0, \"pres\": 72.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 23.6, \"pedi\": 0.84, \"age\": 58.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 167.0, \"pres\": 0.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 32.3, \"pedi\": 0.839, \"age\": 30.0, \"class\": 1.0}, {\"preg\": 2.0, \"plas\": 144.0, \"pres\": 58.0, \"skin\": 33.0, \"insu\": 135.0, \"mass\": 31.6, \"pedi\": 0.422, \"age\": 25.0, \"class\": 1.0}, {\"preg\": 5.0, \"plas\": 77.0, \"pres\": 82.0, \"skin\": 41.0, \"insu\": 42.0, \"mass\": 35.8, \"pedi\": 0.156, \"age\": 35.0, \"class\": 0.0}, {\"preg\": 5.0, \"plas\": 115.0, \"pres\": 98.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 52.9, \"pedi\": 0.209, \"age\": 28.0, \"class\": 1.0}, {\"preg\": 3.0, \"plas\": 150.0, \"pres\": 76.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 21.0, \"pedi\": 0.207, \"age\": 37.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 120.0, \"pres\": 76.0, \"skin\": 37.0, \"insu\": 105.0, \"mass\": 39.7, \"pedi\": 0.215, \"age\": 29.0, \"class\": 0.0}, {\"preg\": 10.0, \"plas\": 161.0, \"pres\": 68.0, \"skin\": 23.0, \"insu\": 132.0, \"mass\": 25.5, \"pedi\": 0.326, \"age\": 47.0, \"class\": 1.0}, {\"preg\": 0.0, \"plas\": 137.0, \"pres\": 68.0, \"skin\": 14.0, \"insu\": 148.0, \"mass\": 24.8, \"pedi\": 0.143, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 128.0, \"pres\": 68.0, \"skin\": 19.0, \"insu\": 180.0, \"mass\": 30.5, \"pedi\": 1.391, \"age\": 25.0, \"class\": 1.0}, {\"preg\": 2.0, \"plas\": 124.0, \"pres\": 68.0, \"skin\": 28.0, \"insu\": 205.0, \"mass\": 32.9, \"pedi\": 0.875, \"age\": 30.0, \"class\": 1.0}, {\"preg\": 6.0, \"plas\": 80.0, \"pres\": 66.0, \"skin\": 30.0, \"insu\": 0.0, \"mass\": 26.2, \"pedi\": 0.313, \"age\": 41.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 106.0, \"pres\": 70.0, \"skin\": 37.0, \"insu\": 148.0, \"mass\": 39.4, \"pedi\": 0.605, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 155.0, \"pres\": 74.0, \"skin\": 17.0, \"insu\": 96.0, \"mass\": 26.6, \"pedi\": 0.433, \"age\": 27.0, \"class\": 1.0}, {\"preg\": 3.0, \"plas\": 113.0, \"pres\": 50.0, \"skin\": 10.0, \"insu\": 85.0, \"mass\": 29.5, \"pedi\": 0.626, \"age\": 25.0, \"class\": 0.0}, {\"preg\": 7.0, \"plas\": 109.0, \"pres\": 80.0, \"skin\": 31.0, \"insu\": 0.0, \"mass\": 35.9, \"pedi\": 1.127, \"age\": 43.0, \"class\": 1.0}, {\"preg\": 2.0, \"plas\": 112.0, \"pres\": 68.0, \"skin\": 22.0, \"insu\": 94.0, \"mass\": 34.1, \"pedi\": 0.315, \"age\": 26.0, \"class\": 0.0}, {\"preg\": 3.0, \"plas\": 99.0, \"pres\": 80.0, \"skin\": 11.0, \"insu\": 64.0, \"mass\": 19.3, \"pedi\": 0.284, \"age\": 30.0, \"class\": 0.0}, {\"preg\": 3.0, \"plas\": 182.0, \"pres\": 74.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 30.5, \"pedi\": 0.345, \"age\": 29.0, \"class\": 1.0}, {\"preg\": 3.0, \"plas\": 115.0, \"pres\": 66.0, \"skin\": 39.0, \"insu\": 140.0, \"mass\": 38.1, \"pedi\": 0.15, \"age\": 28.0, \"class\": 0.0}, {\"preg\": 6.0, \"plas\": 194.0, \"pres\": 78.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 23.5, \"pedi\": 0.129, \"age\": 59.0, \"class\": 1.0}, {\"preg\": 4.0, \"plas\": 129.0, \"pres\": 60.0, \"skin\": 12.0, \"insu\": 231.0, \"mass\": 27.5, \"pedi\": 0.527, \"age\": 31.0, \"class\": 0.0}, {\"preg\": 3.0, \"plas\": 112.0, \"pres\": 74.0, \"skin\": 30.0, \"insu\": 0.0, \"mass\": 31.6, \"pedi\": 0.197, \"age\": 25.0, \"class\": 1.0}, {\"preg\": 0.0, \"plas\": 124.0, \"pres\": 70.0, \"skin\": 20.0, \"insu\": 0.0, \"mass\": 27.4, \"pedi\": 0.254, \"age\": 36.0, \"class\": 1.0}, {\"preg\": 13.0, \"plas\": 152.0, \"pres\": 90.0, \"skin\": 33.0, \"insu\": 29.0, \"mass\": 26.8, \"pedi\": 0.731, \"age\": 43.0, \"class\": 1.0}, {\"preg\": 2.0, \"plas\": 112.0, \"pres\": 75.0, \"skin\": 32.0, \"insu\": 0.0, \"mass\": 35.7, \"pedi\": 0.148, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 157.0, \"pres\": 72.0, \"skin\": 21.0, \"insu\": 168.0, \"mass\": 25.6, \"pedi\": 0.123, \"age\": 24.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 122.0, \"pres\": 64.0, \"skin\": 32.0, \"insu\": 156.0, \"mass\": 35.1, \"pedi\": 0.692, \"age\": 30.0, \"class\": 1.0}, {\"preg\": 10.0, \"plas\": 179.0, \"pres\": 70.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 35.1, \"pedi\": 0.2, \"age\": 37.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 102.0, \"pres\": 86.0, \"skin\": 36.0, \"insu\": 120.0, \"mass\": 45.5, \"pedi\": 0.127, \"age\": 23.0, \"class\": 1.0}, {\"preg\": 6.0, \"plas\": 105.0, \"pres\": 70.0, \"skin\": 32.0, \"insu\": 68.0, \"mass\": 30.8, \"pedi\": 0.122, \"age\": 37.0, \"class\": 0.0}, {\"preg\": 8.0, \"plas\": 118.0, \"pres\": 72.0, \"skin\": 19.0, \"insu\": 0.0, \"mass\": 23.1, \"pedi\": 1.476, \"age\": 46.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 87.0, \"pres\": 58.0, \"skin\": 16.0, \"insu\": 52.0, \"mass\": 32.7, \"pedi\": 0.166, \"age\": 25.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 180.0, \"pres\": 0.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 43.3, \"pedi\": 0.282, \"age\": 41.0, \"class\": 1.0}, {\"preg\": 12.0, \"plas\": 106.0, \"pres\": 80.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 23.6, \"pedi\": 0.137, \"age\": 44.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 95.0, \"pres\": 60.0, \"skin\": 18.0, \"insu\": 58.0, \"mass\": 23.9, \"pedi\": 0.26, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 165.0, \"pres\": 76.0, \"skin\": 43.0, \"insu\": 255.0, \"mass\": 47.9, \"pedi\": 0.259, \"age\": 26.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 117.0, \"pres\": 0.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 33.8, \"pedi\": 0.932, \"age\": 44.0, \"class\": 0.0}, {\"preg\": 5.0, \"plas\": 115.0, \"pres\": 76.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 31.2, \"pedi\": 0.343, \"age\": 44.0, \"class\": 1.0}, {\"preg\": 9.0, \"plas\": 152.0, \"pres\": 78.0, \"skin\": 34.0, \"insu\": 171.0, \"mass\": 34.2, \"pedi\": 0.893, \"age\": 33.0, \"class\": 1.0}, {\"preg\": 7.0, \"plas\": 178.0, \"pres\": 84.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 39.9, \"pedi\": 0.331, \"age\": 41.0, \"class\": 1.0}, {\"preg\": 1.0, \"plas\": 130.0, \"pres\": 70.0, \"skin\": 13.0, \"insu\": 105.0, \"mass\": 25.9, \"pedi\": 0.472, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 95.0, \"pres\": 74.0, \"skin\": 21.0, \"insu\": 73.0, \"mass\": 25.9, \"pedi\": 0.673, \"age\": 36.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 0.0, \"pres\": 68.0, \"skin\": 35.0, \"insu\": 0.0, \"mass\": 32.0, \"pedi\": 0.389, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 5.0, \"plas\": 122.0, \"pres\": 86.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 34.7, \"pedi\": 0.29, \"age\": 33.0, \"class\": 0.0}, {\"preg\": 8.0, \"plas\": 95.0, \"pres\": 72.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 36.8, \"pedi\": 0.485, \"age\": 57.0, \"class\": 0.0}, {\"preg\": 8.0, \"plas\": 126.0, \"pres\": 88.0, \"skin\": 36.0, \"insu\": 108.0, \"mass\": 38.5, \"pedi\": 0.349, \"age\": 49.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 139.0, \"pres\": 46.0, \"skin\": 19.0, \"insu\": 83.0, \"mass\": 28.7, \"pedi\": 0.654, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 3.0, \"plas\": 116.0, \"pres\": 0.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 23.5, \"pedi\": 0.187, \"age\": 23.0, \"class\": 0.0}, {\"preg\": 3.0, \"plas\": 99.0, \"pres\": 62.0, \"skin\": 19.0, \"insu\": 74.0, \"mass\": 21.8, \"pedi\": 0.279, \"age\": 26.0, \"class\": 0.0}, {\"preg\": 5.0, \"plas\": 0.0, \"pres\": 80.0, \"skin\": 32.0, \"insu\": 0.0, \"mass\": 41.0, \"pedi\": 0.346, \"age\": 37.0, \"class\": 1.0}, {\"preg\": 4.0, \"plas\": 92.0, \"pres\": 80.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 42.2, \"pedi\": 0.237, \"age\": 29.0, \"class\": 0.0}, {\"preg\": 4.0, \"plas\": 137.0, \"pres\": 84.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 31.2, \"pedi\": 0.252, \"age\": 30.0, \"class\": 0.0}, {\"preg\": 3.0, \"plas\": 61.0, \"pres\": 82.0, \"skin\": 28.0, \"insu\": 0.0, \"mass\": 34.4, \"pedi\": 0.243, \"age\": 46.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 90.0, \"pres\": 62.0, \"skin\": 12.0, \"insu\": 43.0, \"mass\": 27.2, \"pedi\": 0.58, \"age\": 24.0, \"class\": 0.0}, {\"preg\": 3.0, \"plas\": 90.0, \"pres\": 78.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 42.7, \"pedi\": 0.559, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 9.0, \"plas\": 165.0, \"pres\": 88.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 30.4, \"pedi\": 0.302, \"age\": 49.0, \"class\": 1.0}, {\"preg\": 1.0, \"plas\": 125.0, \"pres\": 50.0, \"skin\": 40.0, \"insu\": 167.0, \"mass\": 33.3, \"pedi\": 0.962, \"age\": 28.0, \"class\": 1.0}, {\"preg\": 13.0, \"plas\": 129.0, \"pres\": 0.0, \"skin\": 30.0, \"insu\": 0.0, \"mass\": 39.9, \"pedi\": 0.569, \"age\": 44.0, \"class\": 1.0}, {\"preg\": 12.0, \"plas\": 88.0, \"pres\": 74.0, \"skin\": 40.0, \"insu\": 54.0, \"mass\": 35.3, \"pedi\": 0.378, \"age\": 48.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 196.0, \"pres\": 76.0, \"skin\": 36.0, \"insu\": 249.0, \"mass\": 36.5, \"pedi\": 0.875, \"age\": 29.0, \"class\": 1.0}, {\"preg\": 5.0, \"plas\": 189.0, \"pres\": 64.0, \"skin\": 33.0, \"insu\": 325.0, \"mass\": 31.2, \"pedi\": 0.583, \"age\": 29.0, \"class\": 1.0}, {\"preg\": 5.0, \"plas\": 158.0, \"pres\": 70.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 29.8, \"pedi\": 0.207, \"age\": 63.0, \"class\": 0.0}, {\"preg\": 5.0, \"plas\": 103.0, \"pres\": 108.0, \"skin\": 37.0, \"insu\": 0.0, \"mass\": 39.2, \"pedi\": 0.305, \"age\": 65.0, \"class\": 0.0}, {\"preg\": 4.0, \"plas\": 146.0, \"pres\": 78.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 38.5, \"pedi\": 0.52, \"age\": 67.0, \"class\": 1.0}, {\"preg\": 4.0, \"plas\": 147.0, \"pres\": 74.0, \"skin\": 25.0, \"insu\": 293.0, \"mass\": 34.9, \"pedi\": 0.385, \"age\": 30.0, \"class\": 0.0}, {\"preg\": 5.0, \"plas\": 99.0, \"pres\": 54.0, \"skin\": 28.0, \"insu\": 83.0, \"mass\": 34.0, \"pedi\": 0.499, \"age\": 30.0, \"class\": 0.0}, {\"preg\": 6.0, \"plas\": 124.0, \"pres\": 72.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 27.6, \"pedi\": 0.368, \"age\": 29.0, \"class\": 1.0}, {\"preg\": 0.0, \"plas\": 101.0, \"pres\": 64.0, \"skin\": 17.0, \"insu\": 0.0, \"mass\": 21.0, \"pedi\": 0.252, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 3.0, \"plas\": 81.0, \"pres\": 86.0, \"skin\": 16.0, \"insu\": 66.0, \"mass\": 27.5, \"pedi\": 0.306, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 133.0, \"pres\": 102.0, \"skin\": 28.0, \"insu\": 140.0, \"mass\": 32.8, \"pedi\": 0.234, \"age\": 45.0, \"class\": 1.0}, {\"preg\": 3.0, \"plas\": 173.0, \"pres\": 82.0, \"skin\": 48.0, \"insu\": 465.0, \"mass\": 38.4, \"pedi\": 2.137, \"age\": 25.0, \"class\": 1.0}, {\"preg\": 0.0, \"plas\": 118.0, \"pres\": 64.0, \"skin\": 23.0, \"insu\": 89.0, \"mass\": 0.0, \"pedi\": 1.731, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 84.0, \"pres\": 64.0, \"skin\": 22.0, \"insu\": 66.0, \"mass\": 35.8, \"pedi\": 0.545, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 105.0, \"pres\": 58.0, \"skin\": 40.0, \"insu\": 94.0, \"mass\": 34.9, \"pedi\": 0.225, \"age\": 25.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 122.0, \"pres\": 52.0, \"skin\": 43.0, \"insu\": 158.0, \"mass\": 36.2, \"pedi\": 0.816, \"age\": 28.0, \"class\": 0.0}, {\"preg\": 12.0, \"plas\": 140.0, \"pres\": 82.0, \"skin\": 43.0, \"insu\": 325.0, \"mass\": 39.2, \"pedi\": 0.528, \"age\": 58.0, \"class\": 1.0}, {\"preg\": 0.0, \"plas\": 98.0, \"pres\": 82.0, \"skin\": 15.0, \"insu\": 84.0, \"mass\": 25.2, \"pedi\": 0.299, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 87.0, \"pres\": 60.0, \"skin\": 37.0, \"insu\": 75.0, \"mass\": 37.2, \"pedi\": 0.509, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 4.0, \"plas\": 156.0, \"pres\": 75.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 48.3, \"pedi\": 0.238, \"age\": 32.0, \"class\": 1.0}, {\"preg\": 0.0, \"plas\": 93.0, \"pres\": 100.0, \"skin\": 39.0, \"insu\": 72.0, \"mass\": 43.4, \"pedi\": 1.021, \"age\": 35.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 107.0, \"pres\": 72.0, \"skin\": 30.0, \"insu\": 82.0, \"mass\": 30.8, \"pedi\": 0.821, \"age\": 24.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 105.0, \"pres\": 68.0, \"skin\": 22.0, \"insu\": 0.0, \"mass\": 20.0, \"pedi\": 0.236, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 109.0, \"pres\": 60.0, \"skin\": 8.0, \"insu\": 182.0, \"mass\": 25.4, \"pedi\": 0.947, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 90.0, \"pres\": 62.0, \"skin\": 18.0, \"insu\": 59.0, \"mass\": 25.1, \"pedi\": 1.268, \"age\": 25.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 125.0, \"pres\": 70.0, \"skin\": 24.0, \"insu\": 110.0, \"mass\": 24.3, \"pedi\": 0.221, \"age\": 25.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 119.0, \"pres\": 54.0, \"skin\": 13.0, \"insu\": 50.0, \"mass\": 22.3, \"pedi\": 0.205, \"age\": 24.0, \"class\": 0.0}, {\"preg\": 5.0, \"plas\": 116.0, \"pres\": 74.0, \"skin\": 29.0, \"insu\": 0.0, \"mass\": 32.3, \"pedi\": 0.66, \"age\": 35.0, \"class\": 1.0}, {\"preg\": 8.0, \"plas\": 105.0, \"pres\": 100.0, \"skin\": 36.0, \"insu\": 0.0, \"mass\": 43.3, \"pedi\": 0.239, \"age\": 45.0, \"class\": 1.0}, {\"preg\": 5.0, \"plas\": 144.0, \"pres\": 82.0, \"skin\": 26.0, \"insu\": 285.0, \"mass\": 32.0, \"pedi\": 0.452, \"age\": 58.0, \"class\": 1.0}, {\"preg\": 3.0, \"plas\": 100.0, \"pres\": 68.0, \"skin\": 23.0, \"insu\": 81.0, \"mass\": 31.6, \"pedi\": 0.949, \"age\": 28.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 100.0, \"pres\": 66.0, \"skin\": 29.0, \"insu\": 196.0, \"mass\": 32.0, \"pedi\": 0.444, \"age\": 42.0, \"class\": 0.0}, {\"preg\": 5.0, \"plas\": 166.0, \"pres\": 76.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 45.7, \"pedi\": 0.34, \"age\": 27.0, \"class\": 1.0}, {\"preg\": 1.0, \"plas\": 131.0, \"pres\": 64.0, \"skin\": 14.0, \"insu\": 415.0, \"mass\": 23.7, \"pedi\": 0.389, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 4.0, \"plas\": 116.0, \"pres\": 72.0, \"skin\": 12.0, \"insu\": 87.0, \"mass\": 22.1, \"pedi\": 0.463, \"age\": 37.0, \"class\": 0.0}, {\"preg\": 4.0, \"plas\": 158.0, \"pres\": 78.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 32.9, \"pedi\": 0.803, \"age\": 31.0, \"class\": 1.0}, {\"preg\": 2.0, \"plas\": 127.0, \"pres\": 58.0, \"skin\": 24.0, \"insu\": 275.0, \"mass\": 27.7, \"pedi\": 1.6, \"age\": 25.0, \"class\": 0.0}, {\"preg\": 3.0, \"plas\": 96.0, \"pres\": 56.0, \"skin\": 34.0, \"insu\": 115.0, \"mass\": 24.7, \"pedi\": 0.944, \"age\": 39.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 131.0, \"pres\": 66.0, \"skin\": 40.0, \"insu\": 0.0, \"mass\": 34.3, \"pedi\": 0.196, \"age\": 22.0, \"class\": 1.0}, {\"preg\": 3.0, \"plas\": 82.0, \"pres\": 70.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 21.1, \"pedi\": 0.389, \"age\": 25.0, \"class\": 0.0}, {\"preg\": 3.0, \"plas\": 193.0, \"pres\": 70.0, \"skin\": 31.0, \"insu\": 0.0, \"mass\": 34.9, \"pedi\": 0.241, \"age\": 25.0, \"class\": 1.0}, {\"preg\": 4.0, \"plas\": 95.0, \"pres\": 64.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 32.0, \"pedi\": 0.161, \"age\": 31.0, \"class\": 1.0}, {\"preg\": 6.0, \"plas\": 137.0, \"pres\": 61.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 24.2, \"pedi\": 0.151, \"age\": 55.0, \"class\": 0.0}, {\"preg\": 5.0, \"plas\": 136.0, \"pres\": 84.0, \"skin\": 41.0, \"insu\": 88.0, \"mass\": 35.0, \"pedi\": 0.286, \"age\": 35.0, \"class\": 1.0}, {\"preg\": 9.0, \"plas\": 72.0, \"pres\": 78.0, \"skin\": 25.0, \"insu\": 0.0, \"mass\": 31.6, \"pedi\": 0.28, \"age\": 38.0, \"class\": 0.0}, {\"preg\": 5.0, \"plas\": 168.0, \"pres\": 64.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 32.9, \"pedi\": 0.135, \"age\": 41.0, \"class\": 1.0}, {\"preg\": 2.0, \"plas\": 123.0, \"pres\": 48.0, \"skin\": 32.0, \"insu\": 165.0, \"mass\": 42.1, \"pedi\": 0.52, \"age\": 26.0, \"class\": 0.0}, {\"preg\": 4.0, \"plas\": 115.0, \"pres\": 72.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 28.9, \"pedi\": 0.376, \"age\": 46.0, \"class\": 1.0}, {\"preg\": 0.0, \"plas\": 101.0, \"pres\": 62.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 21.9, \"pedi\": 0.336, \"age\": 25.0, \"class\": 0.0}, {\"preg\": 8.0, \"plas\": 197.0, \"pres\": 74.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 25.9, \"pedi\": 1.191, \"age\": 39.0, \"class\": 1.0}, {\"preg\": 1.0, \"plas\": 172.0, \"pres\": 68.0, \"skin\": 49.0, \"insu\": 579.0, \"mass\": 42.4, \"pedi\": 0.702, \"age\": 28.0, \"class\": 1.0}, {\"preg\": 6.0, \"plas\": 102.0, \"pres\": 90.0, \"skin\": 39.0, \"insu\": 0.0, \"mass\": 35.7, \"pedi\": 0.674, \"age\": 28.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 112.0, \"pres\": 72.0, \"skin\": 30.0, \"insu\": 176.0, \"mass\": 34.4, \"pedi\": 0.528, \"age\": 25.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 143.0, \"pres\": 84.0, \"skin\": 23.0, \"insu\": 310.0, \"mass\": 42.4, \"pedi\": 1.076, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 143.0, \"pres\": 74.0, \"skin\": 22.0, \"insu\": 61.0, \"mass\": 26.2, \"pedi\": 0.256, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 138.0, \"pres\": 60.0, \"skin\": 35.0, \"insu\": 167.0, \"mass\": 34.6, \"pedi\": 0.534, \"age\": 21.0, \"class\": 1.0}, {\"preg\": 3.0, \"plas\": 173.0, \"pres\": 84.0, \"skin\": 33.0, \"insu\": 474.0, \"mass\": 35.7, \"pedi\": 0.258, \"age\": 22.0, \"class\": 1.0}, {\"preg\": 1.0, \"plas\": 97.0, \"pres\": 68.0, \"skin\": 21.0, \"insu\": 0.0, \"mass\": 27.2, \"pedi\": 1.095, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 4.0, \"plas\": 144.0, \"pres\": 82.0, \"skin\": 32.0, \"insu\": 0.0, \"mass\": 38.5, \"pedi\": 0.554, \"age\": 37.0, \"class\": 1.0}, {\"preg\": 1.0, \"plas\": 83.0, \"pres\": 68.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 18.2, \"pedi\": 0.624, \"age\": 27.0, \"class\": 0.0}, {\"preg\": 3.0, \"plas\": 129.0, \"pres\": 64.0, \"skin\": 29.0, \"insu\": 115.0, \"mass\": 26.4, \"pedi\": 0.219, \"age\": 28.0, \"class\": 1.0}, {\"preg\": 1.0, \"plas\": 119.0, \"pres\": 88.0, \"skin\": 41.0, \"insu\": 170.0, \"mass\": 45.3, \"pedi\": 0.507, \"age\": 26.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 94.0, \"pres\": 68.0, \"skin\": 18.0, \"insu\": 76.0, \"mass\": 26.0, \"pedi\": 0.561, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 102.0, \"pres\": 64.0, \"skin\": 46.0, \"insu\": 78.0, \"mass\": 40.6, \"pedi\": 0.496, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 115.0, \"pres\": 64.0, \"skin\": 22.0, \"insu\": 0.0, \"mass\": 30.8, \"pedi\": 0.421, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 8.0, \"plas\": 151.0, \"pres\": 78.0, \"skin\": 32.0, \"insu\": 210.0, \"mass\": 42.9, \"pedi\": 0.516, \"age\": 36.0, \"class\": 1.0}, {\"preg\": 4.0, \"plas\": 184.0, \"pres\": 78.0, \"skin\": 39.0, \"insu\": 277.0, \"mass\": 37.0, \"pedi\": 0.264, \"age\": 31.0, \"class\": 1.0}, {\"preg\": 0.0, \"plas\": 94.0, \"pres\": 0.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 0.0, \"pedi\": 0.256, \"age\": 25.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 181.0, \"pres\": 64.0, \"skin\": 30.0, \"insu\": 180.0, \"mass\": 34.1, \"pedi\": 0.328, \"age\": 38.0, \"class\": 1.0}, {\"preg\": 0.0, \"plas\": 135.0, \"pres\": 94.0, \"skin\": 46.0, \"insu\": 145.0, \"mass\": 40.6, \"pedi\": 0.284, \"age\": 26.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 95.0, \"pres\": 82.0, \"skin\": 25.0, \"insu\": 180.0, \"mass\": 35.0, \"pedi\": 0.233, \"age\": 43.0, \"class\": 1.0}, {\"preg\": 2.0, \"plas\": 99.0, \"pres\": 0.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 22.2, \"pedi\": 0.108, \"age\": 23.0, \"class\": 0.0}, {\"preg\": 3.0, \"plas\": 89.0, \"pres\": 74.0, \"skin\": 16.0, \"insu\": 85.0, \"mass\": 30.4, \"pedi\": 0.551, \"age\": 38.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 80.0, \"pres\": 74.0, \"skin\": 11.0, \"insu\": 60.0, \"mass\": 30.0, \"pedi\": 0.527, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 139.0, \"pres\": 75.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 25.6, \"pedi\": 0.167, \"age\": 29.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 90.0, \"pres\": 68.0, \"skin\": 8.0, \"insu\": 0.0, \"mass\": 24.5, \"pedi\": 1.138, \"age\": 36.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 141.0, \"pres\": 0.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 42.4, \"pedi\": 0.205, \"age\": 29.0, \"class\": 1.0}, {\"preg\": 12.0, \"plas\": 140.0, \"pres\": 85.0, \"skin\": 33.0, \"insu\": 0.0, \"mass\": 37.4, \"pedi\": 0.244, \"age\": 41.0, \"class\": 0.0}, {\"preg\": 5.0, \"plas\": 147.0, \"pres\": 75.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 29.9, \"pedi\": 0.434, \"age\": 28.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 97.0, \"pres\": 70.0, \"skin\": 15.0, \"insu\": 0.0, \"mass\": 18.2, \"pedi\": 0.147, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 6.0, \"plas\": 107.0, \"pres\": 88.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 36.8, \"pedi\": 0.727, \"age\": 31.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 189.0, \"pres\": 104.0, \"skin\": 25.0, \"insu\": 0.0, \"mass\": 34.3, \"pedi\": 0.435, \"age\": 41.0, \"class\": 1.0}, {\"preg\": 2.0, \"plas\": 83.0, \"pres\": 66.0, \"skin\": 23.0, \"insu\": 50.0, \"mass\": 32.2, \"pedi\": 0.497, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 4.0, \"plas\": 117.0, \"pres\": 64.0, \"skin\": 27.0, \"insu\": 120.0, \"mass\": 33.2, \"pedi\": 0.23, \"age\": 24.0, \"class\": 0.0}, {\"preg\": 8.0, \"plas\": 108.0, \"pres\": 70.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 30.5, \"pedi\": 0.955, \"age\": 33.0, \"class\": 1.0}, {\"preg\": 4.0, \"plas\": 117.0, \"pres\": 62.0, \"skin\": 12.0, \"insu\": 0.0, \"mass\": 29.7, \"pedi\": 0.38, \"age\": 30.0, \"class\": 1.0}, {\"preg\": 0.0, \"plas\": 180.0, \"pres\": 78.0, \"skin\": 63.0, \"insu\": 14.0, \"mass\": 59.4, \"pedi\": 2.42, \"age\": 25.0, \"class\": 1.0}, {\"preg\": 1.0, \"plas\": 100.0, \"pres\": 72.0, \"skin\": 12.0, \"insu\": 70.0, \"mass\": 25.3, \"pedi\": 0.658, \"age\": 28.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 95.0, \"pres\": 80.0, \"skin\": 45.0, \"insu\": 92.0, \"mass\": 36.5, \"pedi\": 0.33, \"age\": 26.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 104.0, \"pres\": 64.0, \"skin\": 37.0, \"insu\": 64.0, \"mass\": 33.6, \"pedi\": 0.51, \"age\": 22.0, \"class\": 1.0}, {\"preg\": 0.0, \"plas\": 120.0, \"pres\": 74.0, \"skin\": 18.0, \"insu\": 63.0, \"mass\": 30.5, \"pedi\": 0.285, \"age\": 26.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 82.0, \"pres\": 64.0, \"skin\": 13.0, \"insu\": 95.0, \"mass\": 21.2, \"pedi\": 0.415, \"age\": 23.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 134.0, \"pres\": 70.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 28.9, \"pedi\": 0.542, \"age\": 23.0, \"class\": 1.0}, {\"preg\": 0.0, \"plas\": 91.0, \"pres\": 68.0, \"skin\": 32.0, \"insu\": 210.0, \"mass\": 39.9, \"pedi\": 0.381, \"age\": 25.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 119.0, \"pres\": 0.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 19.6, \"pedi\": 0.832, \"age\": 72.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 100.0, \"pres\": 54.0, \"skin\": 28.0, \"insu\": 105.0, \"mass\": 37.8, \"pedi\": 0.498, \"age\": 24.0, \"class\": 0.0}, {\"preg\": 14.0, \"plas\": 175.0, \"pres\": 62.0, \"skin\": 30.0, \"insu\": 0.0, \"mass\": 33.6, \"pedi\": 0.212, \"age\": 38.0, \"class\": 1.0}, {\"preg\": 1.0, \"plas\": 135.0, \"pres\": 54.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 26.7, \"pedi\": 0.687, \"age\": 62.0, \"class\": 0.0}, {\"preg\": 5.0, \"plas\": 86.0, \"pres\": 68.0, \"skin\": 28.0, \"insu\": 71.0, \"mass\": 30.2, \"pedi\": 0.364, \"age\": 24.0, \"class\": 0.0}, {\"preg\": 10.0, \"plas\": 148.0, \"pres\": 84.0, \"skin\": 48.0, \"insu\": 237.0, \"mass\": 37.6, \"pedi\": 1.001, \"age\": 51.0, \"class\": 1.0}, {\"preg\": 9.0, \"plas\": 134.0, \"pres\": 74.0, \"skin\": 33.0, \"insu\": 60.0, \"mass\": 25.9, \"pedi\": 0.46, \"age\": 81.0, \"class\": 0.0}, {\"preg\": 9.0, \"plas\": 120.0, \"pres\": 72.0, \"skin\": 22.0, \"insu\": 56.0, \"mass\": 20.8, \"pedi\": 0.733, \"age\": 48.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 71.0, \"pres\": 62.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 21.8, \"pedi\": 0.416, \"age\": 26.0, \"class\": 0.0}, {\"preg\": 8.0, \"plas\": 74.0, \"pres\": 70.0, \"skin\": 40.0, \"insu\": 49.0, \"mass\": 35.3, \"pedi\": 0.705, \"age\": 39.0, \"class\": 0.0}, {\"preg\": 5.0, \"plas\": 88.0, \"pres\": 78.0, \"skin\": 30.0, \"insu\": 0.0, \"mass\": 27.6, \"pedi\": 0.258, \"age\": 37.0, \"class\": 0.0}, {\"preg\": 10.0, \"plas\": 115.0, \"pres\": 98.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 24.0, \"pedi\": 1.022, \"age\": 34.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 124.0, \"pres\": 56.0, \"skin\": 13.0, \"insu\": 105.0, \"mass\": 21.8, \"pedi\": 0.452, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 74.0, \"pres\": 52.0, \"skin\": 10.0, \"insu\": 36.0, \"mass\": 27.8, \"pedi\": 0.269, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 97.0, \"pres\": 64.0, \"skin\": 36.0, \"insu\": 100.0, \"mass\": 36.8, \"pedi\": 0.6, \"age\": 25.0, \"class\": 0.0}, {\"preg\": 8.0, \"plas\": 120.0, \"pres\": 0.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 30.0, \"pedi\": 0.183, \"age\": 38.0, \"class\": 1.0}, {\"preg\": 6.0, \"plas\": 154.0, \"pres\": 78.0, \"skin\": 41.0, \"insu\": 140.0, \"mass\": 46.1, \"pedi\": 0.571, \"age\": 27.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 144.0, \"pres\": 82.0, \"skin\": 40.0, \"insu\": 0.0, \"mass\": 41.3, \"pedi\": 0.607, \"age\": 28.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 137.0, \"pres\": 70.0, \"skin\": 38.0, \"insu\": 0.0, \"mass\": 33.2, \"pedi\": 0.17, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 119.0, \"pres\": 66.0, \"skin\": 27.0, \"insu\": 0.0, \"mass\": 38.8, \"pedi\": 0.259, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 7.0, \"plas\": 136.0, \"pres\": 90.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 29.9, \"pedi\": 0.21, \"age\": 50.0, \"class\": 0.0}, {\"preg\": 4.0, \"plas\": 114.0, \"pres\": 64.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 28.9, \"pedi\": 0.126, \"age\": 24.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 137.0, \"pres\": 84.0, \"skin\": 27.0, \"insu\": 0.0, \"mass\": 27.3, \"pedi\": 0.231, \"age\": 59.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 105.0, \"pres\": 80.0, \"skin\": 45.0, \"insu\": 191.0, \"mass\": 33.7, \"pedi\": 0.711, \"age\": 29.0, \"class\": 1.0}, {\"preg\": 7.0, \"plas\": 114.0, \"pres\": 76.0, \"skin\": 17.0, \"insu\": 110.0, \"mass\": 23.8, \"pedi\": 0.466, \"age\": 31.0, \"class\": 0.0}, {\"preg\": 8.0, \"plas\": 126.0, \"pres\": 74.0, \"skin\": 38.0, \"insu\": 75.0, \"mass\": 25.9, \"pedi\": 0.162, \"age\": 39.0, \"class\": 0.0}, {\"preg\": 4.0, \"plas\": 132.0, \"pres\": 86.0, \"skin\": 31.0, \"insu\": 0.0, \"mass\": 28.0, \"pedi\": 0.419, \"age\": 63.0, \"class\": 0.0}, {\"preg\": 3.0, \"plas\": 158.0, \"pres\": 70.0, \"skin\": 30.0, \"insu\": 328.0, \"mass\": 35.5, \"pedi\": 0.344, \"age\": 35.0, \"class\": 1.0}, {\"preg\": 0.0, \"plas\": 123.0, \"pres\": 88.0, \"skin\": 37.0, \"insu\": 0.0, \"mass\": 35.2, \"pedi\": 0.197, \"age\": 29.0, \"class\": 0.0}, {\"preg\": 4.0, \"plas\": 85.0, \"pres\": 58.0, \"skin\": 22.0, \"insu\": 49.0, \"mass\": 27.8, \"pedi\": 0.306, \"age\": 28.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 84.0, \"pres\": 82.0, \"skin\": 31.0, \"insu\": 125.0, \"mass\": 38.2, \"pedi\": 0.233, \"age\": 23.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 145.0, \"pres\": 0.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 44.2, \"pedi\": 0.63, \"age\": 31.0, \"class\": 1.0}, {\"preg\": 0.0, \"plas\": 135.0, \"pres\": 68.0, \"skin\": 42.0, \"insu\": 250.0, \"mass\": 42.3, \"pedi\": 0.365, \"age\": 24.0, \"class\": 1.0}, {\"preg\": 1.0, \"plas\": 139.0, \"pres\": 62.0, \"skin\": 41.0, \"insu\": 480.0, \"mass\": 40.7, \"pedi\": 0.536, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 173.0, \"pres\": 78.0, \"skin\": 32.0, \"insu\": 265.0, \"mass\": 46.5, \"pedi\": 1.159, \"age\": 58.0, \"class\": 0.0}, {\"preg\": 4.0, \"plas\": 99.0, \"pres\": 72.0, \"skin\": 17.0, \"insu\": 0.0, \"mass\": 25.6, \"pedi\": 0.294, \"age\": 28.0, \"class\": 0.0}, {\"preg\": 8.0, \"plas\": 194.0, \"pres\": 80.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 26.1, \"pedi\": 0.551, \"age\": 67.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 83.0, \"pres\": 65.0, \"skin\": 28.0, \"insu\": 66.0, \"mass\": 36.8, \"pedi\": 0.629, \"age\": 24.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 89.0, \"pres\": 90.0, \"skin\": 30.0, \"insu\": 0.0, \"mass\": 33.5, \"pedi\": 0.292, \"age\": 42.0, \"class\": 0.0}, {\"preg\": 4.0, \"plas\": 99.0, \"pres\": 68.0, \"skin\": 38.0, \"insu\": 0.0, \"mass\": 32.8, \"pedi\": 0.145, \"age\": 33.0, \"class\": 0.0}, {\"preg\": 4.0, \"plas\": 125.0, \"pres\": 70.0, \"skin\": 18.0, \"insu\": 122.0, \"mass\": 28.9, \"pedi\": 1.144, \"age\": 45.0, \"class\": 1.0}, {\"preg\": 3.0, \"plas\": 80.0, \"pres\": 0.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 0.0, \"pedi\": 0.174, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 6.0, \"plas\": 166.0, \"pres\": 74.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 26.6, \"pedi\": 0.304, \"age\": 66.0, \"class\": 0.0}, {\"preg\": 5.0, \"plas\": 110.0, \"pres\": 68.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 26.0, \"pedi\": 0.292, \"age\": 30.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 81.0, \"pres\": 72.0, \"skin\": 15.0, \"insu\": 76.0, \"mass\": 30.1, \"pedi\": 0.547, \"age\": 25.0, \"class\": 0.0}, {\"preg\": 7.0, \"plas\": 195.0, \"pres\": 70.0, \"skin\": 33.0, \"insu\": 145.0, \"mass\": 25.1, \"pedi\": 0.163, \"age\": 55.0, \"class\": 1.0}, {\"preg\": 6.0, \"plas\": 154.0, \"pres\": 74.0, \"skin\": 32.0, \"insu\": 193.0, \"mass\": 29.3, \"pedi\": 0.839, \"age\": 39.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 117.0, \"pres\": 90.0, \"skin\": 19.0, \"insu\": 71.0, \"mass\": 25.2, \"pedi\": 0.313, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 3.0, \"plas\": 84.0, \"pres\": 72.0, \"skin\": 32.0, \"insu\": 0.0, \"mass\": 37.2, \"pedi\": 0.267, \"age\": 28.0, \"class\": 0.0}, {\"preg\": 6.0, \"plas\": 0.0, \"pres\": 68.0, \"skin\": 41.0, \"insu\": 0.0, \"mass\": 39.0, \"pedi\": 0.727, \"age\": 41.0, \"class\": 1.0}, {\"preg\": 7.0, \"plas\": 94.0, \"pres\": 64.0, \"skin\": 25.0, \"insu\": 79.0, \"mass\": 33.3, \"pedi\": 0.738, \"age\": 41.0, \"class\": 0.0}, {\"preg\": 3.0, \"plas\": 96.0, \"pres\": 78.0, \"skin\": 39.0, \"insu\": 0.0, \"mass\": 37.3, \"pedi\": 0.238, \"age\": 40.0, \"class\": 0.0}, {\"preg\": 10.0, \"plas\": 75.0, \"pres\": 82.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 33.3, \"pedi\": 0.263, \"age\": 38.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 180.0, \"pres\": 90.0, \"skin\": 26.0, \"insu\": 90.0, \"mass\": 36.5, \"pedi\": 0.314, \"age\": 35.0, \"class\": 1.0}, {\"preg\": 1.0, \"plas\": 130.0, \"pres\": 60.0, \"skin\": 23.0, \"insu\": 170.0, \"mass\": 28.6, \"pedi\": 0.692, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 84.0, \"pres\": 50.0, \"skin\": 23.0, \"insu\": 76.0, \"mass\": 30.4, \"pedi\": 0.968, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 8.0, \"plas\": 120.0, \"pres\": 78.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 25.0, \"pedi\": 0.409, \"age\": 64.0, \"class\": 0.0}, {\"preg\": 12.0, \"plas\": 84.0, \"pres\": 72.0, \"skin\": 31.0, \"insu\": 0.0, \"mass\": 29.7, \"pedi\": 0.297, \"age\": 46.0, \"class\": 1.0}, {\"preg\": 0.0, \"plas\": 139.0, \"pres\": 62.0, \"skin\": 17.0, \"insu\": 210.0, \"mass\": 22.1, \"pedi\": 0.207, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 9.0, \"plas\": 91.0, \"pres\": 68.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 24.2, \"pedi\": 0.2, \"age\": 58.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 91.0, \"pres\": 62.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 27.3, \"pedi\": 0.525, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 3.0, \"plas\": 99.0, \"pres\": 54.0, \"skin\": 19.0, \"insu\": 86.0, \"mass\": 25.6, \"pedi\": 0.154, \"age\": 24.0, \"class\": 0.0}, {\"preg\": 3.0, \"plas\": 163.0, \"pres\": 70.0, \"skin\": 18.0, \"insu\": 105.0, \"mass\": 31.6, \"pedi\": 0.268, \"age\": 28.0, \"class\": 1.0}, {\"preg\": 9.0, \"plas\": 145.0, \"pres\": 88.0, \"skin\": 34.0, \"insu\": 165.0, \"mass\": 30.3, \"pedi\": 0.771, \"age\": 53.0, \"class\": 1.0}, {\"preg\": 7.0, \"plas\": 125.0, \"pres\": 86.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 37.6, \"pedi\": 0.304, \"age\": 51.0, \"class\": 0.0}, {\"preg\": 13.0, \"plas\": 76.0, \"pres\": 60.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 32.8, \"pedi\": 0.18, \"age\": 41.0, \"class\": 0.0}, {\"preg\": 6.0, \"plas\": 129.0, \"pres\": 90.0, \"skin\": 7.0, \"insu\": 326.0, \"mass\": 19.6, \"pedi\": 0.582, \"age\": 60.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 68.0, \"pres\": 70.0, \"skin\": 32.0, \"insu\": 66.0, \"mass\": 25.0, \"pedi\": 0.187, \"age\": 25.0, \"class\": 0.0}, {\"preg\": 3.0, \"plas\": 124.0, \"pres\": 80.0, \"skin\": 33.0, \"insu\": 130.0, \"mass\": 33.2, \"pedi\": 0.305, \"age\": 26.0, \"class\": 0.0}, {\"preg\": 6.0, \"plas\": 114.0, \"pres\": 0.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 0.0, \"pedi\": 0.189, \"age\": 26.0, \"class\": 0.0}, {\"preg\": 9.0, \"plas\": 130.0, \"pres\": 70.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 34.2, \"pedi\": 0.652, \"age\": 45.0, \"class\": 1.0}, {\"preg\": 3.0, \"plas\": 125.0, \"pres\": 58.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 31.6, \"pedi\": 0.151, \"age\": 24.0, \"class\": 0.0}, {\"preg\": 3.0, \"plas\": 87.0, \"pres\": 60.0, \"skin\": 18.0, \"insu\": 0.0, \"mass\": 21.8, \"pedi\": 0.444, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 97.0, \"pres\": 64.0, \"skin\": 19.0, \"insu\": 82.0, \"mass\": 18.2, \"pedi\": 0.299, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 3.0, \"plas\": 116.0, \"pres\": 74.0, \"skin\": 15.0, \"insu\": 105.0, \"mass\": 26.3, \"pedi\": 0.107, \"age\": 24.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 117.0, \"pres\": 66.0, \"skin\": 31.0, \"insu\": 188.0, \"mass\": 30.8, \"pedi\": 0.493, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 111.0, \"pres\": 65.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 24.6, \"pedi\": 0.66, \"age\": 31.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 122.0, \"pres\": 60.0, \"skin\": 18.0, \"insu\": 106.0, \"mass\": 29.8, \"pedi\": 0.717, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 107.0, \"pres\": 76.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 45.3, \"pedi\": 0.686, \"age\": 24.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 86.0, \"pres\": 66.0, \"skin\": 52.0, \"insu\": 65.0, \"mass\": 41.3, \"pedi\": 0.917, \"age\": 29.0, \"class\": 0.0}, {\"preg\": 6.0, \"plas\": 91.0, \"pres\": 0.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 29.8, \"pedi\": 0.501, \"age\": 31.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 77.0, \"pres\": 56.0, \"skin\": 30.0, \"insu\": 56.0, \"mass\": 33.3, \"pedi\": 1.251, \"age\": 24.0, \"class\": 0.0}, {\"preg\": 4.0, \"plas\": 132.0, \"pres\": 0.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 32.9, \"pedi\": 0.302, \"age\": 23.0, \"class\": 1.0}, {\"preg\": 0.0, \"plas\": 105.0, \"pres\": 90.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 29.6, \"pedi\": 0.197, \"age\": 46.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 57.0, \"pres\": 60.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 21.7, \"pedi\": 0.735, \"age\": 67.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 127.0, \"pres\": 80.0, \"skin\": 37.0, \"insu\": 210.0, \"mass\": 36.3, \"pedi\": 0.804, \"age\": 23.0, \"class\": 0.0}, {\"preg\": 3.0, \"plas\": 129.0, \"pres\": 92.0, \"skin\": 49.0, \"insu\": 155.0, \"mass\": 36.4, \"pedi\": 0.968, \"age\": 32.0, \"class\": 1.0}, {\"preg\": 8.0, \"plas\": 100.0, \"pres\": 74.0, \"skin\": 40.0, \"insu\": 215.0, \"mass\": 39.4, \"pedi\": 0.661, \"age\": 43.0, \"class\": 1.0}, {\"preg\": 3.0, \"plas\": 128.0, \"pres\": 72.0, \"skin\": 25.0, \"insu\": 190.0, \"mass\": 32.4, \"pedi\": 0.549, \"age\": 27.0, \"class\": 1.0}, {\"preg\": 10.0, \"plas\": 90.0, \"pres\": 85.0, \"skin\": 32.0, \"insu\": 0.0, \"mass\": 34.9, \"pedi\": 0.825, \"age\": 56.0, \"class\": 1.0}, {\"preg\": 4.0, \"plas\": 84.0, \"pres\": 90.0, \"skin\": 23.0, \"insu\": 56.0, \"mass\": 39.5, \"pedi\": 0.159, \"age\": 25.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 88.0, \"pres\": 78.0, \"skin\": 29.0, \"insu\": 76.0, \"mass\": 32.0, \"pedi\": 0.365, \"age\": 29.0, \"class\": 0.0}, {\"preg\": 8.0, \"plas\": 186.0, \"pres\": 90.0, \"skin\": 35.0, \"insu\": 225.0, \"mass\": 34.5, \"pedi\": 0.423, \"age\": 37.0, \"class\": 1.0}, {\"preg\": 5.0, \"plas\": 187.0, \"pres\": 76.0, \"skin\": 27.0, \"insu\": 207.0, \"mass\": 43.6, \"pedi\": 1.034, \"age\": 53.0, \"class\": 1.0}, {\"preg\": 4.0, \"plas\": 131.0, \"pres\": 68.0, \"skin\": 21.0, \"insu\": 166.0, \"mass\": 33.1, \"pedi\": 0.16, \"age\": 28.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 164.0, \"pres\": 82.0, \"skin\": 43.0, \"insu\": 67.0, \"mass\": 32.8, \"pedi\": 0.341, \"age\": 50.0, \"class\": 0.0}, {\"preg\": 4.0, \"plas\": 189.0, \"pres\": 110.0, \"skin\": 31.0, \"insu\": 0.0, \"mass\": 28.5, \"pedi\": 0.68, \"age\": 37.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 116.0, \"pres\": 70.0, \"skin\": 28.0, \"insu\": 0.0, \"mass\": 27.4, \"pedi\": 0.204, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 3.0, \"plas\": 84.0, \"pres\": 68.0, \"skin\": 30.0, \"insu\": 106.0, \"mass\": 31.9, \"pedi\": 0.591, \"age\": 25.0, \"class\": 0.0}, {\"preg\": 6.0, \"plas\": 114.0, \"pres\": 88.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 27.8, \"pedi\": 0.247, \"age\": 66.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 88.0, \"pres\": 62.0, \"skin\": 24.0, \"insu\": 44.0, \"mass\": 29.9, \"pedi\": 0.422, \"age\": 23.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 84.0, \"pres\": 64.0, \"skin\": 23.0, \"insu\": 115.0, \"mass\": 36.9, \"pedi\": 0.471, \"age\": 28.0, \"class\": 0.0}, {\"preg\": 7.0, \"plas\": 124.0, \"pres\": 70.0, \"skin\": 33.0, \"insu\": 215.0, \"mass\": 25.5, \"pedi\": 0.161, \"age\": 37.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 97.0, \"pres\": 70.0, \"skin\": 40.0, \"insu\": 0.0, \"mass\": 38.1, \"pedi\": 0.218, \"age\": 30.0, \"class\": 0.0}, {\"preg\": 8.0, \"plas\": 110.0, \"pres\": 76.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 27.8, \"pedi\": 0.237, \"age\": 58.0, \"class\": 0.0}, {\"preg\": 11.0, \"plas\": 103.0, \"pres\": 68.0, \"skin\": 40.0, \"insu\": 0.0, \"mass\": 46.2, \"pedi\": 0.126, \"age\": 42.0, \"class\": 0.0}, {\"preg\": 11.0, \"plas\": 85.0, \"pres\": 74.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 30.1, \"pedi\": 0.3, \"age\": 35.0, \"class\": 0.0}, {\"preg\": 6.0, \"plas\": 125.0, \"pres\": 76.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 33.8, \"pedi\": 0.121, \"age\": 54.0, \"class\": 1.0}, {\"preg\": 0.0, \"plas\": 198.0, \"pres\": 66.0, \"skin\": 32.0, \"insu\": 274.0, \"mass\": 41.3, \"pedi\": 0.502, \"age\": 28.0, \"class\": 1.0}, {\"preg\": 1.0, \"plas\": 87.0, \"pres\": 68.0, \"skin\": 34.0, \"insu\": 77.0, \"mass\": 37.6, \"pedi\": 0.401, \"age\": 24.0, \"class\": 0.0}, {\"preg\": 6.0, \"plas\": 99.0, \"pres\": 60.0, \"skin\": 19.0, \"insu\": 54.0, \"mass\": 26.9, \"pedi\": 0.497, \"age\": 32.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 91.0, \"pres\": 80.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 32.4, \"pedi\": 0.601, \"age\": 27.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 95.0, \"pres\": 54.0, \"skin\": 14.0, \"insu\": 88.0, \"mass\": 26.1, \"pedi\": 0.748, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 99.0, \"pres\": 72.0, \"skin\": 30.0, \"insu\": 18.0, \"mass\": 38.6, \"pedi\": 0.412, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 6.0, \"plas\": 92.0, \"pres\": 62.0, \"skin\": 32.0, \"insu\": 126.0, \"mass\": 32.0, \"pedi\": 0.085, \"age\": 46.0, \"class\": 0.0}, {\"preg\": 4.0, \"plas\": 154.0, \"pres\": 72.0, \"skin\": 29.0, \"insu\": 126.0, \"mass\": 31.3, \"pedi\": 0.338, \"age\": 37.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 121.0, \"pres\": 66.0, \"skin\": 30.0, \"insu\": 165.0, \"mass\": 34.3, \"pedi\": 0.203, \"age\": 33.0, \"class\": 1.0}, {\"preg\": 3.0, \"plas\": 78.0, \"pres\": 70.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 32.5, \"pedi\": 0.27, \"age\": 39.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 130.0, \"pres\": 96.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 22.6, \"pedi\": 0.268, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 3.0, \"plas\": 111.0, \"pres\": 58.0, \"skin\": 31.0, \"insu\": 44.0, \"mass\": 29.5, \"pedi\": 0.43, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 98.0, \"pres\": 60.0, \"skin\": 17.0, \"insu\": 120.0, \"mass\": 34.7, \"pedi\": 0.198, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 143.0, \"pres\": 86.0, \"skin\": 30.0, \"insu\": 330.0, \"mass\": 30.1, \"pedi\": 0.892, \"age\": 23.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 119.0, \"pres\": 44.0, \"skin\": 47.0, \"insu\": 63.0, \"mass\": 35.5, \"pedi\": 0.28, \"age\": 25.0, \"class\": 0.0}, {\"preg\": 6.0, \"plas\": 108.0, \"pres\": 44.0, \"skin\": 20.0, \"insu\": 130.0, \"mass\": 24.0, \"pedi\": 0.813, \"age\": 35.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 118.0, \"pres\": 80.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 42.9, \"pedi\": 0.693, \"age\": 21.0, \"class\": 1.0}, {\"preg\": 10.0, \"plas\": 133.0, \"pres\": 68.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 27.0, \"pedi\": 0.245, \"age\": 36.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 197.0, \"pres\": 70.0, \"skin\": 99.0, \"insu\": 0.0, \"mass\": 34.7, \"pedi\": 0.575, \"age\": 62.0, \"class\": 1.0}, {\"preg\": 0.0, \"plas\": 151.0, \"pres\": 90.0, \"skin\": 46.0, \"insu\": 0.0, \"mass\": 42.1, \"pedi\": 0.371, \"age\": 21.0, \"class\": 1.0}, {\"preg\": 6.0, \"plas\": 109.0, \"pres\": 60.0, \"skin\": 27.0, \"insu\": 0.0, \"mass\": 25.0, \"pedi\": 0.206, \"age\": 27.0, \"class\": 0.0}, {\"preg\": 12.0, \"plas\": 121.0, \"pres\": 78.0, \"skin\": 17.0, \"insu\": 0.0, \"mass\": 26.5, \"pedi\": 0.259, \"age\": 62.0, \"class\": 0.0}, {\"preg\": 8.0, \"plas\": 100.0, \"pres\": 76.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 38.7, \"pedi\": 0.19, \"age\": 42.0, \"class\": 0.0}, {\"preg\": 8.0, \"plas\": 124.0, \"pres\": 76.0, \"skin\": 24.0, \"insu\": 600.0, \"mass\": 28.7, \"pedi\": 0.687, \"age\": 52.0, \"class\": 1.0}, {\"preg\": 1.0, \"plas\": 93.0, \"pres\": 56.0, \"skin\": 11.0, \"insu\": 0.0, \"mass\": 22.5, \"pedi\": 0.417, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 8.0, \"plas\": 143.0, \"pres\": 66.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 34.9, \"pedi\": 0.129, \"age\": 41.0, \"class\": 1.0}, {\"preg\": 6.0, \"plas\": 103.0, \"pres\": 66.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 24.3, \"pedi\": 0.249, \"age\": 29.0, \"class\": 0.0}, {\"preg\": 3.0, \"plas\": 176.0, \"pres\": 86.0, \"skin\": 27.0, \"insu\": 156.0, \"mass\": 33.3, \"pedi\": 1.154, \"age\": 52.0, \"class\": 1.0}, {\"preg\": 0.0, \"plas\": 73.0, \"pres\": 0.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 21.1, \"pedi\": 0.342, \"age\": 25.0, \"class\": 0.0}, {\"preg\": 11.0, \"plas\": 111.0, \"pres\": 84.0, \"skin\": 40.0, \"insu\": 0.0, \"mass\": 46.8, \"pedi\": 0.925, \"age\": 45.0, \"class\": 1.0}, {\"preg\": 2.0, \"plas\": 112.0, \"pres\": 78.0, \"skin\": 50.0, \"insu\": 140.0, \"mass\": 39.4, \"pedi\": 0.175, \"age\": 24.0, \"class\": 0.0}, {\"preg\": 3.0, \"plas\": 132.0, \"pres\": 80.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 34.4, \"pedi\": 0.402, \"age\": 44.0, \"class\": 1.0}, {\"preg\": 2.0, \"plas\": 82.0, \"pres\": 52.0, \"skin\": 22.0, \"insu\": 115.0, \"mass\": 28.5, \"pedi\": 1.699, \"age\": 25.0, \"class\": 0.0}, {\"preg\": 6.0, \"plas\": 123.0, \"pres\": 72.0, \"skin\": 45.0, \"insu\": 230.0, \"mass\": 33.6, \"pedi\": 0.733, \"age\": 34.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 188.0, \"pres\": 82.0, \"skin\": 14.0, \"insu\": 185.0, \"mass\": 32.0, \"pedi\": 0.682, \"age\": 22.0, \"class\": 1.0}, {\"preg\": 0.0, \"plas\": 67.0, \"pres\": 76.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 45.3, \"pedi\": 0.194, \"age\": 46.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 89.0, \"pres\": 24.0, \"skin\": 19.0, \"insu\": 25.0, \"mass\": 27.8, \"pedi\": 0.559, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 173.0, \"pres\": 74.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 36.8, \"pedi\": 0.088, \"age\": 38.0, \"class\": 1.0}, {\"preg\": 1.0, \"plas\": 109.0, \"pres\": 38.0, \"skin\": 18.0, \"insu\": 120.0, \"mass\": 23.1, \"pedi\": 0.407, \"age\": 26.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 108.0, \"pres\": 88.0, \"skin\": 19.0, \"insu\": 0.0, \"mass\": 27.1, \"pedi\": 0.4, \"age\": 24.0, \"class\": 0.0}, {\"preg\": 6.0, \"plas\": 96.0, \"pres\": 0.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 23.7, \"pedi\": 0.19, \"age\": 28.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 124.0, \"pres\": 74.0, \"skin\": 36.0, \"insu\": 0.0, \"mass\": 27.8, \"pedi\": 0.1, \"age\": 30.0, \"class\": 0.0}, {\"preg\": 7.0, \"plas\": 150.0, \"pres\": 78.0, \"skin\": 29.0, \"insu\": 126.0, \"mass\": 35.2, \"pedi\": 0.692, \"age\": 54.0, \"class\": 1.0}, {\"preg\": 4.0, \"plas\": 183.0, \"pres\": 0.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 28.4, \"pedi\": 0.212, \"age\": 36.0, \"class\": 1.0}, {\"preg\": 1.0, \"plas\": 124.0, \"pres\": 60.0, \"skin\": 32.0, \"insu\": 0.0, \"mass\": 35.8, \"pedi\": 0.514, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 181.0, \"pres\": 78.0, \"skin\": 42.0, \"insu\": 293.0, \"mass\": 40.0, \"pedi\": 1.258, \"age\": 22.0, \"class\": 1.0}, {\"preg\": 1.0, \"plas\": 92.0, \"pres\": 62.0, \"skin\": 25.0, \"insu\": 41.0, \"mass\": 19.5, \"pedi\": 0.482, \"age\": 25.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 152.0, \"pres\": 82.0, \"skin\": 39.0, \"insu\": 272.0, \"mass\": 41.5, \"pedi\": 0.27, \"age\": 27.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 111.0, \"pres\": 62.0, \"skin\": 13.0, \"insu\": 182.0, \"mass\": 24.0, \"pedi\": 0.138, \"age\": 23.0, \"class\": 0.0}, {\"preg\": 3.0, \"plas\": 106.0, \"pres\": 54.0, \"skin\": 21.0, \"insu\": 158.0, \"mass\": 30.9, \"pedi\": 0.292, \"age\": 24.0, \"class\": 0.0}, {\"preg\": 3.0, \"plas\": 174.0, \"pres\": 58.0, \"skin\": 22.0, \"insu\": 194.0, \"mass\": 32.9, \"pedi\": 0.593, \"age\": 36.0, \"class\": 1.0}, {\"preg\": 7.0, \"plas\": 168.0, \"pres\": 88.0, \"skin\": 42.0, \"insu\": 321.0, \"mass\": 38.2, \"pedi\": 0.787, \"age\": 40.0, \"class\": 1.0}, {\"preg\": 6.0, \"plas\": 105.0, \"pres\": 80.0, \"skin\": 28.0, \"insu\": 0.0, \"mass\": 32.5, \"pedi\": 0.878, \"age\": 26.0, \"class\": 0.0}, {\"preg\": 11.0, \"plas\": 138.0, \"pres\": 74.0, \"skin\": 26.0, \"insu\": 144.0, \"mass\": 36.1, \"pedi\": 0.557, \"age\": 50.0, \"class\": 1.0}, {\"preg\": 3.0, \"plas\": 106.0, \"pres\": 72.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 25.8, \"pedi\": 0.207, \"age\": 27.0, \"class\": 0.0}, {\"preg\": 6.0, \"plas\": 117.0, \"pres\": 96.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 28.7, \"pedi\": 0.157, \"age\": 30.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 68.0, \"pres\": 62.0, \"skin\": 13.0, \"insu\": 15.0, \"mass\": 20.1, \"pedi\": 0.257, \"age\": 23.0, \"class\": 0.0}, {\"preg\": 9.0, \"plas\": 112.0, \"pres\": 82.0, \"skin\": 24.0, \"insu\": 0.0, \"mass\": 28.2, \"pedi\": 1.282, \"age\": 50.0, \"class\": 1.0}, {\"preg\": 0.0, \"plas\": 119.0, \"pres\": 0.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 32.4, \"pedi\": 0.141, \"age\": 24.0, \"class\": 1.0}, {\"preg\": 2.0, \"plas\": 112.0, \"pres\": 86.0, \"skin\": 42.0, \"insu\": 160.0, \"mass\": 38.4, \"pedi\": 0.246, \"age\": 28.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 92.0, \"pres\": 76.0, \"skin\": 20.0, \"insu\": 0.0, \"mass\": 24.2, \"pedi\": 1.698, \"age\": 28.0, \"class\": 0.0}, {\"preg\": 6.0, \"plas\": 183.0, \"pres\": 94.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 40.8, \"pedi\": 1.461, \"age\": 45.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 94.0, \"pres\": 70.0, \"skin\": 27.0, \"insu\": 115.0, \"mass\": 43.5, \"pedi\": 0.347, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 108.0, \"pres\": 64.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 30.8, \"pedi\": 0.158, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 4.0, \"plas\": 90.0, \"pres\": 88.0, \"skin\": 47.0, \"insu\": 54.0, \"mass\": 37.7, \"pedi\": 0.362, \"age\": 29.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 125.0, \"pres\": 68.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 24.7, \"pedi\": 0.206, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 132.0, \"pres\": 78.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 32.4, \"pedi\": 0.393, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 5.0, \"plas\": 128.0, \"pres\": 80.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 34.6, \"pedi\": 0.144, \"age\": 45.0, \"class\": 0.0}, {\"preg\": 4.0, \"plas\": 94.0, \"pres\": 65.0, \"skin\": 22.0, \"insu\": 0.0, \"mass\": 24.7, \"pedi\": 0.148, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 7.0, \"plas\": 114.0, \"pres\": 64.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 27.4, \"pedi\": 0.732, \"age\": 34.0, \"class\": 1.0}, {\"preg\": 0.0, \"plas\": 102.0, \"pres\": 78.0, \"skin\": 40.0, \"insu\": 90.0, \"mass\": 34.5, \"pedi\": 0.238, \"age\": 24.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 111.0, \"pres\": 60.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 26.2, \"pedi\": 0.343, \"age\": 23.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 128.0, \"pres\": 82.0, \"skin\": 17.0, \"insu\": 183.0, \"mass\": 27.5, \"pedi\": 0.115, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 10.0, \"plas\": 92.0, \"pres\": 62.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 25.9, \"pedi\": 0.167, \"age\": 31.0, \"class\": 0.0}, {\"preg\": 13.0, \"plas\": 104.0, \"pres\": 72.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 31.2, \"pedi\": 0.465, \"age\": 38.0, \"class\": 1.0}, {\"preg\": 5.0, \"plas\": 104.0, \"pres\": 74.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 28.8, \"pedi\": 0.153, \"age\": 48.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 94.0, \"pres\": 76.0, \"skin\": 18.0, \"insu\": 66.0, \"mass\": 31.6, \"pedi\": 0.649, \"age\": 23.0, \"class\": 0.0}, {\"preg\": 7.0, \"plas\": 97.0, \"pres\": 76.0, \"skin\": 32.0, \"insu\": 91.0, \"mass\": 40.9, \"pedi\": 0.871, \"age\": 32.0, \"class\": 1.0}, {\"preg\": 1.0, \"plas\": 100.0, \"pres\": 74.0, \"skin\": 12.0, \"insu\": 46.0, \"mass\": 19.5, \"pedi\": 0.149, \"age\": 28.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 102.0, \"pres\": 86.0, \"skin\": 17.0, \"insu\": 105.0, \"mass\": 29.3, \"pedi\": 0.695, \"age\": 27.0, \"class\": 0.0}, {\"preg\": 4.0, \"plas\": 128.0, \"pres\": 70.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 34.3, \"pedi\": 0.303, \"age\": 24.0, \"class\": 0.0}, {\"preg\": 6.0, \"plas\": 147.0, \"pres\": 80.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 29.5, \"pedi\": 0.178, \"age\": 50.0, \"class\": 1.0}, {\"preg\": 4.0, \"plas\": 90.0, \"pres\": 0.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 28.0, \"pedi\": 0.61, \"age\": 31.0, \"class\": 0.0}, {\"preg\": 3.0, \"plas\": 103.0, \"pres\": 72.0, \"skin\": 30.0, \"insu\": 152.0, \"mass\": 27.6, \"pedi\": 0.73, \"age\": 27.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 157.0, \"pres\": 74.0, \"skin\": 35.0, \"insu\": 440.0, \"mass\": 39.4, \"pedi\": 0.134, \"age\": 30.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 167.0, \"pres\": 74.0, \"skin\": 17.0, \"insu\": 144.0, \"mass\": 23.4, \"pedi\": 0.447, \"age\": 33.0, \"class\": 1.0}, {\"preg\": 0.0, \"plas\": 179.0, \"pres\": 50.0, \"skin\": 36.0, \"insu\": 159.0, \"mass\": 37.8, \"pedi\": 0.455, \"age\": 22.0, \"class\": 1.0}, {\"preg\": 11.0, \"plas\": 136.0, \"pres\": 84.0, \"skin\": 35.0, \"insu\": 130.0, \"mass\": 28.3, \"pedi\": 0.26, \"age\": 42.0, \"class\": 1.0}, {\"preg\": 0.0, \"plas\": 107.0, \"pres\": 60.0, \"skin\": 25.0, \"insu\": 0.0, \"mass\": 26.4, \"pedi\": 0.133, \"age\": 23.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 91.0, \"pres\": 54.0, \"skin\": 25.0, \"insu\": 100.0, \"mass\": 25.2, \"pedi\": 0.234, \"age\": 23.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 117.0, \"pres\": 60.0, \"skin\": 23.0, \"insu\": 106.0, \"mass\": 33.8, \"pedi\": 0.466, \"age\": 27.0, \"class\": 0.0}, {\"preg\": 5.0, \"plas\": 123.0, \"pres\": 74.0, \"skin\": 40.0, \"insu\": 77.0, \"mass\": 34.1, \"pedi\": 0.269, \"age\": 28.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 120.0, \"pres\": 54.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 26.8, \"pedi\": 0.455, \"age\": 27.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 106.0, \"pres\": 70.0, \"skin\": 28.0, \"insu\": 135.0, \"mass\": 34.2, \"pedi\": 0.142, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 155.0, \"pres\": 52.0, \"skin\": 27.0, \"insu\": 540.0, \"mass\": 38.7, \"pedi\": 0.24, \"age\": 25.0, \"class\": 1.0}, {\"preg\": 2.0, \"plas\": 101.0, \"pres\": 58.0, \"skin\": 35.0, \"insu\": 90.0, \"mass\": 21.8, \"pedi\": 0.155, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 120.0, \"pres\": 80.0, \"skin\": 48.0, \"insu\": 200.0, \"mass\": 38.9, \"pedi\": 1.162, \"age\": 41.0, \"class\": 0.0}, {\"preg\": 11.0, \"plas\": 127.0, \"pres\": 106.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 39.0, \"pedi\": 0.19, \"age\": 51.0, \"class\": 0.0}, {\"preg\": 3.0, \"plas\": 80.0, \"pres\": 82.0, \"skin\": 31.0, \"insu\": 70.0, \"mass\": 34.2, \"pedi\": 1.292, \"age\": 27.0, \"class\": 1.0}, {\"preg\": 10.0, \"plas\": 162.0, \"pres\": 84.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 27.7, \"pedi\": 0.182, \"age\": 54.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 199.0, \"pres\": 76.0, \"skin\": 43.0, \"insu\": 0.0, \"mass\": 42.9, \"pedi\": 1.394, \"age\": 22.0, \"class\": 1.0}, {\"preg\": 8.0, \"plas\": 167.0, \"pres\": 106.0, \"skin\": 46.0, \"insu\": 231.0, \"mass\": 37.6, \"pedi\": 0.165, \"age\": 43.0, \"class\": 1.0}, {\"preg\": 9.0, \"plas\": 145.0, \"pres\": 80.0, \"skin\": 46.0, \"insu\": 130.0, \"mass\": 37.9, \"pedi\": 0.637, \"age\": 40.0, \"class\": 1.0}, {\"preg\": 6.0, \"plas\": 115.0, \"pres\": 60.0, \"skin\": 39.0, \"insu\": 0.0, \"mass\": 33.7, \"pedi\": 0.245, \"age\": 40.0, \"class\": 1.0}, {\"preg\": 1.0, \"plas\": 112.0, \"pres\": 80.0, \"skin\": 45.0, \"insu\": 132.0, \"mass\": 34.8, \"pedi\": 0.217, \"age\": 24.0, \"class\": 0.0}, {\"preg\": 4.0, \"plas\": 145.0, \"pres\": 82.0, \"skin\": 18.0, \"insu\": 0.0, \"mass\": 32.5, \"pedi\": 0.235, \"age\": 70.0, \"class\": 1.0}, {\"preg\": 10.0, \"plas\": 111.0, \"pres\": 70.0, \"skin\": 27.0, \"insu\": 0.0, \"mass\": 27.5, \"pedi\": 0.141, \"age\": 40.0, \"class\": 1.0}, {\"preg\": 6.0, \"plas\": 98.0, \"pres\": 58.0, \"skin\": 33.0, \"insu\": 190.0, \"mass\": 34.0, \"pedi\": 0.43, \"age\": 43.0, \"class\": 0.0}, {\"preg\": 9.0, \"plas\": 154.0, \"pres\": 78.0, \"skin\": 30.0, \"insu\": 100.0, \"mass\": 30.9, \"pedi\": 0.164, \"age\": 45.0, \"class\": 0.0}, {\"preg\": 6.0, \"plas\": 165.0, \"pres\": 68.0, \"skin\": 26.0, \"insu\": 168.0, \"mass\": 33.6, \"pedi\": 0.631, \"age\": 49.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 99.0, \"pres\": 58.0, \"skin\": 10.0, \"insu\": 0.0, \"mass\": 25.4, \"pedi\": 0.551, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 10.0, \"plas\": 68.0, \"pres\": 106.0, \"skin\": 23.0, \"insu\": 49.0, \"mass\": 35.5, \"pedi\": 0.285, \"age\": 47.0, \"class\": 0.0}, {\"preg\": 3.0, \"plas\": 123.0, \"pres\": 100.0, \"skin\": 35.0, \"insu\": 240.0, \"mass\": 57.3, \"pedi\": 0.88, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 8.0, \"plas\": 91.0, \"pres\": 82.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 35.6, \"pedi\": 0.587, \"age\": 68.0, \"class\": 0.0}, {\"preg\": 6.0, \"plas\": 195.0, \"pres\": 70.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 30.9, \"pedi\": 0.328, \"age\": 31.0, \"class\": 1.0}, {\"preg\": 9.0, \"plas\": 156.0, \"pres\": 86.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 24.8, \"pedi\": 0.23, \"age\": 53.0, \"class\": 1.0}, {\"preg\": 0.0, \"plas\": 93.0, \"pres\": 60.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 35.3, \"pedi\": 0.263, \"age\": 25.0, \"class\": 0.0}, {\"preg\": 3.0, \"plas\": 121.0, \"pres\": 52.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 36.0, \"pedi\": 0.127, \"age\": 25.0, \"class\": 1.0}, {\"preg\": 2.0, \"plas\": 101.0, \"pres\": 58.0, \"skin\": 17.0, \"insu\": 265.0, \"mass\": 24.2, \"pedi\": 0.614, \"age\": 23.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 56.0, \"pres\": 56.0, \"skin\": 28.0, \"insu\": 45.0, \"mass\": 24.2, \"pedi\": 0.332, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 162.0, \"pres\": 76.0, \"skin\": 36.0, \"insu\": 0.0, \"mass\": 49.6, \"pedi\": 0.364, \"age\": 26.0, \"class\": 1.0}, {\"preg\": 0.0, \"plas\": 95.0, \"pres\": 64.0, \"skin\": 39.0, \"insu\": 105.0, \"mass\": 44.6, \"pedi\": 0.366, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 4.0, \"plas\": 125.0, \"pres\": 80.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 32.3, \"pedi\": 0.536, \"age\": 27.0, \"class\": 1.0}, {\"preg\": 5.0, \"plas\": 136.0, \"pres\": 82.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 0.0, \"pedi\": 0.64, \"age\": 69.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 129.0, \"pres\": 74.0, \"skin\": 26.0, \"insu\": 205.0, \"mass\": 33.2, \"pedi\": 0.591, \"age\": 25.0, \"class\": 0.0}, {\"preg\": 3.0, \"plas\": 130.0, \"pres\": 64.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 23.1, \"pedi\": 0.314, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 107.0, \"pres\": 50.0, \"skin\": 19.0, \"insu\": 0.0, \"mass\": 28.3, \"pedi\": 0.181, \"age\": 29.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 140.0, \"pres\": 74.0, \"skin\": 26.0, \"insu\": 180.0, \"mass\": 24.1, \"pedi\": 0.828, \"age\": 23.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 144.0, \"pres\": 82.0, \"skin\": 46.0, \"insu\": 180.0, \"mass\": 46.1, \"pedi\": 0.335, \"age\": 46.0, \"class\": 1.0}, {\"preg\": 8.0, \"plas\": 107.0, \"pres\": 80.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 24.6, \"pedi\": 0.856, \"age\": 34.0, \"class\": 0.0}, {\"preg\": 13.0, \"plas\": 158.0, \"pres\": 114.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 42.3, \"pedi\": 0.257, \"age\": 44.0, \"class\": 1.0}, {\"preg\": 2.0, \"plas\": 121.0, \"pres\": 70.0, \"skin\": 32.0, \"insu\": 95.0, \"mass\": 39.1, \"pedi\": 0.886, \"age\": 23.0, \"class\": 0.0}, {\"preg\": 7.0, \"plas\": 129.0, \"pres\": 68.0, \"skin\": 49.0, \"insu\": 125.0, \"mass\": 38.5, \"pedi\": 0.439, \"age\": 43.0, \"class\": 1.0}, {\"preg\": 2.0, \"plas\": 90.0, \"pres\": 60.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 23.5, \"pedi\": 0.191, \"age\": 25.0, \"class\": 0.0}, {\"preg\": 7.0, \"plas\": 142.0, \"pres\": 90.0, \"skin\": 24.0, \"insu\": 480.0, \"mass\": 30.4, \"pedi\": 0.128, \"age\": 43.0, \"class\": 1.0}, {\"preg\": 3.0, \"plas\": 169.0, \"pres\": 74.0, \"skin\": 19.0, \"insu\": 125.0, \"mass\": 29.9, \"pedi\": 0.268, \"age\": 31.0, \"class\": 1.0}, {\"preg\": 0.0, \"plas\": 99.0, \"pres\": 0.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 25.0, \"pedi\": 0.253, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 4.0, \"plas\": 127.0, \"pres\": 88.0, \"skin\": 11.0, \"insu\": 155.0, \"mass\": 34.5, \"pedi\": 0.598, \"age\": 28.0, \"class\": 0.0}, {\"preg\": 4.0, \"plas\": 118.0, \"pres\": 70.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 44.5, \"pedi\": 0.904, \"age\": 26.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 122.0, \"pres\": 76.0, \"skin\": 27.0, \"insu\": 200.0, \"mass\": 35.9, \"pedi\": 0.483, \"age\": 26.0, \"class\": 0.0}, {\"preg\": 6.0, \"plas\": 125.0, \"pres\": 78.0, \"skin\": 31.0, \"insu\": 0.0, \"mass\": 27.6, \"pedi\": 0.565, \"age\": 49.0, \"class\": 1.0}, {\"preg\": 1.0, \"plas\": 168.0, \"pres\": 88.0, \"skin\": 29.0, \"insu\": 0.0, \"mass\": 35.0, \"pedi\": 0.905, \"age\": 52.0, \"class\": 1.0}, {\"preg\": 2.0, \"plas\": 129.0, \"pres\": 0.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 38.5, \"pedi\": 0.304, \"age\": 41.0, \"class\": 0.0}, {\"preg\": 4.0, \"plas\": 110.0, \"pres\": 76.0, \"skin\": 20.0, \"insu\": 100.0, \"mass\": 28.4, \"pedi\": 0.118, \"age\": 27.0, \"class\": 0.0}, {\"preg\": 6.0, \"plas\": 80.0, \"pres\": 80.0, \"skin\": 36.0, \"insu\": 0.0, \"mass\": 39.8, \"pedi\": 0.177, \"age\": 28.0, \"class\": 0.0}, {\"preg\": 10.0, \"plas\": 115.0, \"pres\": 0.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 0.0, \"pedi\": 0.261, \"age\": 30.0, \"class\": 1.0}, {\"preg\": 2.0, \"plas\": 127.0, \"pres\": 46.0, \"skin\": 21.0, \"insu\": 335.0, \"mass\": 34.4, \"pedi\": 0.176, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 9.0, \"plas\": 164.0, \"pres\": 78.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 32.8, \"pedi\": 0.148, \"age\": 45.0, \"class\": 1.0}, {\"preg\": 2.0, \"plas\": 93.0, \"pres\": 64.0, \"skin\": 32.0, \"insu\": 160.0, \"mass\": 38.0, \"pedi\": 0.674, \"age\": 23.0, \"class\": 1.0}, {\"preg\": 3.0, \"plas\": 158.0, \"pres\": 64.0, \"skin\": 13.0, \"insu\": 387.0, \"mass\": 31.2, \"pedi\": 0.295, \"age\": 24.0, \"class\": 0.0}, {\"preg\": 5.0, \"plas\": 126.0, \"pres\": 78.0, \"skin\": 27.0, \"insu\": 22.0, \"mass\": 29.6, \"pedi\": 0.439, \"age\": 40.0, \"class\": 0.0}, {\"preg\": 10.0, \"plas\": 129.0, \"pres\": 62.0, \"skin\": 36.0, \"insu\": 0.0, \"mass\": 41.2, \"pedi\": 0.441, \"age\": 38.0, \"class\": 1.0}, {\"preg\": 0.0, \"plas\": 134.0, \"pres\": 58.0, \"skin\": 20.0, \"insu\": 291.0, \"mass\": 26.4, \"pedi\": 0.352, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 3.0, \"plas\": 102.0, \"pres\": 74.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 29.5, \"pedi\": 0.121, \"age\": 32.0, \"class\": 0.0}, {\"preg\": 7.0, \"plas\": 187.0, \"pres\": 50.0, \"skin\": 33.0, \"insu\": 392.0, \"mass\": 33.9, \"pedi\": 0.826, \"age\": 34.0, \"class\": 1.0}, {\"preg\": 3.0, \"plas\": 173.0, \"pres\": 78.0, \"skin\": 39.0, \"insu\": 185.0, \"mass\": 33.8, \"pedi\": 0.97, \"age\": 31.0, \"class\": 1.0}, {\"preg\": 10.0, \"plas\": 94.0, \"pres\": 72.0, \"skin\": 18.0, \"insu\": 0.0, \"mass\": 23.1, \"pedi\": 0.595, \"age\": 56.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 108.0, \"pres\": 60.0, \"skin\": 46.0, \"insu\": 178.0, \"mass\": 35.5, \"pedi\": 0.415, \"age\": 24.0, \"class\": 0.0}, {\"preg\": 5.0, \"plas\": 97.0, \"pres\": 76.0, \"skin\": 27.0, \"insu\": 0.0, \"mass\": 35.6, \"pedi\": 0.378, \"age\": 52.0, \"class\": 1.0}, {\"preg\": 4.0, \"plas\": 83.0, \"pres\": 86.0, \"skin\": 19.0, \"insu\": 0.0, \"mass\": 29.3, \"pedi\": 0.317, \"age\": 34.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 114.0, \"pres\": 66.0, \"skin\": 36.0, \"insu\": 200.0, \"mass\": 38.1, \"pedi\": 0.289, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 149.0, \"pres\": 68.0, \"skin\": 29.0, \"insu\": 127.0, \"mass\": 29.3, \"pedi\": 0.349, \"age\": 42.0, \"class\": 1.0}, {\"preg\": 5.0, \"plas\": 117.0, \"pres\": 86.0, \"skin\": 30.0, \"insu\": 105.0, \"mass\": 39.1, \"pedi\": 0.251, \"age\": 42.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 111.0, \"pres\": 94.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 32.8, \"pedi\": 0.265, \"age\": 45.0, \"class\": 0.0}, {\"preg\": 4.0, \"plas\": 112.0, \"pres\": 78.0, \"skin\": 40.0, \"insu\": 0.0, \"mass\": 39.4, \"pedi\": 0.236, \"age\": 38.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 116.0, \"pres\": 78.0, \"skin\": 29.0, \"insu\": 180.0, \"mass\": 36.1, \"pedi\": 0.496, \"age\": 25.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 141.0, \"pres\": 84.0, \"skin\": 26.0, \"insu\": 0.0, \"mass\": 32.4, \"pedi\": 0.433, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 175.0, \"pres\": 88.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 22.9, \"pedi\": 0.326, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 92.0, \"pres\": 52.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 30.1, \"pedi\": 0.141, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 3.0, \"plas\": 130.0, \"pres\": 78.0, \"skin\": 23.0, \"insu\": 79.0, \"mass\": 28.4, \"pedi\": 0.323, \"age\": 34.0, \"class\": 1.0}, {\"preg\": 8.0, \"plas\": 120.0, \"pres\": 86.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 28.4, \"pedi\": 0.259, \"age\": 22.0, \"class\": 1.0}, {\"preg\": 2.0, \"plas\": 174.0, \"pres\": 88.0, \"skin\": 37.0, \"insu\": 120.0, \"mass\": 44.5, \"pedi\": 0.646, \"age\": 24.0, \"class\": 1.0}, {\"preg\": 2.0, \"plas\": 106.0, \"pres\": 56.0, \"skin\": 27.0, \"insu\": 165.0, \"mass\": 29.0, \"pedi\": 0.426, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 105.0, \"pres\": 75.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 23.3, \"pedi\": 0.56, \"age\": 53.0, \"class\": 0.0}, {\"preg\": 4.0, \"plas\": 95.0, \"pres\": 60.0, \"skin\": 32.0, \"insu\": 0.0, \"mass\": 35.4, \"pedi\": 0.284, \"age\": 28.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 126.0, \"pres\": 86.0, \"skin\": 27.0, \"insu\": 120.0, \"mass\": 27.4, \"pedi\": 0.515, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 8.0, \"plas\": 65.0, \"pres\": 72.0, \"skin\": 23.0, \"insu\": 0.0, \"mass\": 32.0, \"pedi\": 0.6, \"age\": 42.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 99.0, \"pres\": 60.0, \"skin\": 17.0, \"insu\": 160.0, \"mass\": 36.6, \"pedi\": 0.453, \"age\": 21.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 102.0, \"pres\": 74.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 39.5, \"pedi\": 0.293, \"age\": 42.0, \"class\": 1.0}, {\"preg\": 11.0, \"plas\": 120.0, \"pres\": 80.0, \"skin\": 37.0, \"insu\": 150.0, \"mass\": 42.3, \"pedi\": 0.785, \"age\": 48.0, \"class\": 1.0}, {\"preg\": 3.0, \"plas\": 102.0, \"pres\": 44.0, \"skin\": 20.0, \"insu\": 94.0, \"mass\": 30.8, \"pedi\": 0.4, \"age\": 26.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 109.0, \"pres\": 58.0, \"skin\": 18.0, \"insu\": 116.0, \"mass\": 28.5, \"pedi\": 0.219, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 9.0, \"plas\": 140.0, \"pres\": 94.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 32.7, \"pedi\": 0.734, \"age\": 45.0, \"class\": 1.0}, {\"preg\": 13.0, \"plas\": 153.0, \"pres\": 88.0, \"skin\": 37.0, \"insu\": 140.0, \"mass\": 40.6, \"pedi\": 1.174, \"age\": 39.0, \"class\": 0.0}, {\"preg\": 12.0, \"plas\": 100.0, \"pres\": 84.0, \"skin\": 33.0, \"insu\": 105.0, \"mass\": 30.0, \"pedi\": 0.488, \"age\": 46.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 147.0, \"pres\": 94.0, \"skin\": 41.0, \"insu\": 0.0, \"mass\": 49.3, \"pedi\": 0.358, \"age\": 27.0, \"class\": 1.0}, {\"preg\": 1.0, \"plas\": 81.0, \"pres\": 74.0, \"skin\": 41.0, \"insu\": 57.0, \"mass\": 46.3, \"pedi\": 1.096, \"age\": 32.0, \"class\": 0.0}, {\"preg\": 3.0, \"plas\": 187.0, \"pres\": 70.0, \"skin\": 22.0, \"insu\": 200.0, \"mass\": 36.4, \"pedi\": 0.408, \"age\": 36.0, \"class\": 1.0}, {\"preg\": 6.0, \"plas\": 162.0, \"pres\": 62.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 24.3, \"pedi\": 0.178, \"age\": 50.0, \"class\": 1.0}, {\"preg\": 4.0, \"plas\": 136.0, \"pres\": 70.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 31.2, \"pedi\": 1.182, \"age\": 22.0, \"class\": 1.0}, {\"preg\": 1.0, \"plas\": 121.0, \"pres\": 78.0, \"skin\": 39.0, \"insu\": 74.0, \"mass\": 39.0, \"pedi\": 0.261, \"age\": 28.0, \"class\": 0.0}, {\"preg\": 3.0, \"plas\": 108.0, \"pres\": 62.0, \"skin\": 24.0, \"insu\": 0.0, \"mass\": 26.0, \"pedi\": 0.223, \"age\": 25.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 181.0, \"pres\": 88.0, \"skin\": 44.0, \"insu\": 510.0, \"mass\": 43.3, \"pedi\": 0.222, \"age\": 26.0, \"class\": 1.0}, {\"preg\": 8.0, \"plas\": 154.0, \"pres\": 78.0, \"skin\": 32.0, \"insu\": 0.0, \"mass\": 32.4, \"pedi\": 0.443, \"age\": 45.0, \"class\": 1.0}, {\"preg\": 1.0, \"plas\": 128.0, \"pres\": 88.0, \"skin\": 39.0, \"insu\": 110.0, \"mass\": 36.5, \"pedi\": 1.057, \"age\": 37.0, \"class\": 1.0}, {\"preg\": 7.0, \"plas\": 137.0, \"pres\": 90.0, \"skin\": 41.0, \"insu\": 0.0, \"mass\": 32.0, \"pedi\": 0.391, \"age\": 39.0, \"class\": 0.0}, {\"preg\": 0.0, \"plas\": 123.0, \"pres\": 72.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 36.3, \"pedi\": 0.258, \"age\": 52.0, \"class\": 1.0}, {\"preg\": 1.0, \"plas\": 106.0, \"pres\": 76.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 37.5, \"pedi\": 0.197, \"age\": 26.0, \"class\": 0.0}, {\"preg\": 6.0, \"plas\": 190.0, \"pres\": 92.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 35.5, \"pedi\": 0.278, \"age\": 66.0, \"class\": 1.0}, {\"preg\": 2.0, \"plas\": 88.0, \"pres\": 58.0, \"skin\": 26.0, \"insu\": 16.0, \"mass\": 28.4, \"pedi\": 0.766, \"age\": 22.0, \"class\": 0.0}, {\"preg\": 9.0, \"plas\": 170.0, \"pres\": 74.0, \"skin\": 31.0, \"insu\": 0.0, \"mass\": 44.0, \"pedi\": 0.403, \"age\": 43.0, \"class\": 1.0}, {\"preg\": 9.0, \"plas\": 89.0, \"pres\": 62.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 22.5, \"pedi\": 0.142, \"age\": 33.0, \"class\": 0.0}, {\"preg\": 10.0, \"plas\": 101.0, \"pres\": 76.0, \"skin\": 48.0, \"insu\": 180.0, \"mass\": 32.9, \"pedi\": 0.171, \"age\": 63.0, \"class\": 0.0}, {\"preg\": 2.0, \"plas\": 122.0, \"pres\": 70.0, \"skin\": 27.0, \"insu\": 0.0, \"mass\": 36.8, \"pedi\": 0.34, \"age\": 27.0, \"class\": 0.0}, {\"preg\": 5.0, \"plas\": 121.0, \"pres\": 72.0, \"skin\": 23.0, \"insu\": 112.0, \"mass\": 26.2, \"pedi\": 0.245, \"age\": 30.0, \"class\": 0.0}, {\"preg\": 1.0, \"plas\": 126.0, \"pres\": 60.0, \"skin\": 0.0, \"insu\": 0.0, \"mass\": 30.1, \"pedi\": 0.349, \"age\": 47.0, \"class\": 1.0}, {\"preg\": 1.0, \"plas\": 93.0, \"pres\": 70.0, \"skin\": 31.0, \"insu\": 0.0, \"mass\": 30.4, \"pedi\": 0.315, \"age\": 23.0, \"class\": 0.0}]}};\n",
       "var opt = {};\n",
       "var type = \"vega-lite\";\n",
       "var id = \"189f627b-321e-4056-a5a6-3425ffdc09da\";\n",
       "\n",
       "var output_area = this;\n",
       "\n",
       "require([\"nbextensions/jupyter-vega/index\"], function(vega) {\n",
       "  var target = document.createElement(\"div\");\n",
       "  target.id = id;\n",
       "  target.className = \"vega-embed\";\n",
       "\n",
       "  var style = document.createElement(\"style\");\n",
       "  style.textContent = [\n",
       "    \".vega-embed .error p {\",\n",
       "    \"  color: firebrick;\",\n",
       "    \"  font-size: 14px;\",\n",
       "    \"}\",\n",
       "  ].join(\"\\\\n\");\n",
       "\n",
       "  // element is a jQuery wrapped DOM element inside the output area\n",
       "  // see http://ipython.readthedocs.io/en/stable/api/generated/\\\n",
       "  // IPython.display.html#IPython.display.Javascript.__init__\n",
       "  element[0].appendChild(target);\n",
       "  element[0].appendChild(style);\n",
       "\n",
       "  vega.render(\"#\" + id, spec, type, opt, output_area);\n",
       "}, function (err) {\n",
       "  if (err.requireType !== \"scripterror\") {\n",
       "    throw(err);\n",
       "  }\n",
       "});\n"
      ],
      "text/plain": [
       "<vega.vegalite.VegaLite at 0x7ff7d5e89cc0>"
      ]
     },
     "metadata": {
      "jupyter-vega": "#189f627b-321e-4056-a5a6-3425ffdc09da"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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"
     },
     "metadata": {
      "jupyter-vega": "#189f627b-321e-4056-a5a6-3425ffdc09da"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize dataset which was used above for experiment\n",
    "import altair as alt\n",
    "alt.renderers.enable('notebook')\n",
    "dataset = pypadre.remote_backend.datasets.get(did).data\n",
    "alt.Chart(dataset).mark_point().encode( # Settings for diabetes dataset\n",
    "    x='mass',\n",
    "    y='age',\n",
    "    color='class'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the confusion matrix for the previous experiment\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "mt = ex.metrics[0][0][\"confusion_matrix\"]\n",
    "df_cm = pd.DataFrame(mt)\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(df_cm, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PadreApp from a package\n",
    "from padre.app import pypadre\n",
    "# All interactions with padre should be done via the app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration of padreapp via a configuration file including backend, http, server authentication etc\n",
    "\"\"\"\n",
    " from padre.app.padre_app import pypadre\n",
    " assert pypadre.offline\n",
    " pypadre.authenticate(\"test\", \"mgrani\") # authentication sets the pypadre app into online mode\n",
    " pypadre.config.save()\n",
    " assert pypadre.offline == False\n",
    " # delete all datasets\n",
    " for ds in pypadre.datasets.list():\n",
    "     pypadre.datasets.delete(ds)\n",
    " assert len([ds for ds in pypadre.datasets.list()])==0 # after deletion there should be no datasets\n",
    " # fetch downloads. This should include the datasets that are currently supported.\n",
    " downloads = pypadre.datasets.search_downloads(\"Iris\", type=\"multivariate\") # search for external datasets\n",
    " # user can manipulate downloads here\n",
    " downloaded = pypadre.datasets.download(downloads)\n",
    " for download in downloaded: \n",
    "    pypadre.datasets.put(ds)\n",
    " available = pypadre.datasets.list(\"Iris\")\n",
    " assert downloads.equals(available) # the check might be more complex here, but both sets should be equal\n",
    " dataset = pypadre.datasets.get(available[0])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<padre.core.datasets.Dataset object at 0x7ff7d30639e8>, <padre.core.datasets.Dataset object at 0x7ff7ccb6c6d8>]\n"
     ]
    }
   ],
   "source": [
    "# List all available datasets(Toy Datasets)\n",
    "print(pypadre.datasets.list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Test Experiment SVM 021cc75d-', 'Test Experiment SVM 9617e580-', 'Decorated Experiment', 'Test Experiment SVM', 'Test Experiment SVM 45efad82-', 'Test Experiment SVM b797549a-', 'Test_Incorrect_Execute_Parameters', 'Test Experiment SVM c32e4875-', 'iPython Experiment1 ', 'KNN', 'SVC(Digits)', 'Regression using PCA and SVR(Digits)', 'Gaussian Process Regression', 'Grid search with Random Forest Classifier and PCA(Digits)', 'SVC(Iris)', 'IPython Experiment 3', 'Decision Tree(Iris)', 'Regression using PCA and SVR(Boston_House_Prices)', 'Decision Tree(Digits)', 'Test Experiment SVM bed4e8ac-', 'PLS Regression(Boston_House_Prices)', 'Grid search with Random Forest Classifier and PCA(Iris)', 'Torch', 'Linear SVR', 'Test Experiment SVM 7b43774e-', 'Random Forest Classifier with PCA', 'Experiment using bagging classifier', 'Test Experiment SVM 4af9c9d7-', 'Test Experiment SVM 1da6639c-', 'PLS Regression(Diabetes)', 'Test_Incorrect_Estimator', 'KNN Regressor', 'Test Experiment SVM a367ba28-', 'Regression using PCA and SVR(Diabetes)']\n"
     ]
    }
   ],
   "source": [
    "# List all currently available local experiments\n",
    "print(pypadre.experiments.list_experiments())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: 2019-03-14 16:24:30.065 <padre.experimentcreator.ExperimentCreator object at 0x7ff7d6139f28>:\tiPython Experiment1  created successfully!\n",
      "{'Step_0': {'algorithm': {'attributes': {'path': '.steps[0][1]'},\n",
      "                          'value': 'isomap embedding'},\n",
      "            'doc': {'attributes': {'path': '.steps[0][1].__doc__'},\n",
      "                    'value': 'Isomap Embedding\\n'\n",
      "                             '\\n'\n",
      "                             '    Non-linear dimensionality reduction through '\n",
      "                             'Isometric Mapping\\n'\n",
      "                             '\\n'\n",
      "                             '    Read more in the :ref:`User Guide '\n",
      "                             '<isomap>`.\\n'\n",
      "                             '\\n'\n",
      "                             '    Parameters\\n'\n",
      "                             '    ----------\\n'\n",
      "                             '    n_neighbors : integer\\n'\n",
      "                             '        number of neighbors to consider for each '\n",
      "                             'point.\\n'\n",
      "                             '\\n'\n",
      "                             '    n_components : integer\\n'\n",
      "                             '        number of coordinates for the manifold\\n'\n",
      "                             '\\n'\n",
      "                             \"    eigen_solver : ['auto'|'arpack'|'dense']\\n\"\n",
      "                             \"        'auto' : Attempt to choose the most \"\n",
      "                             'efficient solver\\n'\n",
      "                             '        for the given problem.\\n'\n",
      "                             '\\n'\n",
      "                             \"        'arpack' : Use Arnoldi decomposition to \"\n",
      "                             'find the eigenvalues\\n'\n",
      "                             '        and eigenvectors.\\n'\n",
      "                             '\\n'\n",
      "                             \"        'dense' : Use a direct solver (i.e. \"\n",
      "                             'LAPACK)\\n'\n",
      "                             '        for the eigenvalue decomposition.\\n'\n",
      "                             '\\n'\n",
      "                             '    tol : float\\n'\n",
      "                             '        Convergence tolerance passed to arpack '\n",
      "                             'or lobpcg.\\n'\n",
      "                             \"        not used if eigen_solver == 'dense'.\\n\"\n",
      "                             '\\n'\n",
      "                             '    max_iter : integer\\n'\n",
      "                             '        Maximum number of iterations for the '\n",
      "                             'arpack solver.\\n'\n",
      "                             \"        not used if eigen_solver == 'dense'.\\n\"\n",
      "                             '\\n'\n",
      "                             \"    path_method : string ['auto'|'FW'|'D']\\n\"\n",
      "                             '        Method to use in finding shortest path.\\n'\n",
      "                             '\\n'\n",
      "                             \"        'auto' : attempt to choose the best \"\n",
      "                             'algorithm automatically.\\n'\n",
      "                             '\\n'\n",
      "                             \"        'FW' : Floyd-Warshall algorithm.\\n\"\n",
      "                             '\\n'\n",
      "                             \"        'D' : Dijkstra's algorithm.\\n\"\n",
      "                             '\\n'\n",
      "                             '    neighbors_algorithm : string '\n",
      "                             \"['auto'|'brute'|'kd_tree'|'ball_tree']\\n\"\n",
      "                             '        Algorithm to use for nearest neighbors '\n",
      "                             'search,\\n'\n",
      "                             '        passed to neighbors.NearestNeighbors '\n",
      "                             'instance.\\n'\n",
      "                             '\\n'\n",
      "                             '    n_jobs : int, optional (default = 1)\\n'\n",
      "                             '        The number of parallel jobs to run.\\n'\n",
      "                             '        If ``-1``, then the number of jobs is '\n",
      "                             'set to the number of CPU cores.\\n'\n",
      "                             '\\n'\n",
      "                             '    Attributes\\n'\n",
      "                             '    ----------\\n'\n",
      "                             '    embedding_ : array-like, shape (n_samples, '\n",
      "                             'n_components)\\n'\n",
      "                             '        Stores the embedding vectors.\\n'\n",
      "                             '\\n'\n",
      "                             '    kernel_pca_ : object\\n'\n",
      "                             '        `KernelPCA` object used to implement the '\n",
      "                             'embedding.\\n'\n",
      "                             '\\n'\n",
      "                             '    training_data_ : array-like, shape '\n",
      "                             '(n_samples, n_features)\\n'\n",
      "                             '        Stores the training data.\\n'\n",
      "                             '\\n'\n",
      "                             '    nbrs_ : sklearn.neighbors.NearestNeighbors '\n",
      "                             'instance\\n'\n",
      "                             '        Stores nearest neighbors instance, '\n",
      "                             'including BallTree or KDtree\\n'\n",
      "                             '        if applicable.\\n'\n",
      "                             '\\n'\n",
      "                             '    dist_matrix_ : array-like, shape (n_samples, '\n",
      "                             'n_samples)\\n'\n",
      "                             '        Stores the geodesic distance matrix of '\n",
      "                             'training data.\\n'\n",
      "                             '\\n'\n",
      "                             '    References\\n'\n",
      "                             '    ----------\\n'\n",
      "                             '\\n'\n",
      "                             '    .. [1] Tenenbaum, J.B.; De Silva, V.; & '\n",
      "                             'Langford, J.C. A global geometric\\n'\n",
      "                             '           framework for nonlinear '\n",
      "                             'dimensionality reduction. Science 290 (5500)\\n'\n",
      "                             '    '},\n",
      "            'hyper_parameters': {'execution_parameters': {},\n",
      "                                 'model_parameters': {'eigen_solver': {'attributes': {'path': '.steps[0][1].eigen_solver'},\n",
      "                                                                       'value': 'auto'},\n",
      "                                                      'neighbors_algorithm': {'attributes': {'path': '.steps[0][1].neighbors_algorithm'},\n",
      "                                                                              'value': 'auto'},\n",
      "                                                      'num_components': {'attributes': {'path': '.steps[0][1].n_components'},\n",
      "                                                                         'value': 2},\n",
      "                                                      'num_neighbours': {'attributes': {'path': '.steps[0][1].n_neighbors'},\n",
      "                                                                         'value': 5},\n",
      "                                                      'path_method': {'attributes': {'path': '.steps[0][1].path_method'},\n",
      "                                                                      'value': 'auto'},\n",
      "                                                      'tolerance': {'attributes': {'path': '.steps[0][1].tol'},\n",
      "                                                                    'value': 0}},\n",
      "                                 'optimisation_parameters': {'jobs': {'attributes': {'path': '.steps[0][1].n_jobs'},\n",
      "                                                                      'value': 1},\n",
      "                                                             'max_iterations': {'attributes': {'path': '.steps[0][1].max_iter'},\n",
      "                                                                                'value': None}}}}}\n",
      "INFO: 2019-03-14 16:24:30.674 Experiment<id:iPython Experiment1 >:\tstart: phase=experiment\n",
      "INFO: 2019-03-14 16:24:31.149 \tRun<id:305e9cb7-dccc-4139-9dcf-1e04047c055b;name:iPython Experiment1 >:\tstart: phase=run\n",
      "INFO: 2019-03-14 16:24:31.289 \t\tSplit<id:696a4c68-d76e-4b69-bb3f-2b6d9c2bb661;name:iPython Experiment1 >:\tstart: phase=split\n",
      "INFO: 2019-03-14 16:24:32.200 \t\tSplit<id:696a4c68-d76e-4b69-bb3f-2b6d9c2bb661;name:iPython Experiment1 >:\tstop: phase=split\n",
      "INFO: 2019-03-14 16:24:32.201 \tRun<id:305e9cb7-dccc-4139-9dcf-1e04047c055b;name:iPython Experiment1 >:\tstop: phase=run\n",
      "INFO: 2019-03-14 16:24:32.201 Experiment<id:iPython Experiment1 >:\tstop: phase=experiment\n"
     ]
    }
   ],
   "source": [
    "# Create a simple experiment via experiment creator utilizing a dataset displayed. Run it with multiple parameters\n",
    "pypadre.experiment_creator.clear_experiments()\n",
    "workflow = pypadre.experiment_creator.create_test_pipeline(['isomap embedding'])\n",
    "pypadre.experiment_creator.create(name='iPython Experiment1 ', \n",
    "                                  description='Simple iPython notebook experiment',\n",
    "                                  workflow=workflow, \n",
    "                                  dataset_list=['Digits'])\n",
    "pypadre.experiment_creator.execute()\n",
    "\n",
    "# Aimed at new users for running already created experiments or running from JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using decorators to run experiments\n",
    "from padre.ds_import import load_sklearn_toys\n",
    "from padre import *\n",
    "from padre.app import pypadre\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "@Workflow(exp_name=\"Decorated Experiment\",\n",
    "          description=\"Test experiment with decorators and SVC\")\n",
    "def create_test_pipeline():\n",
    "    estimators = [('SVC', SVC(probability=True))]\n",
    "    return Pipeline(estimators)\n",
    "\n",
    "\n",
    "@Dataset(exp_name=\"Decorated Experiment\")\n",
    "def get_dataset():\n",
    "    ds = [i for i in load_sklearn_toys()][2]\n",
    "    return ds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating the workflow\n",
      "INFO: 2019-03-14 16:24:35.940 Experiment<id:Decorated Experiment>:\tstart: phase=experiment\n",
      "INFO: 2019-03-14 16:24:36.233 \tRun<id:702334d1-3781-4ee4-b136-a06c4c16d1d0;name:Decorated Experiment>:\tstart: phase=run\n",
      "INFO: 2019-03-14 16:24:36.377 \t\tSplit<id:fe8ef67e-56d5-419d-b918-c4b73c1c3acf;name:Decorated Experiment>:\tstart: phase=split\n",
      "INFO: 2019-03-14 16:24:37.436 \t\tSplit<id:fe8ef67e-56d5-419d-b918-c4b73c1c3acf;name:Decorated Experiment>:\tstop: phase=split\n",
      "INFO: 2019-03-14 16:24:37.467 \tRun<id:702334d1-3781-4ee4-b136-a06c4c16d1d0;name:Decorated Experiment>:\tstop: phase=run\n",
      "INFO: 2019-03-14 16:24:37.483 Experiment<id:Decorated Experiment>:\tstop: phase=experiment\n",
      "creating the workflow\n",
      "INFO: 2019-03-14 16:24:37.912 Experiment<id:Decorated Experiment>:\tstart: phase=experiment\n",
      "INFO: 2019-03-14 16:24:38.228 \tRun<id:59a5c707-6503-453e-b53a-067af7b0a377;name:Decorated Experiment>:\tstart: phase=run\n",
      "INFO: 2019-03-14 16:24:38.383 \t\tSplit<id:37c51fda-8131-40b3-9c66-238aac0db6c9;name:Decorated Experiment>:\tstart: phase=split\n",
      "INFO: 2019-03-14 16:24:39.370 \t\tSplit<id:37c51fda-8131-40b3-9c66-238aac0db6c9;name:Decorated Experiment>:\tstop: phase=split\n",
      "INFO: 2019-03-14 16:24:39.401 \tRun<id:59a5c707-6503-453e-b53a-067af7b0a377;name:Decorated Experiment>:\tstop: phase=run\n",
      "INFO: 2019-03-14 16:24:39.418 Experiment<id:Decorated Experiment>:\tstop: phase=experiment\n"
     ]
    }
   ],
   "source": [
    "ex = pypadre.experiments.run(decorated=True)\n",
    "ex = run(\"Decorated Experiment\")  # run the experiment and report\n",
    "# Aimed at engineers and scientists for quick prototyping and trying out features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and running experiments via pure code (example.py)\n",
    "def create_test_pipeline():\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.decomposition import PCA\n",
    "    # estimators = [('PCA', PCA()), ('clf', SVC())]\n",
    "    estimators = [('SVC', SVC(probability=True))]\n",
    "    return Pipeline(estimators)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: 2019-03-14 16:24:55.163 Experiment<id:Test Experiment SVM>:\tstart: phase=experiment\n",
      "INFO: 2019-03-14 16:24:55.539 Experiment<id:Test Experiment SVM>:\tstart: phase=experiment\n",
      "INFO: 2019-03-14 16:24:55.884 \tRun<id:9726c66f-05d7-4b7f-9283-1dea7ae17e07;name:Test Experiment SVM>:\tstart: phase=run\n",
      "INFO: 2019-03-14 16:24:56.182 \tRun<id:9726c66f-05d7-4b7f-9283-1dea7ae17e07;name:Test Experiment SVM>:\tstart: phase=run\n",
      "INFO: 2019-03-14 16:24:56.347 \t\tSplit<id:f4191096-8a98-4987-97cc-2d684747c604;name:Test Experiment SVM>:\tstart: phase=split\n",
      "INFO: 2019-03-14 16:24:56.498 \t\tSplit<id:f4191096-8a98-4987-97cc-2d684747c604;name:Test Experiment SVM>:\tstart: phase=split\n",
      "INFO: 2019-03-14 16:24:57.477 \t\tSplit<id:f4191096-8a98-4987-97cc-2d684747c604;name:Test Experiment SVM>:\tstop: phase=split\n",
      "INFO: 2019-03-14 16:24:58.118 \t\tSplit<id:f4191096-8a98-4987-97cc-2d684747c604;name:Test Experiment SVM>:\tstop: phase=split\n",
      "INFO: 2019-03-14 16:24:58.150 \tRun<id:9726c66f-05d7-4b7f-9283-1dea7ae17e07;name:Test Experiment SVM>:\tstop: phase=run\n",
      "INFO: 2019-03-14 16:24:58.150 \tRun<id:9726c66f-05d7-4b7f-9283-1dea7ae17e07;name:Test Experiment SVM>:\tstop: phase=run\n",
      "INFO: 2019-03-14 16:24:58.166 Experiment<id:Test Experiment SVM>:\tstop: phase=experiment\n",
      "INFO: 2019-03-14 16:24:58.166 Experiment<id:Test Experiment SVM>:\tstop: phase=experiment\n"
     ]
    }
   ],
   "source": [
    "from padre.ds_import import load_sklearn_toys\n",
    "from padre.core import Experiment\n",
    "from padre.base import PadreLogger\n",
    "from padre.eventhandler import add_logger\n",
    "\n",
    "logger = PadreLogger()\n",
    "logger.backend = pypadre.repository\n",
    "add_logger(logger=logger)\n",
    "ds = [i for i in load_sklearn_toys()][2]\n",
    "ex = Experiment(name=\"Test Experiment SVM\",\n",
    "                    description=\"Testing Support Vector Machines via SKLearn Pipeline\",\n",
    "                    dataset=ds,\n",
    "                    workflow=create_test_pipeline(), keep_splits=True, strategy=\"random\")\n",
    "ex.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The PyTorch wrapper supports the use of User Defined Callbacks \n",
    "from padre.core.wrappers.wrapper_pytorch import WrapperPytorch, CallBack\n",
    "class TestCallbacks(CallBack):\n",
    "    def on_compute_loss(self, loss):\n",
    "        print('Function on compute loss. Loss = {loss}'.format(loss=loss))\n",
    "\n",
    "    def on_epoch_end(self, obj):\n",
    "        print('Epoch ended')\n",
    "\n",
    "    def on_epoch_start(self, obj):\n",
    "        print('Epoch started')\n",
    "\n",
    "    def on_iteration_start(self, obj):\n",
    "        print('Iteration started')\n",
    "\n",
    "    def on_iteration_end(self, obj):\n",
    "        print('Iteration ended')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/christofer/PycharmProjects/PyPaDRe/tests/examples\n",
      "Epoch started\n",
      "Epoch started\n",
      "Epoch started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.23088191347808237\n",
      "Function on compute loss. Loss = 0.23088191347808237\n",
      "Function on compute loss. Loss = 0.23088191347808237\n",
      "1 0.23088191347808237\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.20520843390277194\n",
      "Function on compute loss. Loss = 0.20520843390277194\n",
      "Function on compute loss. Loss = 0.20520843390277194\n",
      "2 0.20520843390277194\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.25276096161270867\n",
      "Function on compute loss. Loss = 0.25276096161270867\n",
      "Function on compute loss. Loss = 0.25276096161270867\n",
      "3 0.25276096161270867\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.20848914065546884\n",
      "Function on compute loss. Loss = 0.20848914065546884\n",
      "Function on compute loss. Loss = 0.20848914065546884\n",
      "4 0.20848914065546884\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.22483836431635945\n",
      "Function on compute loss. Loss = 0.22483836431635945\n",
      "Function on compute loss. Loss = 0.22483836431635945\n",
      "5 0.22483836431635945\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.22820878808746667\n",
      "Function on compute loss. Loss = 0.22820878808746667\n",
      "Function on compute loss. Loss = 0.22820878808746667\n",
      "6 0.22820878808746667\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.24380103168600223\n",
      "Function on compute loss. Loss = 0.24380103168600223\n",
      "Function on compute loss. Loss = 0.24380103168600223\n",
      "7 0.24380103168600223\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.2116314363444565\n",
      "Function on compute loss. Loss = 0.2116314363444565\n",
      "Function on compute loss. Loss = 0.2116314363444565\n",
      "8 0.2116314363444565\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.24229064479439957\n",
      "Function on compute loss. Loss = 0.24229064479439957\n",
      "Function on compute loss. Loss = 0.24229064479439957\n",
      "9 0.24229064479439957\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.23376550714105726\n",
      "Function on compute loss. Loss = 0.23376550714105726\n",
      "Function on compute loss. Loss = 0.23376550714105726\n",
      "10 0.23376550714105726\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.21233257025630617\n",
      "Function on compute loss. Loss = 0.21233257025630617\n",
      "Function on compute loss. Loss = 0.21233257025630617\n",
      "11 0.21233257025630617\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.22604745727894435\n",
      "Function on compute loss. Loss = 0.22604745727894435\n",
      "Function on compute loss. Loss = 0.22604745727894435\n",
      "12 0.22604745727894435\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.20004026794921728\n",
      "Function on compute loss. Loss = 0.20004026794921728\n",
      "Function on compute loss. Loss = 0.20004026794921728\n",
      "13 0.20004026794921728\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.21414823756182536\n",
      "Function on compute loss. Loss = 0.21414823756182536\n",
      "Function on compute loss. Loss = 0.21414823756182536\n",
      "14 0.21414823756182536\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.2299980814534527\n",
      "Function on compute loss. Loss = 0.2299980814534527\n",
      "Function on compute loss. Loss = 0.2299980814534527\n",
      "15 0.2299980814534527\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.2567440498025843\n",
      "Function on compute loss. Loss = 0.2567440498025843\n",
      "Function on compute loss. Loss = 0.2567440498025843\n",
      "16 0.2567440498025843\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.23692713376642663\n",
      "Function on compute loss. Loss = 0.23692713376642663\n",
      "Function on compute loss. Loss = 0.23692713376642663\n",
      "17 0.23692713376642663\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.21592100032197378\n",
      "Function on compute loss. Loss = 0.21592100032197378\n",
      "Function on compute loss. Loss = 0.21592100032197378\n",
      "18 0.21592100032197378\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.22022693301928462\n",
      "Function on compute loss. Loss = 0.22022693301928462\n",
      "Function on compute loss. Loss = 0.22022693301928462\n",
      "19 0.22022693301928462\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.21633619906716023\n",
      "Function on compute loss. Loss = 0.21633619906716023\n",
      "Function on compute loss. Loss = 0.21633619906716023\n",
      "20 0.21633619906716023\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.21952490600585026\n",
      "Function on compute loss. Loss = 0.21952490600585026\n",
      "Function on compute loss. Loss = 0.21952490600585026\n",
      "21 0.21952490600585026\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.22586314510833416\n",
      "Function on compute loss. Loss = 0.22586314510833416\n",
      "Function on compute loss. Loss = 0.22586314510833416\n",
      "22 0.22586314510833416\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.23027271396076426\n",
      "Function on compute loss. Loss = 0.23027271396076426\n",
      "Function on compute loss. Loss = 0.23027271396076426\n",
      "23 0.23027271396076426\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Epoch ended\n",
      "Epoch ended\n",
      "Epoch ended\n",
      "Epoch started\n",
      "Epoch started\n",
      "Epoch started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.23580122322307454\n",
      "Function on compute loss. Loss = 0.23580122322307454\n",
      "Function on compute loss. Loss = 0.23580122322307454\n",
      "24 0.23580122322307454\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.23646277162752977\n",
      "Function on compute loss. Loss = 0.23646277162752977\n",
      "Function on compute loss. Loss = 0.23646277162752977\n",
      "25 0.23646277162752977\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.22072457520306915\n",
      "Function on compute loss. Loss = 0.22072457520306915\n",
      "Function on compute loss. Loss = 0.22072457520306915\n",
      "26 0.22072457520306915\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.1969304071976967\n",
      "Function on compute loss. Loss = 0.1969304071976967\n",
      "Function on compute loss. Loss = 0.1969304071976967\n",
      "27 0.1969304071976967\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.2235407634702819\n",
      "Function on compute loss. Loss = 0.2235407634702819\n",
      "Function on compute loss. Loss = 0.2235407634702819\n",
      "28 0.2235407634702819\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.2517499270446117\n",
      "Function on compute loss. Loss = 0.2517499270446117\n",
      "Function on compute loss. Loss = 0.2517499270446117\n",
      "29 0.2517499270446117\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.22124539847166969\n",
      "Function on compute loss. Loss = 0.22124539847166969\n",
      "Function on compute loss. Loss = 0.22124539847166969\n",
      "30 0.22124539847166969\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.20729823797773994\n",
      "Function on compute loss. Loss = 0.20729823797773994\n",
      "Function on compute loss. Loss = 0.20729823797773994\n",
      "31 0.20729823797773994\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.1952499272608582\n",
      "Function on compute loss. Loss = 0.1952499272608582\n",
      "Function on compute loss. Loss = 0.1952499272608582\n",
      "32 0.1952499272608582\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.2184259710703824\n",
      "Function on compute loss. Loss = 0.2184259710703824\n",
      "Function on compute loss. Loss = 0.2184259710703824\n",
      "33 0.2184259710703824\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.2268889996355333\n",
      "Function on compute loss. Loss = 0.2268889996355333\n",
      "Function on compute loss. Loss = 0.2268889996355333\n",
      "34 0.2268889996355333\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.24334782973456578\n",
      "Function on compute loss. Loss = 0.24334782973456578\n",
      "Function on compute loss. Loss = 0.24334782973456578\n",
      "35 0.24334782973456578\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.2024565369393029\n",
      "Function on compute loss. Loss = 0.2024565369393029\n",
      "Function on compute loss. Loss = 0.2024565369393029\n",
      "36 0.2024565369393029\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.22173969197742113\n",
      "Function on compute loss. Loss = 0.22173969197742113\n",
      "Function on compute loss. Loss = 0.22173969197742113\n",
      "37 0.22173969197742113\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.2120844970862711\n",
      "Function on compute loss. Loss = 0.2120844970862711\n",
      "Function on compute loss. Loss = 0.2120844970862711\n",
      "38 0.2120844970862711\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.23889904689780392\n",
      "Function on compute loss. Loss = 0.23889904689780392\n",
      "Function on compute loss. Loss = 0.23889904689780392\n",
      "39 0.23889904689780392\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.2245927769044965\n",
      "Function on compute loss. Loss = 0.2245927769044965\n",
      "Function on compute loss. Loss = 0.2245927769044965\n",
      "40 0.2245927769044965\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.2530187659989192\n",
      "Function on compute loss. Loss = 0.2530187659989192\n",
      "Function on compute loss. Loss = 0.2530187659989192\n",
      "41 0.2530187659989192\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.2077013227849203\n",
      "Function on compute loss. Loss = 0.2077013227849203\n",
      "Function on compute loss. Loss = 0.2077013227849203\n",
      "42 0.2077013227849203\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.2530258144611732\n",
      "Function on compute loss. Loss = 0.2530258144611732\n",
      "Function on compute loss. Loss = 0.2530258144611732\n",
      "43 0.2530258144611732\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.22430767898158138\n",
      "Function on compute loss. Loss = 0.22430767898158138\n",
      "Function on compute loss. Loss = 0.22430767898158138\n",
      "44 0.22430767898158138\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.233737132819693\n",
      "Function on compute loss. Loss = 0.233737132819693\n",
      "Function on compute loss. Loss = 0.233737132819693\n",
      "45 0.233737132819693\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Epoch ended\n",
      "Epoch ended\n",
      "Epoch ended\n",
      "Epoch started\n",
      "Epoch started\n",
      "Epoch started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.21578722054870225\n",
      "Function on compute loss. Loss = 0.21578722054870225\n",
      "Function on compute loss. Loss = 0.21578722054870225\n",
      "46 0.21578722054870225\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.21874811512057396\n",
      "Function on compute loss. Loss = 0.21874811512057396\n",
      "Function on compute loss. Loss = 0.21874811512057396\n",
      "47 0.21874811512057396\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.22640187896702502\n",
      "Function on compute loss. Loss = 0.22640187896702502\n",
      "Function on compute loss. Loss = 0.22640187896702502\n",
      "48 0.22640187896702502\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.2541124507470268\n",
      "Function on compute loss. Loss = 0.2541124507470268\n",
      "Function on compute loss. Loss = 0.2541124507470268\n",
      "49 0.2541124507470268\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.23750367409135723\n",
      "Function on compute loss. Loss = 0.23750367409135723\n",
      "Function on compute loss. Loss = 0.23750367409135723\n",
      "50 0.23750367409135723\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.23883385247649505\n",
      "Function on compute loss. Loss = 0.23883385247649505\n",
      "Function on compute loss. Loss = 0.23883385247649505\n",
      "51 0.23883385247649505\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.22360733023987875\n",
      "Function on compute loss. Loss = 0.22360733023987875\n",
      "Function on compute loss. Loss = 0.22360733023987875\n",
      "52 0.22360733023987875\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.22770133583173974\n",
      "Function on compute loss. Loss = 0.22770133583173974\n",
      "Function on compute loss. Loss = 0.22770133583173974\n",
      "53 0.22770133583173974\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.21820686464397115\n",
      "Function on compute loss. Loss = 0.21820686464397115\n",
      "Function on compute loss. Loss = 0.21820686464397115\n",
      "54 0.21820686464397115\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.1987648565819397\n",
      "Function on compute loss. Loss = 0.1987648565819397\n",
      "Function on compute loss. Loss = 0.1987648565819397\n",
      "55 0.1987648565819397\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.22773506820496872\n",
      "Function on compute loss. Loss = 0.22773506820496872\n",
      "Function on compute loss. Loss = 0.22773506820496872\n",
      "56 0.22773506820496872\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.2145962241669797\n",
      "Function on compute loss. Loss = 0.2145962241669797\n",
      "Function on compute loss. Loss = 0.2145962241669797\n",
      "57 0.2145962241669797\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.22566483886189673\n",
      "Function on compute loss. Loss = 0.22566483886189673\n",
      "Function on compute loss. Loss = 0.22566483886189673\n",
      "58 0.22566483886189673\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.23798184971071326\n",
      "Function on compute loss. Loss = 0.23798184971071326\n",
      "Function on compute loss. Loss = 0.23798184971071326\n",
      "59 0.23798184971071326\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.21901169013073563\n",
      "Function on compute loss. Loss = 0.21901169013073563\n",
      "Function on compute loss. Loss = 0.21901169013073563\n",
      "60 0.21901169013073563\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.21892294419321426\n",
      "Function on compute loss. Loss = 0.21892294419321426\n",
      "Function on compute loss. Loss = 0.21892294419321426\n",
      "61 0.21892294419321426\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.22678089278685107\n",
      "Function on compute loss. Loss = 0.22678089278685107\n",
      "Function on compute loss. Loss = 0.22678089278685107\n",
      "62 0.22678089278685107\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.2075326404359626\n",
      "Function on compute loss. Loss = 0.2075326404359626\n",
      "Function on compute loss. Loss = 0.2075326404359626\n",
      "63 0.2075326404359626\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.2277597705572375\n",
      "Function on compute loss. Loss = 0.2277597705572375\n",
      "Function on compute loss. Loss = 0.2277597705572375\n",
      "64 0.2277597705572375\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.2100107141312386\n",
      "Function on compute loss. Loss = 0.2100107141312386\n",
      "Function on compute loss. Loss = 0.2100107141312386\n",
      "65 0.2100107141312386\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.2532548973880904\n",
      "Function on compute loss. Loss = 0.2532548973880904\n",
      "Function on compute loss. Loss = 0.2532548973880904\n",
      "66 0.2532548973880904\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.2117075597817456\n",
      "Function on compute loss. Loss = 0.2117075597817456\n",
      "Function on compute loss. Loss = 0.2117075597817456\n",
      "67 0.2117075597817456\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.23029406212689968\n",
      "Function on compute loss. Loss = 0.23029406212689968\n",
      "Function on compute loss. Loss = 0.23029406212689968\n",
      "68 0.23029406212689968\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Epoch ended\n",
      "Epoch ended\n",
      "Epoch ended\n",
      "Epoch started\n",
      "Epoch started\n",
      "Epoch started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.19855421203990187\n",
      "Function on compute loss. Loss = 0.19855421203990187\n",
      "Function on compute loss. Loss = 0.19855421203990187\n",
      "69 0.19855421203990187\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.20570915319443148\n",
      "Function on compute loss. Loss = 0.20570915319443148\n",
      "Function on compute loss. Loss = 0.20570915319443148\n",
      "70 0.20570915319443148\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.22753422764950176\n",
      "Function on compute loss. Loss = 0.22753422764950176\n",
      "Function on compute loss. Loss = 0.22753422764950176\n",
      "71 0.22753422764950176\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.2263659338463362\n",
      "Function on compute loss. Loss = 0.2263659338463362\n",
      "Function on compute loss. Loss = 0.2263659338463362\n",
      "72 0.2263659338463362\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.23295882299466514\n",
      "Function on compute loss. Loss = 0.23295882299466514\n",
      "Function on compute loss. Loss = 0.23295882299466514\n",
      "73 0.23295882299466514\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.2327756218968959\n",
      "Function on compute loss. Loss = 0.2327756218968959\n",
      "Function on compute loss. Loss = 0.2327756218968959\n",
      "74 0.2327756218968959\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.22470709731653749\n",
      "Function on compute loss. Loss = 0.22470709731653749\n",
      "Function on compute loss. Loss = 0.22470709731653749\n",
      "75 0.22470709731653749\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.25584478662706994\n",
      "Function on compute loss. Loss = 0.25584478662706994\n",
      "Function on compute loss. Loss = 0.25584478662706994\n",
      "76 0.25584478662706994\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.21189154518381198\n",
      "Function on compute loss. Loss = 0.21189154518381198\n",
      "Function on compute loss. Loss = 0.21189154518381198\n",
      "77 0.21189154518381198\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.23248027364924032\n",
      "Function on compute loss. Loss = 0.23248027364924032\n",
      "Function on compute loss. Loss = 0.23248027364924032\n",
      "78 0.23248027364924032\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.22555163511476758\n",
      "Function on compute loss. Loss = 0.22555163511476758\n",
      "Function on compute loss. Loss = 0.22555163511476758\n",
      "79 0.22555163511476758\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.23598262284167543\n",
      "Function on compute loss. Loss = 0.23598262284167543\n",
      "Function on compute loss. Loss = 0.23598262284167543\n",
      "80 0.23598262284167543\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.22746236706136586\n",
      "Function on compute loss. Loss = 0.22746236706136586\n",
      "Function on compute loss. Loss = 0.22746236706136586\n",
      "81 0.22746236706136586\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.2227924822050397\n",
      "Function on compute loss. Loss = 0.2227924822050397\n",
      "Function on compute loss. Loss = 0.2227924822050397\n",
      "82 0.2227924822050397\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.22431219946959213\n",
      "Function on compute loss. Loss = 0.22431219946959213\n",
      "Function on compute loss. Loss = 0.22431219946959213\n",
      "83 0.22431219946959213\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.22179172193638727\n",
      "Function on compute loss. Loss = 0.22179172193638727\n",
      "Function on compute loss. Loss = 0.22179172193638727\n",
      "84 0.22179172193638727\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function on compute loss. Loss = 0.20790900707194473\n",
      "Function on compute loss. Loss = 0.20790900707194473\n",
      "Function on compute loss. Loss = 0.20790900707194473\n",
      "85 0.20790900707194473\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.2526666607052099\n",
      "Function on compute loss. Loss = 0.2526666607052099\n",
      "Function on compute loss. Loss = 0.2526666607052099\n",
      "86 0.2526666607052099\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.23232069352115173\n",
      "Function on compute loss. Loss = 0.23232069352115173\n",
      "Function on compute loss. Loss = 0.23232069352115173\n",
      "87 0.23232069352115173\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.22017709531959082\n",
      "Function on compute loss. Loss = 0.22017709531959082\n",
      "Function on compute loss. Loss = 0.22017709531959082\n",
      "88 0.22017709531959082\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.21545010844939652\n",
      "Function on compute loss. Loss = 0.21545010844939652\n",
      "Function on compute loss. Loss = 0.21545010844939652\n",
      "89 0.21545010844939652\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.21346532897144216\n",
      "Function on compute loss. Loss = 0.21346532897144216\n",
      "Function on compute loss. Loss = 0.21346532897144216\n",
      "90 0.21346532897144216\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Epoch ended\n",
      "Epoch ended\n",
      "Epoch ended\n",
      "Epoch started\n",
      "Epoch started\n",
      "Epoch started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.2175726357012772\n",
      "Function on compute loss. Loss = 0.2175726357012772\n",
      "Function on compute loss. Loss = 0.2175726357012772\n",
      "91 0.2175726357012772\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.2488112987652333\n",
      "Function on compute loss. Loss = 0.2488112987652333\n",
      "Function on compute loss. Loss = 0.2488112987652333\n",
      "92 0.2488112987652333\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.2068455063083652\n",
      "Function on compute loss. Loss = 0.2068455063083652\n",
      "Function on compute loss. Loss = 0.2068455063083652\n",
      "93 0.2068455063083652\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.2231140092256901\n",
      "Function on compute loss. Loss = 0.2231140092256901\n",
      "Function on compute loss. Loss = 0.2231140092256901\n",
      "94 0.2231140092256901\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.22521863267538625\n",
      "Function on compute loss. Loss = 0.22521863267538625\n",
      "Function on compute loss. Loss = 0.22521863267538625\n",
      "95 0.22521863267538625\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.2167954849500785\n",
      "Function on compute loss. Loss = 0.2167954849500785\n",
      "Function on compute loss. Loss = 0.2167954849500785\n",
      "96 0.2167954849500785\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.24423626524910494\n",
      "Function on compute loss. Loss = 0.24423626524910494\n",
      "Function on compute loss. Loss = 0.24423626524910494\n",
      "97 0.24423626524910494\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.24372002232632034\n",
      "Function on compute loss. Loss = 0.24372002232632034\n",
      "Function on compute loss. Loss = 0.24372002232632034\n",
      "98 0.24372002232632034\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.2432887613332273\n",
      "Function on compute loss. Loss = 0.2432887613332273\n",
      "Function on compute loss. Loss = 0.2432887613332273\n",
      "99 0.2432887613332273\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration started\n",
      "Iteration started\n",
      "Iteration started\n",
      "Function on compute loss. Loss = 0.2077723339527314\n",
      "Function on compute loss. Loss = 0.2077723339527314\n",
      "Function on compute loss. Loss = 0.2077723339527314\n",
      "100 0.2077723339527314\n",
      "Iteration ended\n",
      "Iteration ended\n",
      "Iteration ended\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christofer/PycharmProjects/test_folder/venv/lib/python3.6/site-packages/torch/nn/modules/container.py:91: UserWarning: Implicit dimension choice for softmin has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    }
   ],
   "source": [
    "# Running an experiment with a custom wrapper\n",
    "from sklearn.pipeline import Pipeline\n",
    "import os\n",
    "\n",
    "print(os.getcwd())\n",
    "# Define the parameters\n",
    "params = dict()\n",
    "\n",
    "import json\n",
    "with open('../proof_of_concept/Pytorch/classification.json') as json_data:\n",
    "    params = json.load(json_data)\n",
    "\n",
    "obj = WrapperPytorch(params=params)\n",
    "obj.set_callbacks([TestCallbacks()])\n",
    "estimators = [('pytorch', obj)]\n",
    "workflow = Pipeline(estimators)\n",
    "ds = [i for i in load_sklearn_toys()][4]\n",
    "\n",
    "ex = Experiment(name=\"Torch\",\n",
    "                description=\"Testing Torch via SKLearn Pipeline\",\n",
    "                dataset=ds,\n",
    "                workflow=workflow)\n",
    "ex.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential Execution\n",
      "{'Step_0': {'algorithm': {'attributes': {'path': '.steps[0][1]'},\n",
      "                          'value': 'isomap embedding'},\n",
      "            'doc': {'attributes': {'path': '.steps[0][1].__doc__'},\n",
      "                    'value': 'Isomap Embedding\\n'\n",
      "                             '\\n'\n",
      "                             '    Non-linear dimensionality reduction through '\n",
      "                             'Isometric Mapping\\n'\n",
      "                             '\\n'\n",
      "                             '    Read more in the :ref:`User Guide '\n",
      "                             '<isomap>`.\\n'\n",
      "                             '\\n'\n",
      "                             '    Parameters\\n'\n",
      "                             '    ----------\\n'\n",
      "                             '    n_neighbors : integer\\n'\n",
      "                             '        number of neighbors to consider for each '\n",
      "                             'point.\\n'\n",
      "                             '\\n'\n",
      "                             '    n_components : integer\\n'\n",
      "                             '        number of coordinates for the manifold\\n'\n",
      "                             '\\n'\n",
      "                             \"    eigen_solver : ['auto'|'arpack'|'dense']\\n\"\n",
      "                             \"        'auto' : Attempt to choose the most \"\n",
      "                             'efficient solver\\n'\n",
      "                             '        for the given problem.\\n'\n",
      "                             '\\n'\n",
      "                             \"        'arpack' : Use Arnoldi decomposition to \"\n",
      "                             'find the eigenvalues\\n'\n",
      "                             '        and eigenvectors.\\n'\n",
      "                             '\\n'\n",
      "                             \"        'dense' : Use a direct solver (i.e. \"\n",
      "                             'LAPACK)\\n'\n",
      "                             '        for the eigenvalue decomposition.\\n'\n",
      "                             '\\n'\n",
      "                             '    tol : float\\n'\n",
      "                             '        Convergence tolerance passed to arpack '\n",
      "                             'or lobpcg.\\n'\n",
      "                             \"        not used if eigen_solver == 'dense'.\\n\"\n",
      "                             '\\n'\n",
      "                             '    max_iter : integer\\n'\n",
      "                             '        Maximum number of iterations for the '\n",
      "                             'arpack solver.\\n'\n",
      "                             \"        not used if eigen_solver == 'dense'.\\n\"\n",
      "                             '\\n'\n",
      "                             \"    path_method : string ['auto'|'FW'|'D']\\n\"\n",
      "                             '        Method to use in finding shortest path.\\n'\n",
      "                             '\\n'\n",
      "                             \"        'auto' : attempt to choose the best \"\n",
      "                             'algorithm automatically.\\n'\n",
      "                             '\\n'\n",
      "                             \"        'FW' : Floyd-Warshall algorithm.\\n\"\n",
      "                             '\\n'\n",
      "                             \"        'D' : Dijkstra's algorithm.\\n\"\n",
      "                             '\\n'\n",
      "                             '    neighbors_algorithm : string '\n",
      "                             \"['auto'|'brute'|'kd_tree'|'ball_tree']\\n\"\n",
      "                             '        Algorithm to use for nearest neighbors '\n",
      "                             'search,\\n'\n",
      "                             '        passed to neighbors.NearestNeighbors '\n",
      "                             'instance.\\n'\n",
      "                             '\\n'\n",
      "                             '    n_jobs : int, optional (default = 1)\\n'\n",
      "                             '        The number of parallel jobs to run.\\n'\n",
      "                             '        If ``-1``, then the number of jobs is '\n",
      "                             'set to the number of CPU cores.\\n'\n",
      "                             '\\n'\n",
      "                             '    Attributes\\n'\n",
      "                             '    ----------\\n'\n",
      "                             '    embedding_ : array-like, shape (n_samples, '\n",
      "                             'n_components)\\n'\n",
      "                             '        Stores the embedding vectors.\\n'\n",
      "                             '\\n'\n",
      "                             '    kernel_pca_ : object\\n'\n",
      "                             '        `KernelPCA` object used to implement the '\n",
      "                             'embedding.\\n'\n",
      "                             '\\n'\n",
      "                             '    training_data_ : array-like, shape '\n",
      "                             '(n_samples, n_features)\\n'\n",
      "                             '        Stores the training data.\\n'\n",
      "                             '\\n'\n",
      "                             '    nbrs_ : sklearn.neighbors.NearestNeighbors '\n",
      "                             'instance\\n'\n",
      "                             '        Stores nearest neighbors instance, '\n",
      "                             'including BallTree or KDtree\\n'\n",
      "                             '        if applicable.\\n'\n",
      "                             '\\n'\n",
      "                             '    dist_matrix_ : array-like, shape (n_samples, '\n",
      "                             'n_samples)\\n'\n",
      "                             '        Stores the geodesic distance matrix of '\n",
      "                             'training data.\\n'\n",
      "                             '\\n'\n",
      "                             '    References\\n'\n",
      "                             '    ----------\\n'\n",
      "                             '\\n'\n",
      "                             '    .. [1] Tenenbaum, J.B.; De Silva, V.; & '\n",
      "                             'Langford, J.C. A global geometric\\n'\n",
      "                             '           framework for nonlinear '\n",
      "                             'dimensionality reduction. Science 290 (5500)\\n'\n",
      "                             '    '},\n",
      "            'hyper_parameters': {'execution_parameters': {},\n",
      "                                 'model_parameters': {'eigen_solver': {'attributes': {'path': '.steps[0][1].eigen_solver'},\n",
      "                                                                       'value': 'auto'},\n",
      "                                                      'neighbors_algorithm': {'attributes': {'path': '.steps[0][1].neighbors_algorithm'},\n",
      "                                                                              'value': 'auto'},\n",
      "                                                      'num_components': {'attributes': {'path': '.steps[0][1].n_components'},\n",
      "                                                                         'value': 2},\n",
      "                                                      'num_neighbours': {'attributes': {'path': '.steps[0][1].n_neighbors'},\n",
      "                                                                         'value': 5},\n",
      "                                                      'path_method': {'attributes': {'path': '.steps[0][1].path_method'},\n",
      "                                                                      'value': 'auto'},\n",
      "                                                      'tolerance': {'attributes': {'path': '.steps[0][1].tol'},\n",
      "                                                                    'value': 0}},\n",
      "                                 'optimisation_parameters': {'jobs': {'attributes': {'path': '.steps[0][1].n_jobs'},\n",
      "                                                                      'value': 1},\n",
      "                                                             'max_iterations': {'attributes': {'path': '.steps[0][1].max_iter'},\n",
      "                                                                                'value': None}}}}}\n",
      "{'Step_0': {'algorithm': {'attributes': {'path': '.steps[0][1]'},\n",
      "                          'value': 'principal component analysis'},\n",
      "            'doc': {'attributes': {'path': '.steps[0][1].__doc__'},\n",
      "                    'value': 'Principal component analysis (PCA)\\n'\n",
      "                             '\\n'\n",
      "                             '    Linear dimensionality reduction using '\n",
      "                             'Singular Value Decomposition of the\\n'\n",
      "                             '    data to project it to a lower dimensional '\n",
      "                             'space.\\n'\n",
      "                             '\\n'\n",
      "                             '    It uses the LAPACK implementation of the '\n",
      "                             'full SVD or a randomized truncated\\n'\n",
      "                             '    SVD by the method of Halko et al. 2009, '\n",
      "                             'depending on the shape of the input\\n'\n",
      "                             '    data and the number of components to '\n",
      "                             'extract.\\n'\n",
      "                             '\\n'\n",
      "                             '    It can also use the scipy.sparse.linalg '\n",
      "                             'ARPACK implementation of the\\n'\n",
      "                             '    truncated SVD.\\n'\n",
      "                             '\\n'\n",
      "                             '    Notice that this class does not support '\n",
      "                             'sparse input. See\\n'\n",
      "                             '    :class:`TruncatedSVD` for an alternative '\n",
      "                             'with sparse data.\\n'\n",
      "                             '\\n'\n",
      "                             '    Read more in the :ref:`User Guide <PCA>`.\\n'\n",
      "                             '\\n'\n",
      "                             '    Parameters\\n'\n",
      "                             '    ----------\\n'\n",
      "                             '    n_components : int, float, None or string\\n'\n",
      "                             '        Number of components to keep.\\n'\n",
      "                             '        if n_components is not set all '\n",
      "                             'components are kept::\\n'\n",
      "                             '\\n'\n",
      "                             '            n_components == min(n_samples, '\n",
      "                             'n_features)\\n'\n",
      "                             '\\n'\n",
      "                             \"        if n_components == 'mle' and svd_solver \"\n",
      "                             \"== 'full', Minka's MLE is used\\n\"\n",
      "                             '        to guess the dimension\\n'\n",
      "                             '        if ``0 < n_components < 1`` and '\n",
      "                             \"svd_solver == 'full', select the number\\n\"\n",
      "                             '        of components such that the amount of '\n",
      "                             'variance that needs to be\\n'\n",
      "                             '        explained is greater than the percentage '\n",
      "                             'specified by n_components\\n'\n",
      "                             '        n_components cannot be equal to '\n",
      "                             \"n_features for svd_solver == 'arpack'.\\n\"\n",
      "                             '\\n'\n",
      "                             '    copy : bool (default True)\\n'\n",
      "                             '        If False, data passed to fit are '\n",
      "                             'overwritten and running\\n'\n",
      "                             '        fit(X).transform(X) will not yield the '\n",
      "                             'expected results,\\n'\n",
      "                             '        use fit_transform(X) instead.\\n'\n",
      "                             '\\n'\n",
      "                             '    whiten : bool, optional (default False)\\n'\n",
      "                             '        When True (False by default) the '\n",
      "                             '`components_` vectors are multiplied\\n'\n",
      "                             '        by the square root of n_samples and then '\n",
      "                             'divided by the singular values\\n'\n",
      "                             '        to ensure uncorrelated outputs with unit '\n",
      "                             'component-wise variances.\\n'\n",
      "                             '\\n'\n",
      "                             '        Whitening will remove some information '\n",
      "                             'from the transformed signal\\n'\n",
      "                             '        (the relative variance scales of the '\n",
      "                             'components) but can sometime\\n'\n",
      "                             '        improve the predictive accuracy of the '\n",
      "                             'downstream estimators by\\n'\n",
      "                             '        making their data respect some '\n",
      "                             'hard-wired assumptions.\\n'\n",
      "                             '\\n'\n",
      "                             \"    svd_solver : string {'auto', 'full', \"\n",
      "                             \"'arpack', 'randomized'}\\n\"\n",
      "                             '        auto :\\n'\n",
      "                             '            the solver is selected by a default '\n",
      "                             'policy based on `X.shape` and\\n'\n",
      "                             '            `n_components`: if the input data is '\n",
      "                             'larger than 500x500 and the\\n'\n",
      "                             '            number of components to extract is '\n",
      "                             'lower than 80% of the smallest\\n'\n",
      "                             '            dimension of the data, then the more '\n",
      "                             \"efficient 'randomized'\\n\"\n",
      "                             '            method is enabled. Otherwise the '\n",
      "                             'exact full SVD is computed and\\n'\n",
      "                             '            optionally truncated afterwards.\\n'\n",
      "                             '        full :\\n'\n",
      "                             '            run exact full SVD calling the '\n",
      "                             'standard LAPACK solver via\\n'\n",
      "                             '            `scipy.linalg.svd` and select the '\n",
      "                             'components by postprocessing\\n'\n",
      "                             '        arpack :\\n'\n",
      "                             '            run SVD truncated to n_components '\n",
      "                             'calling ARPACK solver via\\n'\n",
      "                             '            `scipy.sparse.linalg.svds`. It '\n",
      "                             'requires strictly\\n'\n",
      "                             '            0 < n_components < X.shape[1]\\n'\n",
      "                             '        randomized :\\n'\n",
      "                             '            run randomized SVD by the method of '\n",
      "                             'Halko et al.\\n'\n",
      "                             '\\n'\n",
      "                             '        .. versionadded:: 0.18.0\\n'\n",
      "                             '\\n'\n",
      "                             '    tol : float >= 0, optional (default .0)\\n'\n",
      "                             '        Tolerance for singular values computed '\n",
      "                             \"by svd_solver == 'arpack'.\\n\"\n",
      "                             '\\n'\n",
      "                             '        .. versionadded:: 0.18.0\\n'\n",
      "                             '\\n'\n",
      "                             \"    iterated_power : int >= 0, or 'auto', \"\n",
      "                             \"(default 'auto')\\n\"\n",
      "                             '        Number of iterations for the power '\n",
      "                             'method computed by\\n'\n",
      "                             \"        svd_solver == 'randomized'.\\n\"\n",
      "                             '\\n'\n",
      "                             '        .. versionadded:: 0.18.0\\n'\n",
      "                             '\\n'\n",
      "                             '    random_state : int, RandomState instance or '\n",
      "                             'None, optional (default None)\\n'\n",
      "                             '        If int, random_state is the seed used by '\n",
      "                             'the random number generator;\\n'\n",
      "                             '        If RandomState instance, random_state is '\n",
      "                             'the random number generator;\\n'\n",
      "                             '        If None, the random number generator is '\n",
      "                             'the RandomState instance used\\n'\n",
      "                             '        by `np.random`. Used when ``svd_solver`` '\n",
      "                             \"== 'arpack' or 'randomized'.\\n\"\n",
      "                             '\\n'\n",
      "                             '        .. versionadded:: 0.18.0\\n'\n",
      "                             '\\n'\n",
      "                             '    Attributes\\n'\n",
      "                             '    ----------\\n'\n",
      "                             '    components_ : array, shape (n_components, '\n",
      "                             'n_features)\\n'\n",
      "                             '        Principal axes in feature space, '\n",
      "                             'representing the directions of\\n'\n",
      "                             '        maximum variance in the data. The '\n",
      "                             'components are sorted by\\n'\n",
      "                             '        ``explained_variance_``.\\n'\n",
      "                             '\\n'\n",
      "                             '    explained_variance_ : array, shape '\n",
      "                             '(n_components,)\\n'\n",
      "                             '        The amount of variance explained by each '\n",
      "                             'of the selected components.\\n'\n",
      "                             '\\n'\n",
      "                             '        Equal to n_components largest '\n",
      "                             'eigenvalues\\n'\n",
      "                             '        of the covariance matrix of X.\\n'\n",
      "                             '\\n'\n",
      "                             '        .. versionadded:: 0.18\\n'\n",
      "                             '\\n'\n",
      "                             '    explained_variance_ratio_ : array, shape '\n",
      "                             '(n_components,)\\n'\n",
      "                             '        Percentage of variance explained by each '\n",
      "                             'of the selected components.\\n'\n",
      "                             '\\n'\n",
      "                             '        If ``n_components`` is not set then all '\n",
      "                             'components are stored and the\\n'\n",
      "                             '        sum of explained variances is equal to '\n",
      "                             '1.0.\\n'\n",
      "                             '\\n'\n",
      "                             '    singular_values_ : array, shape '\n",
      "                             '(n_components,)\\n'\n",
      "                             '        The singular values corresponding to '\n",
      "                             'each of the selected components.\\n'\n",
      "                             '        The singular values are equal to the '\n",
      "                             '2-norms of the ``n_components``\\n'\n",
      "                             '        variables in the lower-dimensional '\n",
      "                             'space.\\n'\n",
      "                             '\\n'\n",
      "                             '    mean_ : array, shape (n_features,)\\n'\n",
      "                             '        Per-feature empirical mean, estimated '\n",
      "                             'from the training set.\\n'\n",
      "                             '\\n'\n",
      "                             '        Equal to `X.mean(axis=0)`.\\n'\n",
      "                             '\\n'\n",
      "                             '    n_components_ : int\\n'\n",
      "                             '        The estimated number of components. When '\n",
      "                             'n_components is set\\n'\n",
      "                             \"        to 'mle' or a number between 0 and 1 \"\n",
      "                             \"(with svd_solver == 'full') this\\n\"\n",
      "                             '        number is estimated from input data. '\n",
      "                             'Otherwise it equals the parameter\\n'\n",
      "                             '        n_components, or n_features if '\n",
      "                             'n_components is None.\\n'\n",
      "                             '\\n'\n",
      "                             '    noise_variance_ : float\\n'\n",
      "                             '        The estimated noise covariance following '\n",
      "                             'the Probabilistic PCA model\\n'\n",
      "                             '        from Tipping and Bishop 1999. See '\n",
      "                             '\"Pattern Recognition and\\n'\n",
      "                             '        Machine Learning\" by C. Bishop, 12.2.1 '\n",
      "                             'p. 574 or\\n'\n",
      "                             '        '\n",
      "                             'http://www.miketipping.com/papers/met-mppca.pdf. '\n",
      "                             'It is required to\\n'\n",
      "                             '        computed the estimated data covariance '\n",
      "                             'and score samples.\\n'\n",
      "                             '\\n'\n",
      "                             '        Equal to the average of (min(n_features, '\n",
      "                             'n_samples) - n_components)\\n'\n",
      "                             '        smallest eigenvalues of the covariance '\n",
      "                             'matrix of X.\\n'\n",
      "                             '\\n'\n",
      "                             '    References\\n'\n",
      "                             '    ----------\\n'\n",
      "                             \"    For n_components == 'mle', this class uses \"\n",
      "                             'the method of `Thomas P. Minka:\\n'\n",
      "                             '    Automatic Choice of Dimensionality for PCA. '\n",
      "                             'NIPS 2000: 598-604`\\n'\n",
      "                             '\\n'\n",
      "                             '    Implements the probabilistic PCA model '\n",
      "                             'from:\\n'\n",
      "                             '    M. Tipping and C. Bishop, Probabilistic '\n",
      "                             'Principal Component Analysis,\\n'\n",
      "                             '    Journal of the Royal Statistical Society, '\n",
      "                             'Series B, 61, Part 3, pp. 611-622\\n'\n",
      "                             '    via the score and score_samples methods.\\n'\n",
      "                             '    See '\n",
      "                             'http://www.miketipping.com/papers/met-mppca.pdf\\n'\n",
      "                             '\\n'\n",
      "                             \"    For svd_solver == 'arpack', refer to \"\n",
      "                             '`scipy.sparse.linalg.svds`.\\n'\n",
      "                             '\\n'\n",
      "                             \"    For svd_solver == 'randomized', see:\\n\"\n",
      "                             '    `Finding structure with randomness: '\n",
      "                             'Stochastic algorithms\\n'\n",
      "                             '    for constructing approximate matrix '\n",
      "                             'decompositions Halko, et al., 2009\\n'\n",
      "                             '    (arXiv:909)`\\n'\n",
      "                             '    `A randomized algorithm for the '\n",
      "                             'decomposition of matrices\\n'\n",
      "                             '    Per-Gunnar Martinsson, Vladimir Rokhlin and '\n",
      "                             'Mark Tygert`\\n'\n",
      "                             '\\n'\n",
      "                             '\\n'\n",
      "                             '    Examples\\n'\n",
      "                             '    --------\\n'\n",
      "                             '    >>> import numpy as np\\n'\n",
      "                             '    >>> from sklearn.decomposition import PCA\\n'\n",
      "                             '    >>> X = np.array([[-1, -1], [-2, -1], [-3, '\n",
      "                             '-2], [1, 1], [2, 1], [3, 2]])\\n'\n",
      "                             '    >>> pca = PCA(n_components=2)\\n'\n",
      "                             '    >>> pca.fit(X)\\n'\n",
      "                             \"    PCA(copy=True, iterated_power='auto', \"\n",
      "                             'n_components=2, random_state=None,\\n'\n",
      "                             \"      svd_solver='auto', tol=0.0, whiten=False)\\n\"\n",
      "                             '    >>> print(pca.explained_variance_ratio_)  # '\n",
      "                             'doctest: +ELLIPSIS\\n'\n",
      "                             '    [ 0.99244...  0.00755...]\\n'\n",
      "                             '    >>> print(pca.singular_values_)  # doctest: '\n",
      "                             '+ELLIPSIS\\n'\n",
      "                             '    [ 6.30061...  0.54980...]\\n'\n",
      "                             '\\n'\n",
      "                             '    >>> pca = PCA(n_components=2, '\n",
      "                             \"svd_solver='full')\\n\"\n",
      "                             '    >>> pca.fit(X)                 # doctest: '\n",
      "                             '+ELLIPSIS +NORMALIZE_WHITESPACE\\n'\n",
      "                             \"    PCA(copy=True, iterated_power='auto', \"\n",
      "                             'n_components=2, random_state=None,\\n'\n",
      "                             \"      svd_solver='full', tol=0.0, whiten=False)\\n\"\n",
      "                             '    >>> print(pca.explained_variance_ratio_)  # '\n",
      "                             'doctest: +ELLIPSIS\\n'\n",
      "                             '    [ 0.99244...  0.00755...]\\n'\n",
      "                             '    >>> print(pca.singular_values_)  # doctest: '\n",
      "                             '+ELLIPSIS\\n'\n",
      "                             '    [ 6.30061...  0.54980...]\\n'\n",
      "                             '\\n'\n",
      "                             '    >>> pca = PCA(n_components=1, '\n",
      "                             \"svd_solver='arpack')\\n\"\n",
      "                             '    >>> pca.fit(X)\\n'\n",
      "                             \"    PCA(copy=True, iterated_power='auto', \"\n",
      "                             'n_components=1, random_state=None,\\n'\n",
      "                             \"      svd_solver='arpack', tol=0.0, \"\n",
      "                             'whiten=False)\\n'\n",
      "                             '    >>> print(pca.explained_variance_ratio_)  # '\n",
      "                             'doctest: +ELLIPSIS\\n'\n",
      "                             '    [ 0.99244...]\\n'\n",
      "                             '    >>> print(pca.singular_values_)  # doctest: '\n",
      "                             '+ELLIPSIS\\n'\n",
      "                             '    [ 6.30061...]\\n'\n",
      "                             '\\n'\n",
      "                             '    See also\\n'\n",
      "                             '    --------\\n'\n",
      "                             '    KernelPCA\\n'\n",
      "                             '    SparsePCA\\n'\n",
      "                             '    TruncatedSVD\\n'\n",
      "                             '    IncrementalPCA\\n'\n",
      "                             '    '},\n",
      "            'hyper_parameters': {'execution_parameters': {},\n",
      "                                 'model_parameters': {'copy': {'attributes': {'path': '.steps[0][1].copy'},\n",
      "                                                               'value': True},\n",
      "                                                      'iterated_power': {'attributes': {'path': '.steps[0][1].iterated_power'},\n",
      "                                                                         'value': 'auto'},\n",
      "                                                      'num_components': {'attributes': {'path': '.steps[0][1].n_components'},\n",
      "                                                                         'value': None},\n",
      "                                                      'random_state': {'attributes': {'path': '.steps[0][1].random_state'},\n",
      "                                                                       'value': None},\n",
      "                                                      'svd_solver': {'attributes': {'path': '.steps[0][1].svd_solver'},\n",
      "                                                                     'value': 'auto'},\n",
      "                                                      'tolerance': {'attributes': {'path': '.steps[0][1].tol'},\n",
      "                                                                    'value': 0.0},\n",
      "                                                      'whiten': {'attributes': {'path': '.steps[0][1].whiten'},\n",
      "                                                                 'value': False}},\n",
      "                                 'optimisation_parameters': {}}},\n",
      " 'Step_1': {'algorithm': {'attributes': {'path': '.steps[1][1]'},\n",
      "                          'value': 'epsilon-support vector regression'},\n",
      "            'doc': {'attributes': {'path': '.steps[1][1].__doc__'},\n",
      "                    'value': 'Epsilon-Support Vector Regression.\\n'\n",
      "                             '\\n'\n",
      "                             '    The free parameters in the model are C and '\n",
      "                             'epsilon.\\n'\n",
      "                             '\\n'\n",
      "                             '    The implementation is based on libsvm.\\n'\n",
      "                             '\\n'\n",
      "                             '    Read more in the :ref:`User Guide '\n",
      "                             '<svm_regression>`.\\n'\n",
      "                             '\\n'\n",
      "                             '    Parameters\\n'\n",
      "                             '    ----------\\n'\n",
      "                             '    C : float, optional (default=1.0)\\n'\n",
      "                             '        Penalty parameter C of the error term.\\n'\n",
      "                             '\\n'\n",
      "                             '    epsilon : float, optional (default=0.1)\\n'\n",
      "                             '         Epsilon in the epsilon-SVR model. It '\n",
      "                             'specifies the epsilon-tube\\n'\n",
      "                             '         within which no penalty is associated '\n",
      "                             'in the training loss function\\n'\n",
      "                             '         with points predicted within a distance '\n",
      "                             'epsilon from the actual\\n'\n",
      "                             '         value.\\n'\n",
      "                             '\\n'\n",
      "                             \"    kernel : string, optional (default='rbf')\\n\"\n",
      "                             '         Specifies the kernel type to be used in '\n",
      "                             'the algorithm.\\n'\n",
      "                             \"         It must be one of 'linear', 'poly', \"\n",
      "                             \"'rbf', 'sigmoid', 'precomputed' or\\n\"\n",
      "                             '         a callable.\\n'\n",
      "                             \"         If none is given, 'rbf' will be used. \"\n",
      "                             'If a callable is given it is\\n'\n",
      "                             '         used to precompute the kernel matrix.\\n'\n",
      "                             '\\n'\n",
      "                             '    degree : int, optional (default=3)\\n'\n",
      "                             '        Degree of the polynomial kernel function '\n",
      "                             \"('poly').\\n\"\n",
      "                             '        Ignored by all other kernels.\\n'\n",
      "                             '\\n'\n",
      "                             \"    gamma : float, optional (default='auto')\\n\"\n",
      "                             \"        Kernel coefficient for 'rbf', 'poly' and \"\n",
      "                             \"'sigmoid'.\\n\"\n",
      "                             \"        If gamma is 'auto' then 1/n_features \"\n",
      "                             'will be used instead.\\n'\n",
      "                             '\\n'\n",
      "                             '    coef0 : float, optional (default=0.0)\\n'\n",
      "                             '        Independent term in kernel function.\\n'\n",
      "                             \"        It is only significant in 'poly' and \"\n",
      "                             \"'sigmoid'.\\n\"\n",
      "                             '\\n'\n",
      "                             '    shrinking : boolean, optional '\n",
      "                             '(default=True)\\n'\n",
      "                             '        Whether to use the shrinking heuristic.\\n'\n",
      "                             '\\n'\n",
      "                             '    tol : float, optional (default=1e-3)\\n'\n",
      "                             '        Tolerance for stopping criterion.\\n'\n",
      "                             '\\n'\n",
      "                             '    cache_size : float, optional\\n'\n",
      "                             '        Specify the size of the kernel cache (in '\n",
      "                             'MB).\\n'\n",
      "                             '\\n'\n",
      "                             '    verbose : bool, default: False\\n'\n",
      "                             '        Enable verbose output. Note that this '\n",
      "                             'setting takes advantage of a\\n'\n",
      "                             '        per-process runtime setting in libsvm '\n",
      "                             'that, if enabled, may not work\\n'\n",
      "                             '        properly in a multithreaded context.\\n'\n",
      "                             '\\n'\n",
      "                             '    max_iter : int, optional (default=-1)\\n'\n",
      "                             '        Hard limit on iterations within solver, '\n",
      "                             'or -1 for no limit.\\n'\n",
      "                             '\\n'\n",
      "                             '    Attributes\\n'\n",
      "                             '    ----------\\n'\n",
      "                             '    support_ : array-like, shape = [n_SV]\\n'\n",
      "                             '        Indices of support vectors.\\n'\n",
      "                             '\\n'\n",
      "                             '    support_vectors_ : array-like, shape = [nSV, '\n",
      "                             'n_features]\\n'\n",
      "                             '        Support vectors.\\n'\n",
      "                             '\\n'\n",
      "                             '    dual_coef_ : array, shape = [1, n_SV]\\n'\n",
      "                             '        Coefficients of the support vector in '\n",
      "                             'the decision function.\\n'\n",
      "                             '\\n'\n",
      "                             '    coef_ : array, shape = [1, n_features]\\n'\n",
      "                             '        Weights assigned to the features '\n",
      "                             '(coefficients in the primal\\n'\n",
      "                             '        problem). This is only available in the '\n",
      "                             'case of a linear kernel.\\n'\n",
      "                             '\\n'\n",
      "                             '        `coef_` is readonly property derived '\n",
      "                             'from `dual_coef_` and\\n'\n",
      "                             '        `support_vectors_`.\\n'\n",
      "                             '\\n'\n",
      "                             '    intercept_ : array, shape = [1]\\n'\n",
      "                             '        Constants in decision function.\\n'\n",
      "                             '\\n'\n",
      "                             '    sample_weight : array-like, shape = '\n",
      "                             '[n_samples]\\n'\n",
      "                             '            Individual weights for each sample\\n'\n",
      "                             '\\n'\n",
      "                             '    Examples\\n'\n",
      "                             '    --------\\n'\n",
      "                             '    >>> from sklearn.svm import SVR\\n'\n",
      "                             '    >>> import numpy as np\\n'\n",
      "                             '    >>> n_samples, n_features = 10, 5\\n'\n",
      "                             '    >>> np.random.seed(0)\\n'\n",
      "                             '    >>> y = np.random.randn(n_samples)\\n'\n",
      "                             '    >>> X = np.random.randn(n_samples, '\n",
      "                             'n_features)\\n'\n",
      "                             '    >>> clf = SVR(C=1.0, epsilon=0.2)\\n'\n",
      "                             '    >>> clf.fit(X, y) #doctest: '\n",
      "                             '+NORMALIZE_WHITESPACE\\n'\n",
      "                             '    SVR(C=1.0, cache_size=200, coef0=0.0, '\n",
      "                             \"degree=3, epsilon=0.2, gamma='auto',\\n\"\n",
      "                             \"        kernel='rbf', max_iter=-1, \"\n",
      "                             'shrinking=True, tol=0.001, verbose=False)\\n'\n",
      "                             '\\n'\n",
      "                             '    See also\\n'\n",
      "                             '    --------\\n'\n",
      "                             '    NuSVR\\n'\n",
      "                             '        Support Vector Machine for regression '\n",
      "                             'implemented using libsvm\\n'\n",
      "                             '        using a parameter to control the number '\n",
      "                             'of support vectors.\\n'\n",
      "                             '\\n'\n",
      "                             '    LinearSVR\\n'\n",
      "                             '        Scalable Linear Support Vector Machine '\n",
      "                             'for regression\\n'\n",
      "                             '        implemented using liblinear.\\n'\n",
      "                             '    '},\n",
      "            'hyper_parameters': {'execution_parameters': {},\n",
      "                                 'model_parameters': {'epsilon': {'attributes': {'path': '.steps[1][1].epsilon'},\n",
      "                                                                  'value': 0.1},\n",
      "                                                      'error_penalty': {'attributes': {'path': '.steps[1][1].C'},\n",
      "                                                                        'value': 1.0},\n",
      "                                                      'independent_kernel_term': {'attributes': {'path': '.steps[1][1].coef0'},\n",
      "                                                                                  'value': 0.0},\n",
      "                                                      'kernel': {'attributes': {'path': '.steps[1][1].kernel'},\n",
      "                                                                 'value': 'rbf'},\n",
      "                                                      'kernel_coefficient': {'attributes': {'path': '.steps[1][1].gamma'},\n",
      "                                                                             'value': 'auto'},\n",
      "                                                      'poly_degree': {'attributes': {'path': '.steps[1][1].degree'},\n",
      "                                                                      'value': 3},\n",
      "                                                      'shrinking': {'attributes': {'path': '.steps[1][1].shrinking'},\n",
      "                                                                    'value': True},\n",
      "                                                      'tolerance': {'attributes': {'path': '.steps[1][1].tol'},\n",
      "                                                                    'value': 0.001},\n",
      "                                                      'verbose': {'attributes': {'path': '.steps[1][1].verbose'},\n",
      "                                                                  'value': False}},\n",
      "                                 'optimisation_parameters': {'cache_size': {'attributes': {'path': '.steps[1][1].cache_size'},\n",
      "                                                                            'value': 200},\n",
      "                                                             'max_iterations': {'attributes': {'path': '.steps[1][1].max_iter'},\n",
      "                                                                                'value': -1}}}}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     run  \\\n",
      "0   1cb3a134-cb41-4d79-837e-b4b04b52edcd   \n",
      "1   a22d0e90-1e8b-4950-8e0e-3ff832fcc69c   \n",
      "2   7b0f8cb2-f97f-4a7b-a3c3-4f162abe5ecb   \n",
      "3   80b4ca92-0735-4c30-aa34-327780edf989   \n",
      "4   87c722a2-a2f1-429e-9547-0026221dd828   \n",
      "5   4df134e2-b1d1-413d-bbf5-9d25057c00a0   \n",
      "6   57c34920-6d9e-425d-bee7-40fdcd41d827   \n",
      "7   42bbfc90-5e5c-413e-bf13-2cc10035e772   \n",
      "8   39d1226e-e3f8-4ce5-af66-93aa3499c7d6   \n",
      "9   bc3c76ce-2a3a-4f50-b3da-eaf10d44fca2   \n",
      "10  c14ca213-577c-427a-9e98-86c247dc3825   \n",
      "11  afc99a0f-0440-4726-8e51-e09688cd40eb   \n",
      "12  4d2dab17-22df-4679-9cf5-21257b600437   \n",
      "13  8a11b536-6fa0-40da-8427-782fbab1b618   \n",
      "14  d09f82fc-42e4-4866-a26e-c400d3143beb   \n",
      "15  09def742-6f19-4124-b143-87968128b652   \n",
      "16  0cf12480-d6ba-4645-985f-c9ab171729ff   \n",
      "17  13fa5e2c-f1bc-4bfd-86ca-d08bd7ebd0bf   \n",
      "18  5e828639-03f2-474f-87d2-0b303691a04d   \n",
      "19  eb211c46-b02d-403b-baab-0b98e8e5d252   \n",
      "20  8059956c-5ba5-4276-b047-5805f84f6424   \n",
      "21  d70c5edd-6fdc-4ee5-b9be-ebee30c97499   \n",
      "22  8a83eefd-eda9-49f0-9fb1-ee702ef673dd   \n",
      "23  5c3ec8d2-c3c8-4aa8-a03d-d923efedb5a9   \n",
      "24  acf6c447-dc45-43c0-853c-a9c60975dd48   \n",
      "25  3e7a1984-a80b-4b45-962c-eb70fff21805   \n",
      "26  44d99c69-b206-47cb-add3-eb539b362c6f   \n",
      "27  e372404c-ad0b-4f86-b6b7-1883825bed52   \n",
      "28  80f514b1-e63f-497d-8525-53349505b390   \n",
      "29  d95c53d7-a37b-4062-aa00-d2078ab98e7a   \n",
      "30  9df952f4-a49b-481d-ba8b-5d6b16cff1ae   \n",
      "31  cfe8d6ce-4367-4198-a606-e6522d086e8a   \n",
      "32  5c0b7fc0-e273-4063-b768-3c4b36ee112a   \n",
      "33  43180e35-a30e-4db2-b31e-12d378c4d6c2   \n",
      "34  58cf2d70-347b-4ee0-8394-5b8b1543e08f   \n",
      "35  5ab19dff-4761-43f2-bb0a-d48103491792   \n",
      "36  678e5f86-4f4f-4340-af30-76aaeda40763   \n",
      "37  0168aab9-a2f2-4f30-b9fd-75ced39d5067   \n",
      "38  9436c3dc-ea10-4a52-a683-d11195c408bc   \n",
      "39  e000f4cb-9ec7-47bb-97ae-9243689b8df5   \n",
      "40  b8f73e8a-3310-4561-82de-940b5da23f47   \n",
      "41  24c1440e-a88b-405d-98c7-185d5349f952   \n",
      "42  727f6c45-5288-42e8-9e6a-304aa5c64c81   \n",
      "43  8b0b41c3-4625-44d1-b1d3-5986c3fe722b   \n",
      "44  02d4c754-0a5a-40b6-9aa5-63c67569022e   \n",
      "45  2255beb2-4894-44e5-b3bf-f74fdb0c4959   \n",
      "\n",
      "                                   split                      dataset  \\\n",
      "0   3f24e9c8-917e-470b-8061-3f4fa41db752                     diabetes   \n",
      "1   c25ee49f-3b62-4641-9369-ea002218e0db  Boston House Prices dataset   \n",
      "2   79bdd34c-015e-4502-a937-dfdb254bb170  Boston House Prices dataset   \n",
      "3   c6a5eba4-32db-4596-ac08-ce9687de83ab  Boston House Prices dataset   \n",
      "4   5d7d295e-36ad-4f6e-ace8-7d1120437a96  Boston House Prices dataset   \n",
      "5   5e70efff-a73b-4740-93b9-1d3a94ee905c  Boston House Prices dataset   \n",
      "6   c93758dc-4ab9-481a-b0f7-a01005ab59df  Boston House Prices dataset   \n",
      "7   e58587a6-f425-49e6-84ba-3bbeb92f8d99  Boston House Prices dataset   \n",
      "8   fa9adfc6-b05b-4bfe-8f87-35ee5ff7dd84  Boston House Prices dataset   \n",
      "9   92b0a80a-e5de-463e-b800-0dfbf5f7127a  Boston House Prices dataset   \n",
      "10  e61be4c4-af5f-436c-be58-2636168ced84  Boston House Prices dataset   \n",
      "11  2c8cbd3c-3ff6-4dfa-b0db-21232bad439e  Boston House Prices dataset   \n",
      "12  96f1b3c7-c0a5-4951-af88-21120004b807  Boston House Prices dataset   \n",
      "13  6a464916-a459-472c-9594-c1f57de5c68a  Boston House Prices dataset   \n",
      "14  73f4d558-6870-4425-a002-a3a6b8bb7022  Boston House Prices dataset   \n",
      "15  66e51e31-fb86-4390-a6ce-4a9533c5bd9b  Boston House Prices dataset   \n",
      "16  15fe6556-6c9d-43dd-91bc-05191638ae70  Boston House Prices dataset   \n",
      "17  e56eb8bb-f86a-462c-88ea-ab43ddf21015  Boston House Prices dataset   \n",
      "18  b7e1269b-405a-4ad4-bfbd-8d8db84a2db3  Boston House Prices dataset   \n",
      "19  3d759ed2-ce79-4e90-9b53-d209768ee3da  Boston House Prices dataset   \n",
      "20  5553f2d5-aeb1-45b3-906e-bf12e89082ac  Boston House Prices dataset   \n",
      "21  743f93af-dd12-4032-bacb-5f5298f14f57  Boston House Prices dataset   \n",
      "22  baa3871c-1e38-46d4-854a-39f8e9d2d310  Boston House Prices dataset   \n",
      "23  be2f4393-865d-4df2-a5b6-b62142e74dc3  Boston House Prices dataset   \n",
      "24  a7a1640e-7603-4cbe-abf1-a32ac3f223cc  Boston House Prices dataset   \n",
      "25  0e07f546-7762-45df-aaf8-b32199c160ab  Boston House Prices dataset   \n",
      "26  8d3053a1-385d-4706-9e6c-dc26d274fd86  Boston House Prices dataset   \n",
      "27  dfbec3c5-34bd-4379-96b0-3b925b62d41a  Boston House Prices dataset   \n",
      "28  a6a084a5-1c8b-4fa7-b254-7028a0e0cd81  Boston House Prices dataset   \n",
      "29  83e61845-19b2-46d9-9c09-1d170ef69ab9  Boston House Prices dataset   \n",
      "30  c9bb325a-3e8d-4728-821f-0e9b320f964a  Boston House Prices dataset   \n",
      "31  bb72fa2e-91cd-41be-b4b3-4bd9f1f30293  Boston House Prices dataset   \n",
      "32  b3b94d1e-373f-452d-9776-c0820d821396  Boston House Prices dataset   \n",
      "33  b3706e82-0ae7-46b0-98a1-c556898eda30  Boston House Prices dataset   \n",
      "34  491583f6-918a-41bf-ada6-25c5764ed8b5  Boston House Prices dataset   \n",
      "35  3d16366f-127b-4ff3-851b-e892528d1b84  Boston House Prices dataset   \n",
      "36  e1d23cdf-277c-4f58-b802-4f2289f4158b  Boston House Prices dataset   \n",
      "37  6da7bcb0-1705-4ab9-9870-60cd68253d1b  Boston House Prices dataset   \n",
      "38  ba06b963-7623-48cd-b0a3-c299d631efa1  Boston House Prices dataset   \n",
      "39  ea9d4f70-00a7-4533-8ae8-c6d2ca6ec699  Boston House Prices dataset   \n",
      "40  a8d761c8-d0a0-4a8d-8cda-07d591e8f3d3  Boston House Prices dataset   \n",
      "41  e6715a49-7f18-4789-be09-0ceebaaf7f31  Boston House Prices dataset   \n",
      "42  a21ef334-7817-46f8-a28c-4018d3b5b47a  Boston House Prices dataset   \n",
      "43  372f2a69-3b42-4876-b647-b12130cb86d8  Boston House Prices dataset   \n",
      "44  b38df3af-c410-4ed4-a10a-610a1b4237f9  Boston House Prices dataset   \n",
      "45  fd82b463-06f3-4482-9edf-ee92741dc04f  Boston House Prices dataset   \n",
      "\n",
      "   epsilon-support vector regression.error_penalty  \\\n",
      "0                                                -   \n",
      "1                                              0.5   \n",
      "2                                              0.5   \n",
      "3                                              0.5   \n",
      "4                                              0.5   \n",
      "5                                              0.5   \n",
      "6                                              0.5   \n",
      "7                                              0.5   \n",
      "8                                              0.5   \n",
      "9                                              0.5   \n",
      "10                                             0.5   \n",
      "11                                             0.5   \n",
      "12                                             0.5   \n",
      "13                                             0.5   \n",
      "14                                             0.5   \n",
      "15                                             0.5   \n",
      "16                                             1.0   \n",
      "17                                             1.0   \n",
      "18                                             1.0   \n",
      "19                                             1.0   \n",
      "20                                             1.0   \n",
      "21                                             1.0   \n",
      "22                                             1.0   \n",
      "23                                             1.0   \n",
      "24                                             1.0   \n",
      "25                                             1.0   \n",
      "26                                             1.0   \n",
      "27                                             1.0   \n",
      "28                                             1.0   \n",
      "29                                             1.0   \n",
      "30                                             1.0   \n",
      "31                                             1.5   \n",
      "32                                             1.5   \n",
      "33                                             1.5   \n",
      "34                                             1.5   \n",
      "35                                             1.5   \n",
      "36                                             1.5   \n",
      "37                                             1.5   \n",
      "38                                             1.5   \n",
      "39                                             1.5   \n",
      "40                                             1.5   \n",
      "41                                             1.5   \n",
      "42                                             1.5   \n",
      "43                                             1.5   \n",
      "44                                             1.5   \n",
      "45                                             1.5   \n",
      "\n",
      "   epsilon-support vector regression.poly_degree  \\\n",
      "0                                              -   \n",
      "1                                              1   \n",
      "2                                              1   \n",
      "3                                              1   \n",
      "4                                              1   \n",
      "5                                              1   \n",
      "6                                              2   \n",
      "7                                              2   \n",
      "8                                              2   \n",
      "9                                              2   \n",
      "10                                             2   \n",
      "11                                             3   \n",
      "12                                             3   \n",
      "13                                             3   \n",
      "14                                             3   \n",
      "15                                             3   \n",
      "16                                             1   \n",
      "17                                             1   \n",
      "18                                             1   \n",
      "19                                             1   \n",
      "20                                             1   \n",
      "21                                             2   \n",
      "22                                             2   \n",
      "23                                             2   \n",
      "24                                             2   \n",
      "25                                             2   \n",
      "26                                             3   \n",
      "27                                             3   \n",
      "28                                             3   \n",
      "29                                             3   \n",
      "30                                             3   \n",
      "31                                             1   \n",
      "32                                             1   \n",
      "33                                             1   \n",
      "34                                             1   \n",
      "35                                             1   \n",
      "36                                             2   \n",
      "37                                             2   \n",
      "38                                             2   \n",
      "39                                             2   \n",
      "40                                             2   \n",
      "41                                             3   \n",
      "42                                             3   \n",
      "43                                             3   \n",
      "44                                             3   \n",
      "45                                             3   \n",
      "\n",
      "   principal component analysis.num_components            accuracy  \\\n",
      "0                                            -  0.6927083333333334   \n",
      "1                                            2                   -   \n",
      "2                                            3                   -   \n",
      "3                                            4                   -   \n",
      "4                                            5                   -   \n",
      "5                                            6                   -   \n",
      "6                                            2                   -   \n",
      "7                                            3                   -   \n",
      "8                                            4                   -   \n",
      "9                                            5                   -   \n",
      "10                                           6                   -   \n",
      "11                                           2                   -   \n",
      "12                                           3                   -   \n",
      "13                                           4                   -   \n",
      "14                                           5                   -   \n",
      "15                                           6                   -   \n",
      "16                                           2                   -   \n",
      "17                                           3                   -   \n",
      "18                                           4                   -   \n",
      "19                                           5                   -   \n",
      "20                                           6                   -   \n",
      "21                                           2                   -   \n",
      "22                                           3                   -   \n",
      "23                                           4                   -   \n",
      "24                                           5                   -   \n",
      "25                                           6                   -   \n",
      "26                                           2                   -   \n",
      "27                                           3                   -   \n",
      "28                                           4                   -   \n",
      "29                                           5                   -   \n",
      "30                                           6                   -   \n",
      "31                                           2                   -   \n",
      "32                                           3                   -   \n",
      "33                                           4                   -   \n",
      "34                                           5                   -   \n",
      "35                                           6                   -   \n",
      "36                                           2                   -   \n",
      "37                                           3                   -   \n",
      "38                                           4                   -   \n",
      "39                                           5                   -   \n",
      "40                                           6                   -   \n",
      "41                                           2                   -   \n",
      "42                                           3                   -   \n",
      "43                                           4                   -   \n",
      "44                                           5                   -   \n",
      "45                                           6                   -   \n",
      "\n",
      "               f1_score recall           precision  \n",
      "0   0.40923076923076923    0.5  0.3463541666666667  \n",
      "1                     -      -                   -  \n",
      "2                     -      -                   -  \n",
      "3                     -      -                   -  \n",
      "4                     -      -                   -  \n",
      "5                     -      -                   -  \n",
      "6                     -      -                   -  \n",
      "7                     -      -                   -  \n",
      "8                     -      -                   -  \n",
      "9                     -      -                   -  \n",
      "10                    -      -                   -  \n",
      "11                    -      -                   -  \n",
      "12                    -      -                   -  \n",
      "13                    -      -                   -  \n",
      "14                    -      -                   -  \n",
      "15                    -      -                   -  \n",
      "16                    -      -                   -  \n",
      "17                    -      -                   -  \n",
      "18                    -      -                   -  \n",
      "19                    -      -                   -  \n",
      "20                    -      -                   -  \n",
      "21                    -      -                   -  \n",
      "22                    -      -                   -  \n",
      "23                    -      -                   -  \n",
      "24                    -      -                   -  \n",
      "25                    -      -                   -  \n",
      "26                    -      -                   -  \n",
      "27                    -      -                   -  \n",
      "28                    -      -                   -  \n",
      "29                    -      -                   -  \n",
      "30                    -      -                   -  \n",
      "31                    -      -                   -  \n",
      "32                    -      -                   -  \n",
      "33                    -      -                   -  \n",
      "34                    -      -                   -  \n",
      "35                    -      -                   -  \n",
      "36                    -      -                   -  \n",
      "37                    -      -                   -  \n",
      "38                    -      -                   -  \n",
      "39                    -      -                   -  \n",
      "40                    -      -                   -  \n",
      "41                    -      -                   -  \n",
      "42                    -      -                   -  \n",
      "43                    -      -                   -  \n",
      "44                    -      -                   -  \n",
      "45                    -      -                   -  \n"
     ]
    }
   ],
   "source": [
    "# Compare the metrics between experiments and also between runs(Displaying pandas dataframe)\n",
    "from padre.experimentexecutor import ExperimentExecutor\n",
    "# Create an experiment with multiple runs\n",
    "params_pca = {'num_components': [2, 3, 4, 5, 6]}\n",
    "params_svr = {'C': [0.5, 1.0, 1.5],\n",
    "              'poly_degree': [1, 2, 3]}\n",
    "params_dict = {'SVR': params_svr, 'pca': params_pca}\n",
    "workflow = pypadre.experiment_creator.create_test_pipeline(['pca', 'SVR'])\n",
    "params_dict = pypadre.experiment_creator.convert_alternate_estimator_names(params_dict)\n",
    "pypadre.experiment_creator.clear_experiments('IPython Experiment 3')\n",
    "pypadre.experiment_creator.create(name='IPython Experiment 3',\n",
    "                                  description='Experiment to test the comparison of metrics',\n",
    "                                  dataset_list=['Boston_House_Prices'],\n",
    "                                  workflow=workflow,\n",
    "                                  params=params_dict\n",
    "                                 )\n",
    "experiments_executor = ExperimentExecutor(experiments=pypadre.experiment_creator.createExperimentList())\n",
    "experiments_executor.execute(local_run=True, threads=1)\n",
    "#from padre.metrics import CompareMetrics\n",
    "#metrics = CompareMetrics(experiments_list=experiments_executor.experiments)\n",
    "#print(metrics.show_metrics())\n",
    "pypadre.metrics_evaluator.add_experiments(experiments_executor.experiments)\n",
    "print(pypadre.metrics_evaluator.show_metrics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     run  \\\n",
      "0   bc3c76ce-2a3a-4f50-b3da-eaf10d44fca2   \n",
      "1   9436c3dc-ea10-4a52-a683-d11195c408bc   \n",
      "2   eb211c46-b02d-403b-baab-0b98e8e5d252   \n",
      "3   acf6c447-dc45-43c0-853c-a9c60975dd48   \n",
      "4   43180e35-a30e-4db2-b31e-12d378c4d6c2   \n",
      "5   02d4c754-0a5a-40b6-9aa5-63c67569022e   \n",
      "6   8b0b41c3-4625-44d1-b1d3-5986c3fe722b   \n",
      "7   8a11b536-6fa0-40da-8427-782fbab1b618   \n",
      "8   5c3ec8d2-c3c8-4aa8-a03d-d923efedb5a9   \n",
      "9   39d1226e-e3f8-4ce5-af66-93aa3499c7d6   \n",
      "10  d09f82fc-42e4-4866-a26e-c400d3143beb   \n",
      "11  58cf2d70-347b-4ee0-8394-5b8b1543e08f   \n",
      "12  5e828639-03f2-474f-87d2-0b303691a04d   \n",
      "13  d95c53d7-a37b-4062-aa00-d2078ab98e7a   \n",
      "14  87c722a2-a2f1-429e-9547-0026221dd828   \n",
      "15  80b4ca92-0735-4c30-aa34-327780edf989   \n",
      "16  e000f4cb-9ec7-47bb-97ae-9243689b8df5   \n",
      "17  80f514b1-e63f-497d-8525-53349505b390   \n",
      "\n",
      "                                   split                      dataset  \\\n",
      "0   92b0a80a-e5de-463e-b800-0dfbf5f7127a  Boston House Prices dataset   \n",
      "1   ba06b963-7623-48cd-b0a3-c299d631efa1  Boston House Prices dataset   \n",
      "2   3d759ed2-ce79-4e90-9b53-d209768ee3da  Boston House Prices dataset   \n",
      "3   a7a1640e-7603-4cbe-abf1-a32ac3f223cc  Boston House Prices dataset   \n",
      "4   b3706e82-0ae7-46b0-98a1-c556898eda30  Boston House Prices dataset   \n",
      "5   b38df3af-c410-4ed4-a10a-610a1b4237f9  Boston House Prices dataset   \n",
      "6   372f2a69-3b42-4876-b647-b12130cb86d8  Boston House Prices dataset   \n",
      "7   6a464916-a459-472c-9594-c1f57de5c68a  Boston House Prices dataset   \n",
      "8   be2f4393-865d-4df2-a5b6-b62142e74dc3  Boston House Prices dataset   \n",
      "9   fa9adfc6-b05b-4bfe-8f87-35ee5ff7dd84  Boston House Prices dataset   \n",
      "10  73f4d558-6870-4425-a002-a3a6b8bb7022  Boston House Prices dataset   \n",
      "11  491583f6-918a-41bf-ada6-25c5764ed8b5  Boston House Prices dataset   \n",
      "12  b7e1269b-405a-4ad4-bfbd-8d8db84a2db3  Boston House Prices dataset   \n",
      "13  83e61845-19b2-46d9-9c09-1d170ef69ab9  Boston House Prices dataset   \n",
      "14  5d7d295e-36ad-4f6e-ace8-7d1120437a96  Boston House Prices dataset   \n",
      "15  c6a5eba4-32db-4596-ac08-ce9687de83ab  Boston House Prices dataset   \n",
      "16  ea9d4f70-00a7-4533-8ae8-c6d2ca6ec699  Boston House Prices dataset   \n",
      "17  a6a084a5-1c8b-4fa7-b254-7028a0e0cd81  Boston House Prices dataset   \n",
      "\n",
      "   epsilon-support vector regression.error_penalty  \\\n",
      "0                                              0.5   \n",
      "1                                              1.5   \n",
      "2                                              1.0   \n",
      "3                                              1.0   \n",
      "4                                              1.5   \n",
      "5                                              1.5   \n",
      "6                                              1.5   \n",
      "7                                              0.5   \n",
      "8                                              1.0   \n",
      "9                                              0.5   \n",
      "10                                             0.5   \n",
      "11                                             1.5   \n",
      "12                                             1.0   \n",
      "13                                             1.0   \n",
      "14                                             0.5   \n",
      "15                                             0.5   \n",
      "16                                             1.5   \n",
      "17                                             1.0   \n",
      "\n",
      "   epsilon-support vector regression.poly_degree  \\\n",
      "0                                              2   \n",
      "1                                              2   \n",
      "2                                              1   \n",
      "3                                              2   \n",
      "4                                              1   \n",
      "5                                              3   \n",
      "6                                              3   \n",
      "7                                              3   \n",
      "8                                              2   \n",
      "9                                              2   \n",
      "10                                             3   \n",
      "11                                             1   \n",
      "12                                             1   \n",
      "13                                             3   \n",
      "14                                             1   \n",
      "15                                             1   \n",
      "16                                             2   \n",
      "17                                             3   \n",
      "\n",
      "   principal component analysis.num_components accuracy f1_score recall  \\\n",
      "0                                            5        -        -      -   \n",
      "1                                            4        -        -      -   \n",
      "2                                            5        -        -      -   \n",
      "3                                            5        -        -      -   \n",
      "4                                            4        -        -      -   \n",
      "5                                            5        -        -      -   \n",
      "6                                            4        -        -      -   \n",
      "7                                            4        -        -      -   \n",
      "8                                            4        -        -      -   \n",
      "9                                            4        -        -      -   \n",
      "10                                           5        -        -      -   \n",
      "11                                           5        -        -      -   \n",
      "12                                           4        -        -      -   \n",
      "13                                           5        -        -      -   \n",
      "14                                           5        -        -      -   \n",
      "15                                           4        -        -      -   \n",
      "16                                           5        -        -      -   \n",
      "17                                           4        -        -      -   \n",
      "\n",
      "   precision  \n",
      "0          -  \n",
      "1          -  \n",
      "2          -  \n",
      "3          -  \n",
      "4          -  \n",
      "5          -  \n",
      "6          -  \n",
      "7          -  \n",
      "8          -  \n",
      "9          -  \n",
      "10         -  \n",
      "11         -  \n",
      "12         -  \n",
      "13         -  \n",
      "14         -  \n",
      "15         -  \n",
      "16         -  \n",
      "17         -  \n"
     ]
    }
   ],
   "source": [
    "# Analyze metrics where the runs have specific values for a component\n",
    "pypadre.metrics_evaluator.analyze_runs(\n",
    "    ['principal component analysis.num_components.4', 'principal component analysis.num_components.5'])\n",
    "df = pypadre.metrics_evaluator.display_results()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    run                                 split  \\\n",
      "0  9436c3dc-ea10-4a52-a683-d11195c408bc  ba06b963-7623-48cd-b0a3-c299d631efa1   \n",
      "1  43180e35-a30e-4db2-b31e-12d378c4d6c2  b3706e82-0ae7-46b0-98a1-c556898eda30   \n",
      "2  8b0b41c3-4625-44d1-b1d3-5986c3fe722b  372f2a69-3b42-4876-b647-b12130cb86d8   \n",
      "3  8a11b536-6fa0-40da-8427-782fbab1b618  6a464916-a459-472c-9594-c1f57de5c68a   \n",
      "4  5c3ec8d2-c3c8-4aa8-a03d-d923efedb5a9  be2f4393-865d-4df2-a5b6-b62142e74dc3   \n",
      "5  39d1226e-e3f8-4ce5-af66-93aa3499c7d6  fa9adfc6-b05b-4bfe-8f87-35ee5ff7dd84   \n",
      "6  5e828639-03f2-474f-87d2-0b303691a04d  b7e1269b-405a-4ad4-bfbd-8d8db84a2db3   \n",
      "7  80b4ca92-0735-4c30-aa34-327780edf989  c6a5eba4-32db-4596-ac08-ce9687de83ab   \n",
      "8  80f514b1-e63f-497d-8525-53349505b390  a6a084a5-1c8b-4fa7-b254-7028a0e0cd81   \n",
      "\n",
      "                       dataset  \\\n",
      "0  Boston House Prices dataset   \n",
      "1  Boston House Prices dataset   \n",
      "2  Boston House Prices dataset   \n",
      "3  Boston House Prices dataset   \n",
      "4  Boston House Prices dataset   \n",
      "5  Boston House Prices dataset   \n",
      "6  Boston House Prices dataset   \n",
      "7  Boston House Prices dataset   \n",
      "8  Boston House Prices dataset   \n",
      "\n",
      "  epsilon-support vector regression.error_penalty  \\\n",
      "0                                             1.5   \n",
      "1                                             1.5   \n",
      "2                                             1.5   \n",
      "3                                             0.5   \n",
      "4                                             1.0   \n",
      "5                                             0.5   \n",
      "6                                             1.0   \n",
      "7                                             0.5   \n",
      "8                                             1.0   \n",
      "\n",
      "  epsilon-support vector regression.poly_degree  \\\n",
      "0                                             2   \n",
      "1                                             1   \n",
      "2                                             3   \n",
      "3                                             3   \n",
      "4                                             2   \n",
      "5                                             2   \n",
      "6                                             1   \n",
      "7                                             1   \n",
      "8                                             3   \n",
      "\n",
      "  principal component analysis.num_components  mean_error  \n",
      "0                                           4    2.778587  \n",
      "1                                           4    0.722125  \n",
      "2                                           4    1.817757  \n",
      "3                                           4    1.406032  \n",
      "4                                           4    1.011386  \n",
      "5                                           4    0.874651  \n",
      "6                                           4    1.037645  \n",
      "7                                           4    2.228226  \n",
      "8                                           4    1.452759  \n"
     ]
    }
   ],
   "source": [
    "# Display only the metric specified by the user\n",
    "pypadre.metrics_evaluator.analyze_runs(['principal component analysis.num_components.4'], ['mean_error'])\n",
    "df = pypadre.metrics_evaluator.display_results()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload/Sync experiments on demand\n",
    "# Not Implemented. Experiments can only be uploaded during the execution of the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the experiment via SPA and check the results and visualize them(if possible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download an experiment from the server and run it.(via the experiment.JSON file) Example for reproducibility\n",
    "# Downloading experiment from server is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of random seeds used in the experiment\n",
    "# Not yet implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running an experiment on the server cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracking the progress of experiments including hyperparameter search\n",
    "# Code to be included in the backend for logging the porogress"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
