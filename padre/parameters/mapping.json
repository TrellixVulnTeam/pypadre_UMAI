{
	"algorithms": [
		{
			"name": "linear regression",
			"other_names": [],
			"type": "Regression",
			"hyper_parameters": {
				"model_parameters": [
					{
						"name": "intercept",
						"kind_of_value": "boolean",
						"optional": "True",
						"description": "Whether to calculate the intercept for this model.",
						"default_value_in_scikit-learn": "True"
					},
					{
						"name": "normalize",
						"kind_of_value": "boolean",
						"optional": "True",
						"description": "Normalizes the regressors before regression by subtracting the mean and dividing by the l2-norm.",
						"default_value_in_scikit-learn": "False"
					}
				],
				"optimisation_parameters": [
					{
						"name": "jobs",
						"kind_of_value": "integer",
						"optional": "True",
						"description": "The number of jobs to use for the computation. If -1 all CPUs are used.",
						"default_value_in_scikit-learn": "1"
					}
				],
				"execution_parameters": [
				]
			}
		},
		{
			"name": "ridge regression",
			"other_names": ["Tikhonov regularization", "weight decay", "Tikhonov–Miller method", "Phillips–Twomey method", "constrained linear inversion", "linear regularization"],
			"type": "Regression",
			"hyper_parameters": {
				"model_parameters": [
					{
						"name": "alpha",
						"kind_of_value": "float, array-like in the shape of the targets",
						"optional": "False",
						"description": "Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. Alpha corresponds to C^-1 in other linear models such as LogisticRegression or LinearSVC. If an array is passed, penalties are assumed to be specific to the targets. Hence they must correspond in number.",
						"default_value_in_scikit-learn": "1.0"
					},
					{
						"name": "intercept",
						"kind_of_value": "boolean",
						"optional": "True",
						"description": "Whether to calculate the intercept for this model.",
						"default_value_in_scikit-learn": "True"
					},
					{
						"name": "normalize",
						"kind_of_value": "boolean",
						"optional": "True",
						"description": "Normalizes the regressors before regression by subtracting the mean and dividing by the l2-norm.",
						"default_value_in_scikit-learn": "False"
					},
					{
						"name": "tol",
						"kind_of_value": "float",
						"optional": "True",
						"description": "Precision of the solution.",
						"default_value_in_scikit-learn": "0.001"
					},
					{
						"name": "solver",
						"kind_of_value": "{‘auto’, ‘svd’, ‘cholesky’, ‘lsqr’, ‘sparse_cg’, ‘sag’, ‘saga’}",
						"optional": "True",
						"description": "Solver to use in the computational routines: -‘auto’ chooses the solver automatically based on the type of data. -‘svd’ uses a Singular Value Decomposition of X to compute the Ridge coefficients. More stable for singular matrices than ‘cholesky’. -‘cholesky’ uses the standard scipy.linalg.solve function to obtain a closed-form solution. -‘sparse_cg’ uses the conjugate gradient solver as found in scipy.sparse.linalg.cg. As an iterative algorithm, this solver is more appropriate than ‘cholesky’ for large-scale data (possibility to set tol and max_iter). -‘lsqr’ uses the dedicated regularized least-squares routine scipy.sparse.linalg.lsqr. It is the fastest but may not be available in old scipy versions. It also uses an iterative procedure. -‘sag’ uses a Stochastic Average Gradient descent, and ‘saga’ uses its improved, unbiased version named SAGA. Both methods also use an iterative procedure, and are often faster than other solvers when both n_samples and n_features are large. Note that ‘sag’ and ‘saga’ fast convergence is only guaranteed on features with approximately the same scale. You can preprocess the data with a scaler from sklearn.preprocessing.",
						"default_value_in_scikit-learn": "auto"
					}
				],
				"optimisation_parameters": [
					{
						"name": "max_iterations",
						"kind_of_value": "integer",
						"optional": "True",
						"description": "Maximum number of iterations for conjugate gradient solver. For ‘sparse_cg’ and ‘lsqr’ solvers, the default value is determined by scipy.sparse.linalg. For ‘sag’ solver, the default value is 1000",
						"default_value_in_scikit-learn": "None"
					}
				],
				"execution_parameters": [
				]
			}
		},
		{
			"name": "lasso",
			"other_names": [],
			"type": "Regression",
			"hyper_parameters": {
				"model_parameters": [
					{
						
					}
				],
				"optimisation_parameters": [
					{
						
					}
				],
				"execution_parameters": [
				]
			}
		},
		{
			"name": "",
			"other_names": [],
			"type": "",
			"hyper_parameters": {
				"model_parameters": [
					{
						
					}
				],
				"optimisation_parameters": [
					{
						
					}
				],
				"execution_parameters": [
				]
			}
		}
	]
}