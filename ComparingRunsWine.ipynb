{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from sklearn.datasets import load_iris, load_diabetes\n",
    "from pypadre.binding.metrics import sklearn_metrics\n",
    "from pypadre.examples.base_example import example_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create example app\n",
    "app = example_app()\n",
    "\n",
    "predictions = []\n",
    "idx= 0\n",
    "matrix = []\n",
    "missclassification_matrix = []\n",
    "metrics = []\n",
    "names = []\n",
    "grid_parameters = []\n",
    "cols = ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)',\n",
    "                      'petal width (cm)', 'class']\n",
    "columns = [\"Age\",\n",
    "    \"Sex\",\n",
    "    \"Body mass index\",\n",
    "    \"Average blood pressure\",\n",
    "    \"S1\",\n",
    "    \"S2\",\n",
    "    \"S3\",\n",
    "    \"S4\",\n",
    "    \"S5\",\n",
    "    \"S6\",\n",
    "    \"progression\"]\n",
    "    \n",
    "columns_banknotes = [\n",
    "    \"variance\",\n",
    "    \"skewness\",\n",
    "    \"kurtosis\",\n",
    "    \"entropy\",\n",
    "    \"class\"\n",
    "]\n",
    "\n",
    "columns_abalone=[\n",
    "    \n",
    "    \"Sex\",\n",
    "    \"Length\",\n",
    "    \"Diameter\",\n",
    "    \"Height\",\n",
    "    \"Whole weight\",\n",
    "    \"Shucked weight\",\n",
    "    \"Viscera weight\",\n",
    "    \"Shell weight\",\n",
    "    \"Rings\"\n",
    "]\n",
    "\n",
    "columns_wine=[\n",
    "    \"Fixed acidity.\",\n",
    "\"Volatile acidity.\",\n",
    "\"Citric acid.\",\n",
    "\"Residual sugar.\",\n",
    "\"Chlorides.\",\n",
    "\"Free sulfur dioxide.\",\n",
    "\"Total sulfur dioxide.\",\n",
    "\"Density.\",\n",
    "\"pH.\",\n",
    "\"Sulphates.\",\n",
    "\"Alcohol.\",\n",
    "\"Quality\"\n",
    "\n",
    "]\n",
    "\n",
    "# Define the dataset\n",
    "@app.dataset(name=\"wine\", columns=columns_wine, target_features='Quality')\n",
    "def dataset():\n",
    "    #data = load_iris().data\n",
    "    #target = load_iris().target.reshape(-1, 1)\n",
    "    #return np.append(data, target, axis=1)\n",
    "    return np.loadtxt('winequality-white.csv', delimiter=\";\", usecols=range(12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the reference\n",
    "from pypadre.core.model.code.code_mixin import PythonPackage, PythonFile, GitIdentifier, Function\n",
    "from pathlib import Path\n",
    "\n",
    "path = os.path.abspath('')\n",
    "git_repo = str(Path(''))\n",
    "reference = PythonFile(path=str(Path('')), package=path[len(git_repo) + 1:],\n",
    "                             variable=\"function_name\",\n",
    "                             repository_identifier=GitIdentifier(path=git_repo))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Following metrics would be available for Evaluation: Confusion Matrix, Classification Metrics\n",
      "Calculating d3dec3e0-087d-4cbb-8e2b-439f491a3d88 done.\n",
      "Following metrics would be available for Evaluation: Confusion Matrix, Classification Metrics\n",
      "Calculating 96637eb0-8e88-4a30-90fb-9950ddeacfb7 done.\n",
      "Following metrics would be available for Evaluation: Confusion Matrix, Classification Metrics\n",
      "Calculating ccc02c8e-05e2-44af-b7f5-896155194ba9 done.\n",
      "Following metrics would be available for Evaluation: Confusion Matrix, Classification Metrics\n",
      "Calculating fcc3b58d-6a89-462f-b856-d311c0261827 done.\n",
      "Following metrics would be available for Evaluation: Confusion Matrix, Classification Metrics\n",
      "Calculating 0e196eda-292e-43ee-9f9a-40bc0d8dfc1e done.\n",
      "Following metrics would be available for Evaluation: Confusion Matrix, Classification Metrics\n",
      "Calculating 2b0940bf-3a22-4999-8310-ce3c0debf928 done.\n",
      "Following metrics would be available for Evaluation: Confusion Matrix, Classification Metrics\n",
      "Calculating 8623f169-70b9-4710-b397-774cf2eed608 done.\n",
      "Following metrics would be available for Evaluation: Confusion Matrix, Classification Metrics\n",
      "Calculating 678431fd-d847-42b2-8c3d-a97af882499f done.\n",
      "Following metrics would be available for Evaluation: Confusion Matrix, Classification Metrics\n",
      "Calculating d26cec85-d7f0-49b6-9f5a-4ed73b1c9579 done.\n",
      "Following metrics would be available for Evaluation: Confusion Matrix, Classification Metrics\n",
      "Calculating a16f459f-e8b3-4df2-afb0-c378b121ae35 done.\n",
      "Following metrics would be available for Evaluation: Confusion Matrix, Classification Metrics\n",
      "Calculating a3419408-fb8a-44ff-ad04-7def03ff8308 done.\n",
      "Following metrics would be available for Evaluation: Confusion Matrix, Classification Metrics\n",
      "Calculating 3c814784-4fdb-4413-80b5-44db30b87b56 done.\n",
      "Following metrics would be available for Evaluation: Confusion Matrix, Classification Metrics\n",
      "Calculating 6d0c1bee-f6ed-4042-9071-62f6d23eb5dc done.\n",
      "Following metrics would be available for Evaluation: Confusion Matrix, Classification Metrics\n",
      "Calculating 850eb115-a973-46ae-a1c0-c865eb5b380f done.\n",
      "Following metrics would be available for Evaluation: Confusion Matrix, Classification Metrics\n",
      "Calculating dd80dd02-825d-490f-8961-52de7169ff91 done.\n",
      "Following metrics would be available for Evaluation: Confusion Matrix, Classification Metrics\n",
      "Calculating cc415ee2-3efa-4755-96be-1002e5b0955b done.\n",
      "Following metrics would be available for Evaluation: Confusion Matrix, Classification Metrics\n",
      "Calculating bb4f82e5-1b22-452c-98c7-6d71ec6a1d86 done.\n",
      "Following metrics would be available for Evaluation: Confusion Matrix, Classification Metrics\n",
      "Calculating 9e3abafd-9ccc-4b41-b278-7b173bc3ccf3 done.\n",
      "Following metrics would be available for Evaluation: Confusion Matrix, Classification Metrics\n",
      "Calculating afe8095b-48d0-4dc7-b1aa-1c8a6803249a done.\n",
      "Following metrics would be available for Evaluation: Confusion Matrix, Classification Metrics\n",
      "Calculating 8473635c-87d6-48b3-b0db-4113fe27dd07 done.\n",
      "Following metrics would be available for Evaluation: Confusion Matrix, Classification Metrics\n",
      "Calculating b2d0e18c-8110-42f0-86ce-0751d1ec942c done.\n",
      "Following metrics would be available for Evaluation: Confusion Matrix, Classification Metrics\n",
      "Calculating 5e4babcb-6427-4008-a852-da3e372724dd done.\n",
      "Following metrics would be available for Evaluation: Confusion Matrix, Classification Metrics\n",
      "Calculating a4071f14-27d1-49d3-9587-97be2e0ba170 done.\n",
      "Following metrics would be available for Evaluation: Confusion Matrix, Classification Metrics\n",
      "Calculating 0bcf4fd8-acde-40d4-a4fd-9b5bd937a062 done.\n",
      "Following metrics would be available for Evaluation: Confusion Matrix, Classification Metrics\n",
      "Calculating 3245b50f-b662-4fe8-9831-9f4c2fd7b5c9 done.\n",
      "Following metrics would be available for Evaluation: Confusion Matrix, Classification Metrics\n",
      "Calculating 271ee343-11f3-4a4b-beb7-c160936de57a done.\n",
      "Following metrics would be available for Evaluation: Confusion Matrix, Classification Metrics\n",
      "Calculating 3d18e280-33ae-44d9-81e8-d81e98f93bc8 done.\n",
      "Following metrics would be available for Evaluation: Confusion Matrix, Classification Metrics\n",
      "Calculating 530c1c22-b13b-45b7-85c5-5c6500164264 done.\n",
      "Following metrics would be available for Evaluation: Confusion Matrix, Classification Metrics\n",
      "Calculating 02a17aeb-8ccb-4b34-b53b-805f2757705e done.\n",
      "Following metrics would be available for Evaluation: Confusion Matrix, Classification Metrics\n",
      "Calculating ee0218a3-336d-478c-b399-2ae4b3072ed2 done.\n",
      "Following metrics would be available for Evaluation: Confusion Matrix, Classification Metrics\n",
      "Calculating 7058e523-2718-4a89-9446-a5b32aa27554 done.\n",
      "Following metrics would be available for Evaluation: Confusion Matrix, Classification Metrics\n",
      "Calculating 6f813b22-ce5a-454b-9ece-66a934d1590f done.\n"
     ]
    }
   ],
   "source": [
    "# Grid search over parameters defined below\n",
    "@app.parameter_map()\n",
    "def parameters():\n",
    "    return {'SKLearnEstimator': {'parameters': {'SVC': {'kernel': ['rbf'], 'C': [0.1, 0.5, 1.5], 'tol': [.1, .3]}}}}\n",
    "\n",
    "# Create the experiment\n",
    "@app.experiment(dataset=dataset, reference=reference,\n",
    "                experiment_name=\"Iris SVC - Grid Search with Static Seed\", seed=1, project_name=\"Examples\", \n",
    "                parameters=parameters)\n",
    "def experiment():\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.svm import SVC\n",
    "    estimators = [('SVC', SVC(probability=True, C=1.0))]\n",
    "    return Pipeline(estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [f for f in app._pipeline_output_app.list() if f.parent.parent.parent == experiment]\n",
    "for result in results:\n",
    "    predictions.append(result.results.get('predictions'))\n",
    "    metrics.append(result.metrics)\n",
    "    grid_parameters.append(result.parameter_selection)\n",
    "    names.append(experiment.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search over parameters defined below\n",
    "@app.parameter_map()\n",
    "def parameters():\n",
    "    return {'SKLearnEstimator': {'parameters': {'k-nn classifier': {'n_neighbors': [1, 3, 5, 7, 9, 11],\n",
    "                                                        'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}}}}\n",
    "\n",
    "# Create the experiment\n",
    "@app.experiment(dataset=dataset, reference=reference,\n",
    "                experiment_name=\"Iris KNN - Grid Search with Static Seed\", seed=1, project_name=\"Examples\", \n",
    "                parameters=parameters)\n",
    "def experiment():\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    estimators = [('k-nn classifier', KNeighborsClassifier())]\n",
    "    return Pipeline(estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [f for f in app._pipeline_output_app.list() if f.parent.parent.parent == experiment]\n",
    "\n",
    "for result in results:\n",
    "    predictions.append(result.results.get('predictions'))\n",
    "    metrics.append(result.metrics)\n",
    "    grid_parameters.append(result.parameter_selection)\n",
    "    names.append(experiment.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search over parameters defined below\n",
    "@app.parameter_map()\n",
    "def parameters():\n",
    "    return {'SKLearnEstimator': {'parameters': {'gaussian process classification': {'max_iter_predict': [50, 100, 110, 125, 130],\n",
    "                                                                                    'random_state': [0]}}}}\n",
    "\n",
    "# Create the experiment\n",
    "@app.experiment(dataset=dataset, reference=reference,\n",
    "                experiment_name=\"Iris GPC - Grid Search with Static Seed\", seed=1, project_name=\"Examples\", \n",
    "                parameters=parameters)\n",
    "def experiment():\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.gaussian_process.gpc import GaussianProcessClassifier\n",
    "    estimators = [('gaussian process classification', GaussianProcessClassifier())]\n",
    "    return Pipeline(estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [f for f in app._pipeline_output_app.list() if f.parent.parent.parent == experiment]\n",
    "\n",
    "for result in results:\n",
    "    predictions.append(result.results.get('predictions')) \n",
    "    metrics.append(result.metrics)\n",
    "    grid_parameters.append(result.parameter_selection)   \n",
    "    names.append(experiment.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search over parameters defined below\n",
    "@app.parameter_map()\n",
    "def parameters():\n",
    "    return {'SKLearnEstimator': {'parameters': {'decision tree classifier': {'max_depth_tree': [5, 10, 15],\n",
    "                                                                                    'random_state': [0]}}}}\n",
    "\n",
    "# Create the experiment\n",
    "@app.experiment(dataset=dataset, reference=reference,\n",
    "                experiment_name=\"Iris Decision Tree - Grid Search with Static Seed\", seed=1, project_name=\"Examples\", \n",
    "                parameters=parameters)\n",
    "def experiment():\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.tree.tree import DecisionTreeClassifier\n",
    "    estimators = [('decision tree classifier', DecisionTreeClassifier())]\n",
    "    return Pipeline(estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [f for f in app._pipeline_output_app.list() if f.parent.parent.parent == experiment]\n",
    "\n",
    "for result in results:\n",
    "    predictions.append(result.results.get('predictions'))\n",
    "    metrics.append(result.metrics)\n",
    "    grid_parameters.append(result.parameter_selection)\n",
    "    names.append(experiment.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search over parameters defined below\n",
    "@app.parameter_map()\n",
    "def parameters():\n",
    "    return {'SKLearnEstimator': {'parameters': {'random forest classifier': {'num_estimators': [3, 5, 9, 11],\n",
    "                                                                             'max_depth_tree': [2, 8, 14],\n",
    "                                                                             'random_state': [0]}}}}\n",
    "\n",
    "# Create the experiment\n",
    "@app.experiment(dataset=dataset, reference=reference,\n",
    "                experiment_name=\"Iris Random Forest - Grid Search with Static Seed\", seed=1, project_name=\"Examples\",\n",
    "                allow_metrics = True, \n",
    "                parameters=parameters)\n",
    "def experiment():\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.ensemble.forest import RandomForestClassifier\n",
    "    estimators = [('random forest classifier', RandomForestClassifier())]\n",
    "    return Pipeline(estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [f for f in app._pipeline_output_app.list() if f.parent.parent.parent == experiment]\n",
    "\n",
    "for result in results:\n",
    "    predictions.append(result.results.get('predictions'))\n",
    "    grid_parameters.append(result.parameter_selection)\n",
    "    metrics.append(result.metrics)\n",
    "    names.append(experiment.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search over parameters defined below\n",
    "@app.parameter_map()\n",
    "def parameters():\n",
    "    return {'SKLearnEstimator': {'parameters': {'multi-layer perceptron classifier': {'activation': ['relu', 'tanh', 'identity'],\n",
    "                                                                             'solver': ['lbfgs', 'sgd', 'adam'],\n",
    "                                                                             'batch_size': [2, 15, 20],\n",
    "                                                                             'learning_rate_init': [0.001, 0.01]}}}}\n",
    "\n",
    "# Create the experiment\n",
    "@app.experiment(dataset=dataset, reference=reference, allow_metrics=True,\n",
    "                experiment_name=\"Iris MLP - Grid Search with Static Seed\", seed=1, project_name=\"Examples\", \n",
    "                parameters=parameters)\n",
    "def experiment():\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.neural_network.multilayer_perceptron import MLPClassifier\n",
    "    estimators = [('multi-layer perceptron classifier', MLPClassifier())]\n",
    "    return Pipeline(estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [f for f in app._pipeline_output_app.list() if f.parent.parent.parent == experiment]\n",
    "\n",
    "for result in results:\n",
    "    predictions.append(result.results.get('predictions'))\n",
    "    metrics.append(result.metrics)\n",
    "    grid_parameters.append(result.parameter_selection)\n",
    "    names.append(experiment.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search over parameters defined below\n",
    "@app.parameter_map()\n",
    "def parameters():\n",
    "    return {'SKLearnEstimator': {'parameters': {'AdaBoost classifier': {'random_state': [0]}}}}\n",
    "\n",
    "# Create the experiment\n",
    "@app.experiment(dataset=dataset, reference=reference,\n",
    "                experiment_name=\"Iris Adaboost - Grid Search with Static Seed\", seed=1, project_name=\"Examples\", \n",
    "                parameters=parameters)\n",
    "def experiment():\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.ensemble.weight_boosting import AdaBoostClassifier\n",
    "    estimators = [('AdaBoost classifier', AdaBoostClassifier())]\n",
    "    return Pipeline(estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [f for f in app._pipeline_output_app.list() if f.parent.parent.parent == experiment]\n",
    "\n",
    "for result in results:\n",
    "    predictions.append(result.results.get('predictions'))\n",
    "    metrics.append(result.metrics)\n",
    "    grid_parameters.append(result.parameter_selection)\n",
    "    names.append(experiment.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indices of experiments that satisfy the f1 range\n",
    "f1_min = 0.00\n",
    "f1_max = 1.0\n",
    "idx_list = []\n",
    "f1_score = []\n",
    "idx = 0\n",
    "for metric in metrics:\n",
    "    for key in metric:\n",
    "        if len(metric.get(key)) > 0:\n",
    "            curr_value = metric.get(key)[1].get('f1_score')\n",
    "            \n",
    "    if f1_min <= curr_value <= f1_max:\n",
    "        idx_list.append(idx)\n",
    "        f1_score.append(curr_value)\n",
    "        \n",
    "    idx += 1           \n",
    "\n",
    "\n",
    "print(idx_list)\n",
    "print(len(idx_list))\n",
    "print(len(metrics))\n",
    "print(f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_predictions = [predictions[idx] for idx in idx_list]\n",
    "matrix = np.zeros([len(modified_predictions), len(modified_predictions)])\n",
    "missclassification_matrix = np.zeros([len(modified_predictions), len(modified_predictions)])\n",
    "for idx in range(0, len(modified_predictions)):\n",
    "    curr_dictionary = modified_predictions[idx]\n",
    "    total_predictions = len(list(modified_predictions[idx].keys()))\n",
    "    curr_missclassification_row = []\n",
    "    total_missclassifications = 0\n",
    "    for inner_idx in range(0, len(modified_predictions)):\n",
    "        \n",
    "        total_predictions = len(list(modified_predictions[idx].keys()))\n",
    "        curr_missclassification_row = []\n",
    "        total_missclassifications = 0\n",
    "        \n",
    "        identical_predictions = 0\n",
    "        missclassified = 0\n",
    "        missed = 0\n",
    "        for key in (modified_predictions[idx].keys()):\n",
    "            target_prediction_dict = modified_predictions[inner_idx].get(key)\n",
    "            if target_prediction_dict is not None:\n",
    "                target_prediction = target_prediction_dict.get('predicted')\n",
    "            source_prediction = modified_predictions[idx].get(key).get('predicted')\n",
    "            truth  = modified_predictions[idx].get(key).get('truth')\n",
    "            if source_prediction == target_prediction:\n",
    "                identical_predictions += 1\n",
    "                if source_prediction != truth:\n",
    "                    missclassified += 1\n",
    "            \n",
    "            if source_prediction != truth and truth != target_prediction:\n",
    "                missed += 1\n",
    "            \n",
    "            if source_prediction != truth:\n",
    "                total_missclassifications += 1\n",
    "\n",
    "        if total_missclassifications == 0:\n",
    "            total_missclassifications = 1\n",
    "            missclassified = 1\n",
    "                \n",
    "        print(names[idx]+\"/\"+names[inner_idx])\n",
    "        print(\"Both failed in the same way:\" + str(missclassified))\n",
    "        if missed != missclassified:\n",
    "            print(\"Both failed: \" + str(missed))\n",
    "        print(total_missclassifications)\n",
    "        print(missclassified/total_missclassifications)\n",
    "        print(identical_predictions)\n",
    "        print(total_predictions)\n",
    "        print(identical_predictions/total_predictions)\n",
    "        print(\"-----------------------------------------------------------------------------\")\n",
    "            \n",
    "        missclassification_matrix[idx][inner_idx] = (missclassified/total_missclassifications)\n",
    "        missclassification_matrix[inner_idx][idx] = (missclassified/total_missclassifications)\n",
    "        #missclassification_matrix[idx][idx] = 0\n",
    "        matrix[idx][inner_idx] = identical_predictions/total_predictions\n",
    "        matrix[inner_idx][idx] = identical_predictions/total_predictions                  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missclassification_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_name = []\n",
    "for idx in idx_list:\n",
    "    name = names[idx]\n",
    "    for key in grid_parameters[idx]:\n",
    "        temp = grid_parameters[idx].get(key)\n",
    "        if temp:\n",
    "            params = str(grid_parameters[idx].get(key))\n",
    "            estimator_name_idx = str(grid_parameters[idx].get(key)).find('.')\n",
    "            estimator_name = str(grid_parameters[idx].get(key))[2:estimator_name_idx+1]\n",
    "            \n",
    "    modified_name.append(name.replace('Iris ', \"\").replace(\" - Grid Search with Static Seed\", \"\") + params.replace(estimator_name, \"\"))\n",
    "    \n",
    "print(modified_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig4, ax = plt.subplots(figsize=(10, 10))\n",
    "fig4.subplots_adjust(top=.94)\n",
    "plt.suptitle('Missclassification similarity', fontsize=14, fontweight='bold')\n",
    "\n",
    "sns.heatmap(missclassification_matrix, fmt='.2f', ax=ax, xticklabels=modified_name, yticklabels=modified_name)\n",
    "plt.savefig(\"missclassification.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig5, ax = plt.subplots(figsize=(10, 10))\n",
    "plt.suptitle('Classification similarity', fontsize=14, fontweight='bold')\n",
    "sns.heatmap(matrix,xticklabels=modified_name, yticklabels=modified_name);\n",
    "plt.savefig(\"classification_similarity.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This generates 100 variables that could possibly be assigned to 5 clusters\n",
    "n_variables = len(modified_name)\n",
    "n_clusters = 5\n",
    "n_samples = 1000\n",
    "\n",
    "# To keep this example simple, each cluster will have a fixed size\n",
    "cluster_size = n_variables // n_clusters\n",
    "\n",
    "# Assign each variable to a cluster\n",
    "belongs_to_cluster = np.repeat(range(n_clusters), cluster_size)\n",
    "np.random.shuffle(belongs_to_cluster)\n",
    "\n",
    "# This latent data is used to make variables that belong\n",
    "# to the same cluster correlated.\n",
    "latent = np.random.randn(n_clusters, n_samples)\n",
    "\n",
    "variables = []\n",
    "\"\"\"\n",
    "for i in range(n_variables):\n",
    "    variables.append(\n",
    "        np.random.randn(n_samples) + latent[belongs_to_cluster[i], :]\n",
    "    )\n",
    "\"\"\"\n",
    "variables = np.round(matrix * 100)\n",
    "#variables = np.array(variables)\n",
    "\n",
    "C = np.cov(variables)\n",
    "\n",
    "def score(C):\n",
    "    '''\n",
    "    Function to assign a score to an ordered covariance matrix.\n",
    "    High correlations within a cluster improve the score.\n",
    "    High correlations between clusters decease the score.\n",
    "    '''\n",
    "    score = 0\n",
    "    for cluster in range(n_clusters):\n",
    "        inside_cluster = np.arange(cluster_size) + cluster * cluster_size\n",
    "        outside_cluster = np.setdiff1d(range(n_variables), inside_cluster)\n",
    "\n",
    "        # Belonging to the same cluster\n",
    "        score += np.sum(C[inside_cluster, :][:, inside_cluster])\n",
    "\n",
    "        # Belonging to different clusters\n",
    "        score -= np.sum(C[inside_cluster, :][:, outside_cluster])\n",
    "        score -= np.sum(C[outside_cluster, :][:, inside_cluster])\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "initial_C = C\n",
    "initial_score = score(C)\n",
    "initial_ordering = np.arange(n_variables)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(C, interpolation='nearest')\n",
    "plt.title('Initial C')\n",
    "plt.savefig(\"Before_Clustering.png\")\n",
    "print('Initial ordering:', initial_ordering)\n",
    "print('Initial covariance matrix score:', initial_score)\n",
    "\n",
    "# Pretty dumb greedy optimization algorithm that continuously\n",
    "# swaps rows to improve the score\n",
    "def swap_rows(C, var1, var2):\n",
    "    '''\n",
    "    Function to swap two rows in a covariance matrix,\n",
    "    updating the appropriate columns as well.\n",
    "    '''\n",
    "    D = C.copy()\n",
    "    D[var2, :] = C[var1, :]\n",
    "    D[var1, :] = C[var2, :]\n",
    "\n",
    "    E = D.copy()\n",
    "    E[:, var2] = D[:, var1]\n",
    "    E[:, var1] = D[:, var2]\n",
    "\n",
    "    return E\n",
    "\n",
    "current_C = C\n",
    "current_ordering = initial_ordering\n",
    "current_score = initial_score\n",
    "\n",
    "max_iter = 1000\n",
    "for i in range(max_iter):\n",
    "    # Find the best row swap to make\n",
    "    best_C = current_C\n",
    "    best_ordering = current_ordering\n",
    "    best_score = current_score\n",
    "    for row1 in range(n_variables):\n",
    "        for row2 in range(n_variables):\n",
    "            if row1 == row2:\n",
    "                continue\n",
    "            option_ordering = best_ordering.copy()\n",
    "            option_ordering[row1] = best_ordering[row2]\n",
    "            option_ordering[row2] = best_ordering[row1]\n",
    "            option_C = swap_rows(best_C, row1, row2)\n",
    "            option_score = score(option_C)\n",
    "\n",
    "            if option_score > best_score:\n",
    "                best_C = option_C\n",
    "                best_ordering = option_ordering\n",
    "                best_score = option_score\n",
    "\n",
    "    if best_score > current_score:\n",
    "        # Perform the best row swap\n",
    "        current_C = best_C\n",
    "        current_ordering = best_ordering\n",
    "        current_score = best_score\n",
    "    else:\n",
    "        # No row swap found that improves the solution, we're done\n",
    "        break\n",
    "\n",
    "# Output the result\n",
    "plt.figure()\n",
    "plt.imshow(current_C, interpolation='nearest')\n",
    "plt.title('Best C')\n",
    "plt.savefig(\"clustered.png\")\n",
    "print('Best ordering:', current_ordering)\n",
    "print('Best score:', current_score)\n",
    "print('Cluster     [variables assigned to this cluster]')\n",
    "print('------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_estimators = []\n",
    "new_matrix = deepcopy(current_C)\n",
    "print(new_matrix.shape)\n",
    "rows_to_delete = []\n",
    "for cluster in range(n_clusters):\n",
    "    alternate_estimator_names = []\n",
    "    last_name = \"\"\n",
    "    print('Cluster %02d  %s' % (cluster + 1, current_ordering[cluster*cluster_size:(cluster+1)*cluster_size]))\n",
    "    for i in current_ordering[cluster*cluster_size:(cluster+1)*cluster_size]:\n",
    "        temp_name = modified_name[i]\n",
    "        temp_name = temp_name[:temp_name.find('{')]\n",
    "\n",
    "        if temp_name != last_name:\n",
    "            alternate_estimator_names.append(temp_name)\n",
    "            last_name = temp_name\n",
    "        else:\n",
    "            rows_to_delete.append(i)\n",
    "            new_matrix[:, i] = 0\n",
    "            new_matrix[i, :] = 0\n",
    "        #alternate_estimator_names = alternate_estimator_names.union(str(temp_name[:temp_name.find('{')]))\n",
    "    print(alternate_estimator_names)\n",
    "    print(len(rows_to_delete))\n",
    "    clustered_estimators.append(list(alternate_estimator_names))\n",
    "    \n",
    "print(clustered_estimators)\n",
    "print(rows_to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = new_matrix[~np.all(new_matrix == 0, axis=1)]\n",
    "#t = t[~np.all(t == 0, axis=0)]\n",
    "t = np.delete(t, rows_to_delete, axis=1)\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig4, ax = plt.subplots(figsize=(10, 10))\n",
    "fig4.subplots_adjust(top=.94)\n",
    "plt.suptitle('Classification similarity', fontsize=14, fontweight='bold')\n",
    "\n",
    "sns.heatmap(t, fmt='.2f', ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
